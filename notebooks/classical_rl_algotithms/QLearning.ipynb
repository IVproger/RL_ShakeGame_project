{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow to import modules from the project root directory\n",
    "import sys\n",
    "import os\n",
    "# Add the project root directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from src.ParObsSnakeEnv import ParObsSnakeEnv\n",
    "from src.FullObsSnakeEnv import FullObsSnakeEnv\n",
    "from src.utils import compute_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../artifacts/images/Qlearning.png\" alt=\"Q-learning algorithm\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.5, discount_factor=0.99, epsilon=0.1, learning_rate_decay=0.8, epsilon_decay=0.9):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    def choose_action(self, state, Greedy=False):\n",
    "        state = tuple(state.flatten())\n",
    "        if Greedy:\n",
    "            return np.argmax(self.q_table[state]), None\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample(), None  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state]), None  # Exploit\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        next_action = self.choose_action(next_state, Greedy=True)  # Choose next action using epsilon-greedy approach\n",
    "        next_state = tuple(next_state.flatten())\n",
    "        state = tuple(state.flatten())\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "    def print_qtable(self):\n",
    "        for state, actions in self.q_table.items():\n",
    "            print(f\"State: {state}\")\n",
    "            for action, value in enumerate(actions):\n",
    "                print(f\"  Action {action}: {value:.2f}\")\n",
    "            print()\n",
    "    \n",
    "    def save_table(self, table_path):\n",
    "        with open(table_path, 'wb') as f:\n",
    "            pickle.dump(dict(self.q_table), f)\n",
    "    \n",
    "    def load_table(self, table_path):\n",
    "        with open(table_path, 'rb') as f:\n",
    "            self.q_table = pickle.load(f)\n",
    "\n",
    "    def train(self, num_episodes, save_plots=False, plots_path='plots.png'):\n",
    "        self.episode_rewards = []\n",
    "        self.steps_per_episode = []\n",
    "        self.epsilon_values = []\n",
    "        self.learning_rate_values = []\n",
    "        self.average_q_updates = []\n",
    "\n",
    "        for episode in tqdm(range(num_episodes), desc='Training', unit='Episode'):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            q_update_magnitudes = []\n",
    "\n",
    "            while not done:\n",
    "                action, _ = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                old_q_value = self.q_table[tuple(state.flatten())][action]\n",
    "                self.update_q_value(state, action, reward, next_state)\n",
    "                new_q_value = self.q_table[tuple(state.flatten())][action]\n",
    "                q_update_magnitudes.append(abs(new_q_value - old_q_value))\n",
    "\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "\n",
    "            # Logging metrics for the episode\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.steps_per_episode.append(steps)\n",
    "            self.epsilon_values.append(self.epsilon)\n",
    "            self.learning_rate_values.append(self.learning_rate)\n",
    "            self.average_q_updates.append(np.mean(q_update_magnitudes))\n",
    "\n",
    "            # Decay epsilon and learning rate periodically\n",
    "            if episode % 1000 == 0 and episode != 0:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "                self.learning_rate *= self.learning_rate_decay\n",
    "\n",
    "        # Save plots after training\n",
    "        if save_plots:\n",
    "            self.save_plots(plots_path)\n",
    "\n",
    "    def save_plots(self, plots_path):\n",
    "        plots_dir = os.path.dirname(plots_path)\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "        # Create a figure with subplots\n",
    "        fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "        # Plot rewards\n",
    "        axs[0, 0].plot(self.episode_rewards)\n",
    "        axs[0, 0].set_xlabel('Episode')\n",
    "        axs[0, 0].set_ylabel('Cumulative Reward')\n",
    "        axs[0, 0].set_title('Rewards Over Episodes')\n",
    "\n",
    "        # Plot steps per episode\n",
    "        axs[0, 1].plot(self.steps_per_episode)\n",
    "        axs[0, 1].set_xlabel('Episode')\n",
    "        axs[0, 1].set_ylabel('Steps per Episode')\n",
    "        axs[0, 1].set_title('Steps Over Episodes')\n",
    "\n",
    "        # Plot epsilon values\n",
    "        axs[1, 0].plot(self.epsilon_values)\n",
    "        axs[1, 0].set_xlabel('Episode')\n",
    "        axs[1, 0].set_ylabel('Epsilon')\n",
    "        axs[1, 0].set_title('Epsilon Decay Over Episodes')\n",
    "\n",
    "        # Plot learning rate values\n",
    "        axs[1, 1].plot(self.learning_rate_values)\n",
    "        axs[1, 1].set_xlabel('Episode')\n",
    "        axs[1, 1].set_ylabel('Learning Rate')\n",
    "        axs[1, 1].set_title('Learning Rate Decay Over Episodes')\n",
    "\n",
    "        # Plot Q-value updates\n",
    "        axs[2, 0].plot(self.average_q_updates)\n",
    "        axs[2, 0].set_xlabel('Episode')\n",
    "        axs[2, 0].set_ylabel('Average Q-Value Update')\n",
    "        axs[2, 0].set_title('Q-Value Updates Over Episodes')\n",
    "\n",
    "        # Hide the empty subplot (bottom right)\n",
    "        fig.delaxes(axs[2, 1])\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the figure\n",
    "        plt.savefig(plots_path)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "# env = FullObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "env = ParObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "agent = QLearningAgent(env, epsilon=0.1, discount_factor=0.9, learning_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/15000 [00:00<?, ?Episode/s]/tmp/ipykernel_19932/1296490775.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.q_table[state][action] += self.learning_rate * td_error\n",
      "Training: 100%|██████████| 15000/15000 [00:35<00:00, 427.81Episode/s]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 15000\n",
    "agent.train(num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = 'full 'if isinstance(env, FullObsSnakeEnv) else 'par'\n",
    "\n",
    "table_name = f'q_learning_table_{environment}_{num_episodes}_{grid_size}.pkl'\n",
    "model_weights_dir = os.path.join('../..', 'models', 'q-learning')\n",
    "os.makedirs(model_weights_dir, exist_ok=True)\n",
    "table_path = os.path.join(model_weights_dir, table_name)\n",
    "\n",
    "agent.save_table(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [00:00<00:00, 373.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 1, Episode reward: -49\n",
      "Snake length: 40, Episode reward: 3450\n",
      "Snake length: 30, Episode reward: 2505\n",
      "Snake length: 25, Episode reward: 2037\n",
      "Snake length: 24, Episode reward: 1969\n",
      "Snake length: 22, Episode reward: 1839\n",
      "Snake length: 5, Episode reward: 302\n",
      "Snake length: 9, Episode reward: 617\n",
      "Snake length: 15, Episode reward: 1169\n",
      "Snake length: 21, Episode reward: 1687\n",
      "Snake length: 16, Episode reward: 1244\n",
      "Snake length: 18, Episode reward: 1435\n",
      "Snake length: 24, Episode reward: 1913\n",
      "Snake length: 25, Episode reward: 1979\n",
      "Snake length: 17, Episode reward: 1311\n",
      "Snake length: 15, Episode reward: 1133\n",
      "Snake length: 24, Episode reward: 1908\n",
      "Snake length: 5, Episode reward: 284\n",
      "Snake length: 26, Episode reward: 2205\n",
      "Snake length: 12, Episode reward: 930\n",
      "Snake length: 19, Episode reward: 1572\n",
      "Snake length: 15, Episode reward: 1141\n",
      "Snake length: 29, Episode reward: 2436\n",
      "Snake length: 19, Episode reward: 1493\n",
      "Snake length: 16, Episode reward: 1215\n",
      "Snake length: 12, Episode reward: 906\n",
      "Snake length: 25, Episode reward: 2031\n",
      "Snake length: 36, Episode reward: 3033\n",
      "Snake length: 23, Episode reward: 1909\n",
      "Snake length: 14, Episode reward: 1101\n",
      "Snake length: 29, Episode reward: 2394\n",
      "Snake length: 4, Episode reward: 171\n",
      "Snake length: 22, Episode reward: 1780\n",
      "Snake length: 11, Episode reward: 844\n",
      "Snake length: 36, Episode reward: 3044\n",
      "Snake length: 27, Episode reward: 2252\n",
      "Snake length: 3, Episode reward: 120\n",
      "Snake length: 35, Episode reward: 2993\n",
      "Snake length: 27, Episode reward: 2256\n",
      "Snake length: 36, Episode reward: 2934\n",
      "Snake length: 39, Episode reward: 3290\n",
      "Snake length: 30, Episode reward: 2525\n",
      "Snake length: 29, Episode reward: 2404\n",
      "Snake length: 24, Episode reward: 1995\n",
      "Snake length: 5, Episode reward: 283\n",
      "Snake length: 22, Episode reward: 1834\n",
      "Snake length: 33, Episode reward: 2784\n",
      "Snake length: 26, Episode reward: 2164\n",
      "Snake length: 25, Episode reward: 2052\n",
      "Snake length: 29, Episode reward: 2335\n",
      "Snake length: 17, Episode reward: 1282\n",
      "Snake length: 29, Episode reward: 2312\n",
      "Snake length: 30, Episode reward: 2472\n",
      "Snake length: 5, Episode reward: 283\n",
      "Snake length: 14, Episode reward: 1091\n",
      "Snake length: 25, Episode reward: 2058\n",
      "Snake length: 18, Episode reward: 1416\n",
      "Snake length: 7, Episode reward: 469\n",
      "Snake length: 12, Episode reward: 881\n",
      "Snake length: 28, Episode reward: 2331\n",
      "Snake length: 10, Episode reward: 749\n",
      "Snake length: 21, Episode reward: 1679\n",
      "Snake length: 13, Episode reward: 1002\n",
      "Snake length: 24, Episode reward: 1960\n",
      "Snake length: 16, Episode reward: 1228\n",
      "Snake length: 23, Episode reward: 1845\n",
      "Snake length: 19, Episode reward: 1502\n",
      "Snake length: 25, Episode reward: 2064\n",
      "Snake length: 34, Episode reward: 2875\n",
      "Snake length: 4, Episode reward: 175\n",
      "Snake length: 16, Episode reward: 1281\n",
      "Snake length: 15, Episode reward: 1158\n",
      "Snake length: 37, Episode reward: 3104\n",
      "Snake length: 14, Episode reward: 1068\n",
      "Snake length: 7, Episode reward: 464\n",
      "Snake length: 37, Episode reward: 3187\n",
      "Snake length: 22, Episode reward: 1851\n",
      "Snake length: 10, Episode reward: 702\n",
      "Snake length: 27, Episode reward: 2193\n",
      "Snake length: 34, Episode reward: 2844\n",
      "Snake length: 6, Episode reward: 369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 377.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 38, Episode reward: 3181\n",
      "Snake length: 33, Episode reward: 2774\n",
      "Snake length: 26, Episode reward: 2146\n",
      "Snake length: 18, Episode reward: 1427\n",
      "Snake length: 13, Episode reward: 954\n",
      "Snake length: 7, Episode reward: 452\n",
      "Snake length: 23, Episode reward: 1887\n",
      "Snake length: 9, Episode reward: 640\n",
      "Snake length: 15, Episode reward: 1186\n",
      "Snake length: 6, Episode reward: 352\n",
      "Snake length: 7, Episode reward: 470\n",
      "Snake length: 23, Episode reward: 1845\n",
      "Snake length: 26, Episode reward: 2190\n",
      "Snake length: 33, Episode reward: 2768\n",
      "Snake length: 23, Episode reward: 1838\n",
      "Snake length: 19, Episode reward: 1543\n",
      "Snake length: 24, Episode reward: 1939\n",
      "Snake length: 22, Episode reward: 1742\n",
      "Snake length: 31, Episode reward: 2628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'snake_lengths': [1,\n",
       "  40,\n",
       "  30,\n",
       "  25,\n",
       "  24,\n",
       "  22,\n",
       "  5,\n",
       "  9,\n",
       "  15,\n",
       "  21,\n",
       "  16,\n",
       "  18,\n",
       "  24,\n",
       "  25,\n",
       "  17,\n",
       "  15,\n",
       "  24,\n",
       "  5,\n",
       "  26,\n",
       "  12,\n",
       "  19,\n",
       "  15,\n",
       "  29,\n",
       "  19,\n",
       "  16,\n",
       "  12,\n",
       "  25,\n",
       "  36,\n",
       "  23,\n",
       "  14,\n",
       "  29,\n",
       "  4,\n",
       "  22,\n",
       "  11,\n",
       "  36,\n",
       "  27,\n",
       "  3,\n",
       "  35,\n",
       "  27,\n",
       "  36,\n",
       "  39,\n",
       "  30,\n",
       "  29,\n",
       "  24,\n",
       "  5,\n",
       "  22,\n",
       "  33,\n",
       "  26,\n",
       "  25,\n",
       "  29,\n",
       "  17,\n",
       "  29,\n",
       "  30,\n",
       "  5,\n",
       "  14,\n",
       "  25,\n",
       "  18,\n",
       "  7,\n",
       "  12,\n",
       "  28,\n",
       "  10,\n",
       "  21,\n",
       "  13,\n",
       "  24,\n",
       "  16,\n",
       "  23,\n",
       "  19,\n",
       "  25,\n",
       "  34,\n",
       "  4,\n",
       "  16,\n",
       "  15,\n",
       "  37,\n",
       "  14,\n",
       "  7,\n",
       "  37,\n",
       "  22,\n",
       "  10,\n",
       "  27,\n",
       "  34,\n",
       "  6,\n",
       "  38,\n",
       "  33,\n",
       "  26,\n",
       "  18,\n",
       "  13,\n",
       "  7,\n",
       "  23,\n",
       "  9,\n",
       "  15,\n",
       "  6,\n",
       "  7,\n",
       "  23,\n",
       "  26,\n",
       "  33,\n",
       "  23,\n",
       "  19,\n",
       "  24,\n",
       "  22,\n",
       "  31],\n",
       " 'episode_rewards': [-49,\n",
       "  3450,\n",
       "  2505,\n",
       "  2037,\n",
       "  1969,\n",
       "  1839,\n",
       "  302,\n",
       "  617,\n",
       "  1169,\n",
       "  1687,\n",
       "  1244,\n",
       "  1435,\n",
       "  1913,\n",
       "  1979,\n",
       "  1311,\n",
       "  1133,\n",
       "  1908,\n",
       "  284,\n",
       "  2205,\n",
       "  930,\n",
       "  1572,\n",
       "  1141,\n",
       "  2436,\n",
       "  1493,\n",
       "  1215,\n",
       "  906,\n",
       "  2031,\n",
       "  3033,\n",
       "  1909,\n",
       "  1101,\n",
       "  2394,\n",
       "  171,\n",
       "  1780,\n",
       "  844,\n",
       "  3044,\n",
       "  2252,\n",
       "  120,\n",
       "  2993,\n",
       "  2256,\n",
       "  2934,\n",
       "  3290,\n",
       "  2525,\n",
       "  2404,\n",
       "  1995,\n",
       "  283,\n",
       "  1834,\n",
       "  2784,\n",
       "  2164,\n",
       "  2052,\n",
       "  2335,\n",
       "  1282,\n",
       "  2312,\n",
       "  2472,\n",
       "  283,\n",
       "  1091,\n",
       "  2058,\n",
       "  1416,\n",
       "  469,\n",
       "  881,\n",
       "  2331,\n",
       "  749,\n",
       "  1679,\n",
       "  1002,\n",
       "  1960,\n",
       "  1228,\n",
       "  1845,\n",
       "  1502,\n",
       "  2064,\n",
       "  2875,\n",
       "  175,\n",
       "  1281,\n",
       "  1158,\n",
       "  3104,\n",
       "  1068,\n",
       "  464,\n",
       "  3187,\n",
       "  1851,\n",
       "  702,\n",
       "  2193,\n",
       "  2844,\n",
       "  369,\n",
       "  3181,\n",
       "  2774,\n",
       "  2146,\n",
       "  1427,\n",
       "  954,\n",
       "  452,\n",
       "  1887,\n",
       "  640,\n",
       "  1186,\n",
       "  352,\n",
       "  470,\n",
       "  1845,\n",
       "  2190,\n",
       "  2768,\n",
       "  1838,\n",
       "  1543,\n",
       "  1939,\n",
       "  1742,\n",
       "  2628]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(env, ParObsSnakeEnv):\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size, interact=False)\n",
    "\n",
    "model_metrics_dir = os.path.join('../..', 'artifacts', 'models_stats', 'q-learning')\n",
    "os.makedirs(model_metrics_dir, exist_ok=True)\n",
    "\n",
    "train_metrics_name = f'q_learning_train_metrics_{environment}_{num_episodes}_{grid_size}.png'\n",
    "train_metrics_path = os.path.join(model_metrics_dir, train_metrics_name)\n",
    "agent.save_plots(train_metrics_path)\n",
    "\n",
    "num_simulations = 100\n",
    "sim_metrics_name = f'q_learning_sim_metrics_{environment}_{num_episodes}_{env.grid_size}_{num_simulations}.json'\n",
    "sim_metrics_path = os.path.join(model_metrics_dir, sim_metrics_name)\n",
    "compute_metrics(agent, env, sim_metrics_path, num_simulations=num_simulations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -75\n"
     ]
    }
   ],
   "source": [
    "if isinstance(env, FullObsSnakeEnv):\n",
    "    env.interact = True\n",
    "else:\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size)\n",
    "    \n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = agent.choose_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
