{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "# allow to import modules from the project root directory\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "from src.ParObsSnakeEnv import ParObsSnakeEnv\n",
    "from src.FullObsSnakeEnv import FullObsSnakeEnv\n",
    "from src.utils import compute_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../artifacts/images/SARSA.png\" alt=\"SARSA algorithm\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    def __init__(self, env, learning_rate=0.5, discount_factor=0.99, epsilon=0.1, learning_rate_decay=0.8, epsilon_decay=0.9):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = tuple(state.flatten())\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample(), None  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state]), None  # Exploit\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state, next_action):\n",
    "        next_state = tuple(next_state.flatten())\n",
    "        state = tuple(state.flatten())\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "    \n",
    "    def save_table(self, table_path):\n",
    "        with open(table_path, 'wb') as f:\n",
    "            pickle.dump(dict(self.q_table), f)\n",
    "    \n",
    "    def load_table(self, table_path):\n",
    "        with open(table_path, 'rb') as f:\n",
    "            self.q_table = pickle.load(f)\n",
    "\n",
    "    def train(self, num_episodes, save_plots=False, plots_path='plots.png'):\n",
    "        self.episode_rewards = []\n",
    "        self.steps_per_episode = []\n",
    "        self.epsilon_values = []\n",
    "        self.learning_rate_values = []\n",
    "        self.average_q_updates = []\n",
    "\n",
    "        for episode in tqdm(range(num_episodes), desc='Training', unit='Episode'):\n",
    "            state = self.env.reset()\n",
    "            action, _ = self.choose_action(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            q_update_magnitudes = []\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_action, _ = self.choose_action(next_state)\n",
    "\n",
    "                old_q_value = self.q_table[tuple(state.flatten())][action]\n",
    "                self.update_q_value(state, action, reward, next_state, next_action)\n",
    "                new_q_value = self.q_table[tuple(state.flatten())][action]\n",
    "                q_update_magnitudes.append(abs(new_q_value - old_q_value))\n",
    "\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                steps += 1\n",
    "\n",
    "            # Logging metrics for the episode\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.steps_per_episode.append(steps)\n",
    "            self.epsilon_values.append(self.epsilon)\n",
    "            self.learning_rate_values.append(self.learning_rate)\n",
    "            self.average_q_updates.append(np.mean(q_update_magnitudes))\n",
    "\n",
    "            # Decay epsilon and learning rate periodically\n",
    "            if episode % 1000 == 0 and episode != 0:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "                self.learning_rate *= self.learning_rate_decay\n",
    "\n",
    "        # Save plots after training\n",
    "        if save_plots:\n",
    "            self.save_plots(plots_path)\n",
    "\n",
    "    def save_plots(self, plots_path):\n",
    "        plots_dir = os.path.dirname(plots_path)\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "        # Create a figure with subplots\n",
    "        fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "        # Plot rewards\n",
    "        axs[0, 0].plot(self.episode_rewards)\n",
    "        axs[0, 0].set_xlabel('Episode')\n",
    "        axs[0, 0].set_ylabel('Cumulative Reward')\n",
    "        axs[0, 0].set_title('Rewards Over Episodes')\n",
    "\n",
    "        # Plot steps per episode\n",
    "        axs[0, 1].plot(self.steps_per_episode)\n",
    "        axs[0, 1].set_xlabel('Episode')\n",
    "        axs[0, 1].set_ylabel('Steps per Episode')\n",
    "        axs[0, 1].set_title('Steps Over Episodes')\n",
    "\n",
    "        # Plot epsilon values\n",
    "        axs[1, 0].plot(self.epsilon_values)\n",
    "        axs[1, 0].set_xlabel('Episode')\n",
    "        axs[1, 0].set_ylabel('Epsilon')\n",
    "        axs[1, 0].set_title('Epsilon Decay Over Episodes')\n",
    "\n",
    "        # Plot learning rate values\n",
    "        axs[1, 1].plot(self.learning_rate_values)\n",
    "        axs[1, 1].set_xlabel('Episode')\n",
    "        axs[1, 1].set_ylabel('Learning Rate')\n",
    "        axs[1, 1].set_title('Learning Rate Decay Over Episodes')\n",
    "\n",
    "        # Plot Q-value updates\n",
    "        axs[2, 0].plot(self.average_q_updates)\n",
    "        axs[2, 0].set_xlabel('Episode')\n",
    "        axs[2, 0].set_ylabel('Average Q-Value Update')\n",
    "        axs[2, 0].set_title('Q-Value Updates Over Episodes')\n",
    "\n",
    "        # Hide the empty subplot (bottom right)\n",
    "        fig.delaxes(axs[2, 1])\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the figure\n",
    "        plt.savefig(plots_path)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "# env = FullObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "env = ParObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "agent = SarsaAgent(env, epsilon=0.1, discount_factor=0.9, learning_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50000/50000 [01:28<00:00, 566.25Episode/s]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 50000\n",
    "agent.train(num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = 'full 'if isinstance(env, FullObsSnakeEnv) else 'par'\n",
    "\n",
    "table_name = f'sarsa_table_{environment}_{num_episodes}_{grid_size}.pkl'\n",
    "model_weights_dir = os.path.join('../..', 'models', 'sarsa')\n",
    "os.makedirs(model_weights_dir, exist_ok=True)\n",
    "table_path = os.path.join(model_weights_dir, table_name)\n",
    "\n",
    "agent.save_table(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:00<00:00, 303.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 20, Episode reward: 1550\n",
      "Snake length: 19, Episode reward: 1531\n",
      "Snake length: 23, Episode reward: 1848\n",
      "Snake length: 37, Episode reward: 3170\n",
      "Snake length: 26, Episode reward: 2176\n",
      "Snake length: 19, Episode reward: 1496\n",
      "Snake length: 14, Episode reward: 1039\n",
      "Snake length: 25, Episode reward: 2102\n",
      "Snake length: 36, Episode reward: 3033\n",
      "Snake length: 31, Episode reward: 2564\n",
      "Snake length: 24, Episode reward: 1950\n",
      "Snake length: 49, Episode reward: 4297\n",
      "Snake length: 22, Episode reward: 1750\n",
      "Snake length: 32, Episode reward: 2672\n",
      "Snake length: 7, Episode reward: 430\n",
      "Snake length: 30, Episode reward: 2587\n",
      "Snake length: 35, Episode reward: 2994\n",
      "Snake length: 38, Episode reward: 3165\n",
      "Snake length: 40, Episode reward: 3383\n",
      "Snake length: 18, Episode reward: 1450\n",
      "Snake length: 39, Episode reward: 3391\n",
      "Snake length: 25, Episode reward: 2113\n",
      "Snake length: 26, Episode reward: 2159\n",
      "Snake length: 21, Episode reward: 1657\n",
      "Snake length: 23, Episode reward: 1892\n",
      "Snake length: 19, Episode reward: 1507\n",
      "Snake length: 29, Episode reward: 2376\n",
      "Snake length: 26, Episode reward: 2098\n",
      "Snake length: 22, Episode reward: 1824\n",
      "Snake length: 20, Episode reward: 1636\n",
      "Snake length: 19, Episode reward: 1512\n",
      "Snake length: 16, Episode reward: 1260\n",
      "Snake length: 30, Episode reward: 2472\n",
      "Snake length: 24, Episode reward: 1954\n",
      "Snake length: 9, Episode reward: 658\n",
      "Snake length: 30, Episode reward: 2498\n",
      "Snake length: 25, Episode reward: 2032\n",
      "Snake length: 26, Episode reward: 2095\n",
      "Snake length: 27, Episode reward: 2234\n",
      "Snake length: 22, Episode reward: 1817\n",
      "Snake length: 29, Episode reward: 2425\n",
      "Snake length: 38, Episode reward: 3132\n",
      "Snake length: 36, Episode reward: 3070\n",
      "Snake length: 19, Episode reward: 1562\n",
      "Snake length: 8, Episode reward: 529\n",
      "Snake length: 17, Episode reward: 1353\n",
      "Snake length: 17, Episode reward: 1311\n",
      "Snake length: 30, Episode reward: 2450\n",
      "Snake length: 17, Episode reward: 1360\n",
      "Snake length: 32, Episode reward: 2715\n",
      "Snake length: 26, Episode reward: 2116\n",
      "Snake length: 41, Episode reward: 3471\n",
      "Snake length: 29, Episode reward: 2384\n",
      "Snake length: 27, Episode reward: 2202\n",
      "Snake length: 12, Episode reward: 881\n",
      "Snake length: 28, Episode reward: 2258\n",
      "Snake length: 25, Episode reward: 2026\n",
      "Snake length: 8, Episode reward: 528\n",
      "Snake length: 19, Episode reward: 1546\n",
      "Snake length: 19, Episode reward: 1524\n",
      "Snake length: 32, Episode reward: 2695\n",
      "Snake length: 32, Episode reward: 2678\n",
      "Snake length: 45, Episode reward: 3781\n",
      "Snake length: 31, Episode reward: 2651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 279.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 36, Episode reward: 3092\n",
      "Snake length: 24, Episode reward: 1981\n",
      "Snake length: 5, Episode reward: 264\n",
      "Snake length: 45, Episode reward: 3873\n",
      "Snake length: 22, Episode reward: 1795\n",
      "Snake length: 40, Episode reward: 3425\n",
      "Snake length: 34, Episode reward: 2831\n",
      "Snake length: 28, Episode reward: 2366\n",
      "Snake length: 20, Episode reward: 1560\n",
      "Snake length: 24, Episode reward: 1998\n",
      "Snake length: 22, Episode reward: 1786\n",
      "Snake length: 21, Episode reward: 1741\n",
      "Snake length: 30, Episode reward: 2557\n",
      "Snake length: 37, Episode reward: 3090\n",
      "Snake length: 36, Episode reward: 3019\n",
      "Snake length: 20, Episode reward: 1623\n",
      "Snake length: 9, Episode reward: 676\n",
      "Snake length: 42, Episode reward: 3541\n",
      "Snake length: 39, Episode reward: 3273\n",
      "Snake length: 24, Episode reward: 1980\n",
      "Snake length: 31, Episode reward: 2644\n",
      "Snake length: 49, Episode reward: 4189\n",
      "Snake length: 26, Episode reward: 2116\n",
      "Snake length: 19, Episode reward: 1552\n",
      "Snake length: 25, Episode reward: 2012\n",
      "Snake length: 44, Episode reward: 3698\n",
      "Snake length: 21, Episode reward: 1686\n",
      "Snake length: 29, Episode reward: 2403\n",
      "Snake length: 28, Episode reward: 2390\n",
      "Snake length: 18, Episode reward: 1485\n",
      "Snake length: 34, Episode reward: 2855\n",
      "Snake length: 28, Episode reward: 2333\n",
      "Snake length: 14, Episode reward: 1104\n",
      "Snake length: 32, Episode reward: 2610\n",
      "Snake length: 30, Episode reward: 2524\n",
      "Snake length: 29, Episode reward: 2487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'snake_lengths': [20,\n",
       "  19,\n",
       "  23,\n",
       "  37,\n",
       "  26,\n",
       "  19,\n",
       "  14,\n",
       "  25,\n",
       "  36,\n",
       "  31,\n",
       "  24,\n",
       "  49,\n",
       "  22,\n",
       "  32,\n",
       "  7,\n",
       "  30,\n",
       "  35,\n",
       "  38,\n",
       "  40,\n",
       "  18,\n",
       "  39,\n",
       "  25,\n",
       "  26,\n",
       "  21,\n",
       "  23,\n",
       "  19,\n",
       "  29,\n",
       "  26,\n",
       "  22,\n",
       "  20,\n",
       "  19,\n",
       "  16,\n",
       "  30,\n",
       "  24,\n",
       "  9,\n",
       "  30,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  22,\n",
       "  29,\n",
       "  38,\n",
       "  36,\n",
       "  19,\n",
       "  8,\n",
       "  17,\n",
       "  17,\n",
       "  30,\n",
       "  17,\n",
       "  32,\n",
       "  26,\n",
       "  41,\n",
       "  29,\n",
       "  27,\n",
       "  12,\n",
       "  28,\n",
       "  25,\n",
       "  8,\n",
       "  19,\n",
       "  19,\n",
       "  32,\n",
       "  32,\n",
       "  45,\n",
       "  31,\n",
       "  36,\n",
       "  24,\n",
       "  5,\n",
       "  45,\n",
       "  22,\n",
       "  40,\n",
       "  34,\n",
       "  28,\n",
       "  20,\n",
       "  24,\n",
       "  22,\n",
       "  21,\n",
       "  30,\n",
       "  37,\n",
       "  36,\n",
       "  20,\n",
       "  9,\n",
       "  42,\n",
       "  39,\n",
       "  24,\n",
       "  31,\n",
       "  49,\n",
       "  26,\n",
       "  19,\n",
       "  25,\n",
       "  44,\n",
       "  21,\n",
       "  29,\n",
       "  28,\n",
       "  18,\n",
       "  34,\n",
       "  28,\n",
       "  14,\n",
       "  32,\n",
       "  30,\n",
       "  29],\n",
       " 'episode_rewards': [1550,\n",
       "  1531,\n",
       "  1848,\n",
       "  3170,\n",
       "  2176,\n",
       "  1496,\n",
       "  1039,\n",
       "  2102,\n",
       "  3033,\n",
       "  2564,\n",
       "  1950,\n",
       "  4297,\n",
       "  1750,\n",
       "  2672,\n",
       "  430,\n",
       "  2587,\n",
       "  2994,\n",
       "  3165,\n",
       "  3383,\n",
       "  1450,\n",
       "  3391,\n",
       "  2113,\n",
       "  2159,\n",
       "  1657,\n",
       "  1892,\n",
       "  1507,\n",
       "  2376,\n",
       "  2098,\n",
       "  1824,\n",
       "  1636,\n",
       "  1512,\n",
       "  1260,\n",
       "  2472,\n",
       "  1954,\n",
       "  658,\n",
       "  2498,\n",
       "  2032,\n",
       "  2095,\n",
       "  2234,\n",
       "  1817,\n",
       "  2425,\n",
       "  3132,\n",
       "  3070,\n",
       "  1562,\n",
       "  529,\n",
       "  1353,\n",
       "  1311,\n",
       "  2450,\n",
       "  1360,\n",
       "  2715,\n",
       "  2116,\n",
       "  3471,\n",
       "  2384,\n",
       "  2202,\n",
       "  881,\n",
       "  2258,\n",
       "  2026,\n",
       "  528,\n",
       "  1546,\n",
       "  1524,\n",
       "  2695,\n",
       "  2678,\n",
       "  3781,\n",
       "  2651,\n",
       "  3092,\n",
       "  1981,\n",
       "  264,\n",
       "  3873,\n",
       "  1795,\n",
       "  3425,\n",
       "  2831,\n",
       "  2366,\n",
       "  1560,\n",
       "  1998,\n",
       "  1786,\n",
       "  1741,\n",
       "  2557,\n",
       "  3090,\n",
       "  3019,\n",
       "  1623,\n",
       "  676,\n",
       "  3541,\n",
       "  3273,\n",
       "  1980,\n",
       "  2644,\n",
       "  4189,\n",
       "  2116,\n",
       "  1552,\n",
       "  2012,\n",
       "  3698,\n",
       "  1686,\n",
       "  2403,\n",
       "  2390,\n",
       "  1485,\n",
       "  2855,\n",
       "  2333,\n",
       "  1104,\n",
       "  2610,\n",
       "  2524,\n",
       "  2487]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(env, ParObsSnakeEnv):\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size, interact=False)\n",
    "\n",
    "model_metrics_dir = os.path.join('../..', 'artifacts', 'models_stats', 'sarsa')\n",
    "os.makedirs(model_metrics_dir, exist_ok=True)\n",
    "\n",
    "train_metrics_name = f'sarsa_train_metrics_{environment}_{num_episodes}_{grid_size}.png'\n",
    "train_metrics_path = os.path.join(model_metrics_dir, train_metrics_name)\n",
    "agent.save_plots(train_metrics_path)\n",
    "\n",
    "num_simulations = 100\n",
    "sim_metrics_name = f'sarsa_sim_metrics_{environment}_{num_episodes}_{env.grid_size}_{num_simulations}.json'\n",
    "sim_metrics_path = os.path.join(model_metrics_dir, sim_metrics_name)\n",
    "compute_metrics(agent, env, sim_metrics_path, num_simulations=num_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -75\n"
     ]
    }
   ],
   "source": [
    "if isinstance(env, FullObsSnakeEnv):\n",
    "    env.interact = True\n",
    "else:\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size)\n",
    "    \n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = agent.choose_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
