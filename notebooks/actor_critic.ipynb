{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from SnakeEnv import SnakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.actor = nn.Linear(128, action_dim)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        policy_logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.convert_parameters import parameters_to_vector\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Actor-Critic Network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.actor = nn.Linear(128, action_dim)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        policy_logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return policy_logits, value\n",
    "\n",
    "# Actor-Critic Agent\n",
    "class ActorCriticAgent:\n",
    "    def __init__(self, env, learning_rate=0.001, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        input_dim = env.observation_space.shape[0] * env.observation_space.shape[1] * env.observation_space.shape[2]\n",
    "        self.model = ActorCritic(input_dim, env.action_space.n).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        policy_logits, _ = self.model(state)\n",
    "        policy = F.softmax(policy_logits, dim=-1)\n",
    "        action_dist = torch.distributions.Categorical(policy)\n",
    "        action = action_dist.sample().item()\n",
    "        return action\n",
    "\n",
    "    def compute_returns(self, rewards, dones):\n",
    "        returns = []\n",
    "        g = 0\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            g = reward + self.gamma * g * (1 - done)\n",
    "            returns.insert(0, g)\n",
    "        return torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for episode in tqdm(range(num_episodes), desc='Training', unit='Episode'):\n",
    "            state = self.env.reset()\n",
    "            state = state.flatten()\n",
    "            done = False\n",
    "\n",
    "            states, actions, rewards, dones = [], [], [], []\n",
    "\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = next_state.flatten()\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Compute discounted returns\n",
    "            returns = self.compute_returns(rewards, dones)\n",
    "\n",
    "            # Convert lists to tensors\n",
    "            states_tensor = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "            actions_tensor = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "            returns_tensor = returns.unsqueeze(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            policy_logits, values = self.model(states_tensor)\n",
    "\n",
    "            # Compute advantages\n",
    "            advantages = returns_tensor - values\n",
    "\n",
    "            # Actor loss\n",
    "            log_probs = F.log_softmax(policy_logits, dim=-1)\n",
    "            selected_log_probs = log_probs.gather(1, actions_tensor.unsqueeze(-1))\n",
    "            actor_loss = -selected_log_probs * advantages.detach()\n",
    "\n",
    "            # Critic loss\n",
    "            critic_loss = advantages.pow(2)\n",
    "\n",
    "            # Total loss\n",
    "            loss = actor_loss + critic_loss\n",
    "\n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnv(grid_size=5)\n",
    "agent = ActorCriticAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–Š         | 4052/50000 [00:17<03:41, 207.43Episode/s]"
     ]
    }
   ],
   "source": [
    "agent.train(num_episodes=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous distace: 1\n",
      "Current distance: 2\n",
      ". . . . . \n",
      ". . F . . \n",
      ". . . S . \n",
      ". . . . . \n",
      ". . . . . \n",
      "Reward: -1\n",
      "Previous distace: 2\n",
      "Current distance: 3\n",
      ". . . . . \n",
      ". . F . . \n",
      ". . . . S \n",
      ". . . . . \n",
      ". . . . . \n",
      "Reward: -1\n",
      "Previous distace: 3\n",
      "Current distance: 2\n",
      ". . . . . \n",
      ". . F . S \n",
      ". . . . . \n",
      ". . . . . \n",
      ". . . . . \n",
      "Reward: 1\n",
      "Previous distace: 2\n",
      "Current distance: 1\n",
      ". . . . . \n",
      ". . F S . \n",
      ". . . . . \n",
      ". . . . . \n",
      ". . . . . \n",
      "Reward: 1\n",
      "Previous distace: 1\n",
      "Current distance: 0\n",
      ". . . . . \n",
      ". . S S . \n",
      ". F . . . \n",
      ". . . . . \n",
      ". . . . . \n",
      "Reward: 11\n",
      "Previous distace: 2\n",
      "Current distance: 1\n",
      ". . . . . \n",
      ". S S . . \n",
      ". F . . . \n",
      ". . . . . \n",
      ". . . . . \n",
      "Reward: 1\n",
      "Previous distace: 1\n",
      "Current distance: 2\n",
      ". . . . . \n",
      "S S . . . \n",
      ". F . . . \n",
      ". . . . . \n",
      ". . . . . \n",
      "Reward: -1\n",
      ". . . . . \n",
      "S S . . . \n",
      ". F . . . \n",
      ". . . . . \n",
      ". . . . . \n",
      "Reward: -10\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.choose_action(state.flatten())\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
