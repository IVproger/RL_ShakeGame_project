{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "from src.ParObsSnakeEnv import SnakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "        super(DQN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(input_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.fc3 = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, memory_size=10000, batch_size=64):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_model = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        # Compute current Q-values\n",
    "        q_values = self.model(states)\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        next_q_values = self.target_model(next_states).max(1)[0]\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # Update the Q-network\n",
    "        loss = self.criterion(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "# Train the agent\n",
    "def train_dqn(env, agent, episodes=1000, update_target_every=10):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        agent.update_target_model()\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "        if (episode + 1) % update_target_every == 0:\n",
    "            print(\"Updated target model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5000, Total Reward: -76, Epsilon: 1.000\n",
      "Episode 2/5000, Total Reward: -75, Epsilon: 1.000\n",
      "Episode 3/5000, Total Reward: -76, Epsilon: 1.000\n",
      "Episode 4/5000, Total Reward: 2, Epsilon: 1.000\n",
      "Episode 5/5000, Total Reward: -76, Epsilon: 1.000\n",
      "Episode 6/5000, Total Reward: 6, Epsilon: 1.000\n",
      "Episode 7/5000, Total Reward: -85, Epsilon: 0.975\n",
      "Episode 8/5000, Total Reward: -80, Epsilon: 0.918\n",
      "Episode 9/5000, Total Reward: -76, Epsilon: 0.891\n",
      "Episode 10/5000, Total Reward: -78, Epsilon: 0.848\n",
      "Updated target model.\n",
      "Episode 11/5000, Total Reward: -75, Epsilon: 0.835\n",
      "Episode 12/5000, Total Reward: -75, Epsilon: 0.831\n",
      "Episode 13/5000, Total Reward: -77, Epsilon: 0.810\n",
      "Episode 14/5000, Total Reward: -75, Epsilon: 0.782\n",
      "Episode 15/5000, Total Reward: -74, Epsilon: 0.774\n",
      "Episode 16/5000, Total Reward: -77, Epsilon: 0.763\n",
      "Episode 17/5000, Total Reward: -76, Epsilon: 0.755\n",
      "Episode 18/5000, Total Reward: -73, Epsilon: 0.744\n",
      "Episode 19/5000, Total Reward: -76, Epsilon: 0.627\n",
      "Episode 20/5000, Total Reward: -76, Epsilon: 0.621\n",
      "Updated target model.\n",
      "Episode 21/5000, Total Reward: -77, Epsilon: 0.576\n",
      "Episode 22/5000, Total Reward: -77, Epsilon: 0.545\n",
      "Episode 23/5000, Total Reward: -78, Epsilon: 0.529\n",
      "Episode 24/5000, Total Reward: -73, Epsilon: 0.521\n",
      "Episode 25/5000, Total Reward: -1, Epsilon: 0.440\n",
      "Episode 26/5000, Total Reward: 0, Epsilon: 0.424\n",
      "Episode 27/5000, Total Reward: -81, Epsilon: 0.410\n",
      "Episode 28/5000, Total Reward: -77, Epsilon: 0.404\n",
      "Episode 29/5000, Total Reward: 2, Epsilon: 0.378\n",
      "Episode 30/5000, Total Reward: 7, Epsilon: 0.356\n",
      "Updated target model.\n",
      "Episode 31/5000, Total Reward: -72, Epsilon: 0.332\n",
      "Episode 32/5000, Total Reward: -71, Epsilon: 0.314\n",
      "Episode 33/5000, Total Reward: 1, Epsilon: 0.311\n",
      "Episode 34/5000, Total Reward: 5, Epsilon: 0.287\n",
      "Episode 35/5000, Total Reward: -66, Epsilon: 0.273\n",
      "Episode 36/5000, Total Reward: -81, Epsilon: 0.261\n",
      "Episode 37/5000, Total Reward: -73, Epsilon: 0.252\n",
      "Episode 38/5000, Total Reward: 6, Epsilon: 0.236\n",
      "Episode 39/5000, Total Reward: 8, Epsilon: 0.219\n",
      "Episode 40/5000, Total Reward: -70, Epsilon: 0.208\n",
      "Updated target model.\n",
      "Episode 41/5000, Total Reward: 86, Epsilon: 0.194\n",
      "Episode 42/5000, Total Reward: 2, Epsilon: 0.182\n",
      "Episode 43/5000, Total Reward: 5, Epsilon: 0.177\n",
      "Episode 44/5000, Total Reward: -1, Epsilon: 0.170\n",
      "Episode 45/5000, Total Reward: 256, Epsilon: 0.127\n",
      "Episode 46/5000, Total Reward: -76, Epsilon: 0.122\n",
      "Episode 47/5000, Total Reward: -72, Epsilon: 0.118\n",
      "Episode 48/5000, Total Reward: 11, Epsilon: 0.111\n",
      "Episode 49/5000, Total Reward: -77, Epsilon: 0.110\n",
      "Episode 50/5000, Total Reward: -76, Epsilon: 0.109\n",
      "Updated target model.\n",
      "Episode 51/5000, Total Reward: -75, Epsilon: 0.104\n",
      "Episode 52/5000, Total Reward: -75, Epsilon: 0.103\n",
      "Episode 53/5000, Total Reward: -78, Epsilon: 0.098\n",
      "Episode 54/5000, Total Reward: 741, Epsilon: 0.063\n",
      "Episode 55/5000, Total Reward: 494, Epsilon: 0.049\n",
      "Episode 56/5000, Total Reward: 88, Epsilon: 0.045\n",
      "Episode 57/5000, Total Reward: 257, Epsilon: 0.038\n",
      "Episode 58/5000, Total Reward: 748, Epsilon: 0.026\n",
      "Episode 59/5000, Total Reward: 170, Epsilon: 0.023\n",
      "Episode 60/5000, Total Reward: 663, Epsilon: 0.017\n",
      "Updated target model.\n",
      "Episode 61/5000, Total Reward: 338, Epsilon: 0.011\n",
      "Episode 62/5000, Total Reward: 582, Epsilon: 0.010\n",
      "Episode 63/5000, Total Reward: 332, Epsilon: 0.010\n",
      "Episode 64/5000, Total Reward: 401, Epsilon: 0.010\n",
      "Episode 65/5000, Total Reward: 161, Epsilon: 0.010\n",
      "Episode 66/5000, Total Reward: 178, Epsilon: 0.010\n",
      "Episode 67/5000, Total Reward: 244, Epsilon: 0.010\n",
      "Episode 68/5000, Total Reward: 434, Epsilon: 0.010\n",
      "Episode 69/5000, Total Reward: -71, Epsilon: 0.010\n",
      "Episode 70/5000, Total Reward: 168, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 71/5000, Total Reward: 240, Epsilon: 0.010\n",
      "Episode 72/5000, Total Reward: -76, Epsilon: 0.010\n",
      "Episode 73/5000, Total Reward: 566, Epsilon: 0.010\n",
      "Episode 74/5000, Total Reward: 732, Epsilon: 0.010\n",
      "Episode 75/5000, Total Reward: 167, Epsilon: 0.010\n",
      "Episode 76/5000, Total Reward: 11, Epsilon: 0.010\n",
      "Episode 77/5000, Total Reward: 832, Epsilon: 0.010\n",
      "Episode 78/5000, Total Reward: 12, Epsilon: 0.010\n",
      "Episode 79/5000, Total Reward: 827, Epsilon: 0.010\n",
      "Episode 80/5000, Total Reward: -81, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 81/5000, Total Reward: 583, Epsilon: 0.010\n",
      "Episode 82/5000, Total Reward: 409, Epsilon: 0.010\n",
      "Episode 83/5000, Total Reward: 259, Epsilon: 0.010\n",
      "Episode 84/5000, Total Reward: 684, Epsilon: 0.010\n",
      "Episode 85/5000, Total Reward: 579, Epsilon: 0.010\n",
      "Episode 86/5000, Total Reward: 1074, Epsilon: 0.010\n",
      "Episode 87/5000, Total Reward: 428, Epsilon: 0.010\n",
      "Episode 88/5000, Total Reward: 488, Epsilon: 0.010\n",
      "Episode 89/5000, Total Reward: 501, Epsilon: 0.010\n",
      "Episode 90/5000, Total Reward: 1213, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 91/5000, Total Reward: 1093, Epsilon: 0.010\n",
      "Episode 92/5000, Total Reward: 428, Epsilon: 0.010\n",
      "Episode 93/5000, Total Reward: 756, Epsilon: 0.010\n",
      "Episode 94/5000, Total Reward: 751, Epsilon: 0.010\n",
      "Episode 95/5000, Total Reward: 335, Epsilon: 0.010\n",
      "Episode 96/5000, Total Reward: 971, Epsilon: 0.010\n",
      "Episode 97/5000, Total Reward: 978, Epsilon: 0.010\n",
      "Episode 98/5000, Total Reward: 399, Epsilon: 0.010\n",
      "Episode 99/5000, Total Reward: 329, Epsilon: 0.010\n",
      "Episode 100/5000, Total Reward: 659, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 101/5000, Total Reward: 824, Epsilon: 0.010\n",
      "Episode 102/5000, Total Reward: 1460, Epsilon: 0.010\n",
      "Episode 103/5000, Total Reward: 260, Epsilon: 0.010\n",
      "Episode 104/5000, Total Reward: 1002, Epsilon: 0.010\n",
      "Episode 105/5000, Total Reward: 665, Epsilon: 0.010\n",
      "Episode 106/5000, Total Reward: 330, Epsilon: 0.010\n",
      "Episode 107/5000, Total Reward: 894, Epsilon: 0.010\n",
      "Episode 108/5000, Total Reward: 567, Epsilon: 0.010\n",
      "Episode 109/5000, Total Reward: 1232, Epsilon: 0.010\n",
      "Episode 110/5000, Total Reward: 333, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 111/5000, Total Reward: 830, Epsilon: 0.010\n",
      "Episode 112/5000, Total Reward: 11, Epsilon: 0.010\n",
      "Episode 113/5000, Total Reward: 503, Epsilon: 0.010\n",
      "Episode 114/5000, Total Reward: 1077, Epsilon: 0.010\n",
      "Episode 115/5000, Total Reward: 994, Epsilon: 0.010\n",
      "Episode 116/5000, Total Reward: 724, Epsilon: 0.010\n",
      "Episode 117/5000, Total Reward: 480, Epsilon: 0.010\n",
      "Episode 118/5000, Total Reward: 743, Epsilon: 0.010\n",
      "Episode 119/5000, Total Reward: 493, Epsilon: 0.010\n",
      "Episode 120/5000, Total Reward: 665, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 121/5000, Total Reward: 556, Epsilon: 0.010\n",
      "Episode 122/5000, Total Reward: 86, Epsilon: 0.010\n",
      "Episode 123/5000, Total Reward: 986, Epsilon: 0.010\n",
      "Episode 124/5000, Total Reward: 91, Epsilon: 0.010\n",
      "Episode 125/5000, Total Reward: 83, Epsilon: 0.010\n",
      "Episode 126/5000, Total Reward: 1021, Epsilon: 0.010\n",
      "Episode 127/5000, Total Reward: 663, Epsilon: 0.010\n",
      "Episode 128/5000, Total Reward: 182, Epsilon: 0.010\n",
      "Episode 129/5000, Total Reward: 841, Epsilon: 0.010\n",
      "Episode 130/5000, Total Reward: 658, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 131/5000, Total Reward: 719, Epsilon: 0.010\n",
      "Episode 132/5000, Total Reward: 265, Epsilon: 0.010\n",
      "Episode 133/5000, Total Reward: 244, Epsilon: 0.010\n",
      "Episode 134/5000, Total Reward: 592, Epsilon: 0.010\n",
      "Episode 135/5000, Total Reward: 336, Epsilon: 0.010\n",
      "Episode 136/5000, Total Reward: 987, Epsilon: 0.010\n",
      "Episode 137/5000, Total Reward: 1193, Epsilon: 0.010\n",
      "Episode 138/5000, Total Reward: 909, Epsilon: 0.010\n",
      "Episode 139/5000, Total Reward: 1104, Epsilon: 0.010\n",
      "Episode 140/5000, Total Reward: 677, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 141/5000, Total Reward: 490, Epsilon: 0.010\n",
      "Episode 142/5000, Total Reward: 767, Epsilon: 0.010\n",
      "Episode 143/5000, Total Reward: 89, Epsilon: 0.010\n",
      "Episode 144/5000, Total Reward: 1077, Epsilon: 0.010\n",
      "Episode 145/5000, Total Reward: 1144, Epsilon: 0.010\n",
      "Episode 146/5000, Total Reward: 1244, Epsilon: 0.010\n",
      "Episode 147/5000, Total Reward: 491, Epsilon: 0.010\n",
      "Episode 148/5000, Total Reward: 977, Epsilon: 0.010\n",
      "Episode 149/5000, Total Reward: 1153, Epsilon: 0.010\n",
      "Episode 150/5000, Total Reward: 1401, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 151/5000, Total Reward: 177, Epsilon: 0.010\n",
      "Episode 152/5000, Total Reward: 674, Epsilon: 0.010\n",
      "Episode 153/5000, Total Reward: 670, Epsilon: 0.010\n",
      "Episode 154/5000, Total Reward: 176, Epsilon: 0.010\n",
      "Episode 155/5000, Total Reward: -75, Epsilon: 0.010\n",
      "Episode 156/5000, Total Reward: 4, Epsilon: 0.010\n",
      "Episode 157/5000, Total Reward: 1319, Epsilon: 0.010\n",
      "Episode 158/5000, Total Reward: 171, Epsilon: 0.010\n",
      "Episode 159/5000, Total Reward: 827, Epsilon: 0.010\n",
      "Episode 160/5000, Total Reward: -68, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 161/5000, Total Reward: 341, Epsilon: 0.010\n",
      "Episode 162/5000, Total Reward: 1064, Epsilon: 0.010\n",
      "Episode 163/5000, Total Reward: 402, Epsilon: 0.010\n",
      "Episode 164/5000, Total Reward: 816, Epsilon: 0.010\n",
      "Episode 165/5000, Total Reward: 408, Epsilon: 0.010\n",
      "Episode 166/5000, Total Reward: 1155, Epsilon: 0.010\n",
      "Episode 167/5000, Total Reward: 1241, Epsilon: 0.010\n",
      "Episode 168/5000, Total Reward: 1017, Epsilon: 0.010\n",
      "Episode 169/5000, Total Reward: 1587, Epsilon: 0.010\n",
      "Episode 170/5000, Total Reward: 1275, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 171/5000, Total Reward: 332, Epsilon: 0.010\n",
      "Episode 172/5000, Total Reward: 831, Epsilon: 0.010\n",
      "Episode 173/5000, Total Reward: 1653, Epsilon: 0.010\n",
      "Episode 174/5000, Total Reward: 890, Epsilon: 0.010\n",
      "Episode 175/5000, Total Reward: 572, Epsilon: 0.010\n",
      "Episode 176/5000, Total Reward: 648, Epsilon: 0.010\n",
      "Episode 177/5000, Total Reward: 996, Epsilon: 0.010\n",
      "Episode 178/5000, Total Reward: 978, Epsilon: 0.010\n",
      "Episode 179/5000, Total Reward: 1654, Epsilon: 0.010\n",
      "Episode 180/5000, Total Reward: 1747, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 181/5000, Total Reward: 491, Epsilon: 0.010\n",
      "Episode 182/5000, Total Reward: 1249, Epsilon: 0.010\n",
      "Episode 183/5000, Total Reward: 931, Epsilon: 0.010\n",
      "Episode 184/5000, Total Reward: 1164, Epsilon: 0.010\n",
      "Episode 185/5000, Total Reward: 882, Epsilon: 0.010\n",
      "Episode 186/5000, Total Reward: 1983, Epsilon: 0.010\n",
      "Episode 187/5000, Total Reward: 725, Epsilon: 0.010\n",
      "Episode 188/5000, Total Reward: 907, Epsilon: 0.010\n",
      "Episode 189/5000, Total Reward: 1859, Epsilon: 0.010\n",
      "Episode 190/5000, Total Reward: 818, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 191/5000, Total Reward: 507, Epsilon: 0.010\n",
      "Episode 192/5000, Total Reward: 426, Epsilon: 0.010\n",
      "Episode 193/5000, Total Reward: 733, Epsilon: 0.010\n",
      "Episode 194/5000, Total Reward: 825, Epsilon: 0.010\n",
      "Episode 195/5000, Total Reward: 1704, Epsilon: 0.010\n",
      "Episode 196/5000, Total Reward: 1759, Epsilon: 0.010\n",
      "Episode 197/5000, Total Reward: 1594, Epsilon: 0.010\n",
      "Episode 198/5000, Total Reward: 2550, Epsilon: 0.010\n",
      "Episode 199/5000, Total Reward: 902, Epsilon: 0.010\n",
      "Episode 200/5000, Total Reward: 1720, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 201/5000, Total Reward: -74, Epsilon: 0.010\n",
      "Episode 202/5000, Total Reward: 826, Epsilon: 0.010\n",
      "Episode 203/5000, Total Reward: 1233, Epsilon: 0.010\n",
      "Episode 204/5000, Total Reward: 897, Epsilon: 0.010\n",
      "Episode 205/5000, Total Reward: 1414, Epsilon: 0.010\n",
      "Episode 206/5000, Total Reward: 1174, Epsilon: 0.010\n",
      "Episode 207/5000, Total Reward: 907, Epsilon: 0.010\n",
      "Episode 208/5000, Total Reward: 1636, Epsilon: 0.010\n",
      "Episode 209/5000, Total Reward: 573, Epsilon: 0.010\n",
      "Episode 210/5000, Total Reward: 1380, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 211/5000, Total Reward: 1546, Epsilon: 0.010\n",
      "Episode 212/5000, Total Reward: 1284, Epsilon: 0.010\n",
      "Episode 213/5000, Total Reward: 1727, Epsilon: 0.010\n",
      "Episode 214/5000, Total Reward: 1348, Epsilon: 0.010\n",
      "Episode 215/5000, Total Reward: 1214, Epsilon: 0.010\n",
      "Episode 216/5000, Total Reward: 1244, Epsilon: 0.010\n",
      "Episode 217/5000, Total Reward: 961, Epsilon: 0.010\n",
      "Episode 218/5000, Total Reward: 819, Epsilon: 0.010\n",
      "Episode 219/5000, Total Reward: 2194, Epsilon: 0.010\n",
      "Episode 220/5000, Total Reward: 1365, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 221/5000, Total Reward: 1774, Epsilon: 0.010\n",
      "Episode 222/5000, Total Reward: 1562, Epsilon: 0.010\n",
      "Episode 223/5000, Total Reward: 1661, Epsilon: 0.010\n",
      "Episode 224/5000, Total Reward: 1134, Epsilon: 0.010\n",
      "Episode 225/5000, Total Reward: 992, Epsilon: 0.010\n",
      "Episode 226/5000, Total Reward: 1394, Epsilon: 0.010\n",
      "Episode 227/5000, Total Reward: 1326, Epsilon: 0.010\n",
      "Episode 228/5000, Total Reward: 982, Epsilon: 0.010\n",
      "Episode 229/5000, Total Reward: 85, Epsilon: 0.010\n",
      "Episode 230/5000, Total Reward: 1312, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 231/5000, Total Reward: 1074, Epsilon: 0.010\n",
      "Episode 232/5000, Total Reward: 1067, Epsilon: 0.010\n",
      "Episode 233/5000, Total Reward: 1719, Epsilon: 0.010\n",
      "Episode 234/5000, Total Reward: 2116, Epsilon: 0.010\n",
      "Episode 235/5000, Total Reward: 482, Epsilon: 0.010\n",
      "Episode 236/5000, Total Reward: 1744, Epsilon: 0.010\n",
      "Episode 237/5000, Total Reward: 2065, Epsilon: 0.010\n",
      "Episode 238/5000, Total Reward: 2451, Epsilon: 0.010\n",
      "Episode 239/5000, Total Reward: 1736, Epsilon: 0.010\n",
      "Episode 240/5000, Total Reward: 727, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 241/5000, Total Reward: 1741, Epsilon: 0.010\n",
      "Episode 242/5000, Total Reward: 979, Epsilon: 0.010\n",
      "Episode 243/5000, Total Reward: 732, Epsilon: 0.010\n",
      "Episode 244/5000, Total Reward: 1249, Epsilon: 0.010\n",
      "Episode 245/5000, Total Reward: 833, Epsilon: 0.010\n",
      "Episode 246/5000, Total Reward: 1395, Epsilon: 0.010\n",
      "Episode 247/5000, Total Reward: 2408, Epsilon: 0.010\n",
      "Episode 248/5000, Total Reward: 1004, Epsilon: 0.010\n",
      "Episode 249/5000, Total Reward: 1331, Epsilon: 0.010\n",
      "Episode 250/5000, Total Reward: 2024, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 251/5000, Total Reward: 1967, Epsilon: 0.010\n",
      "Episode 252/5000, Total Reward: 1254, Epsilon: 0.010\n",
      "Episode 253/5000, Total Reward: 1556, Epsilon: 0.010\n",
      "Episode 254/5000, Total Reward: 759, Epsilon: 0.010\n",
      "Episode 255/5000, Total Reward: 569, Epsilon: 0.010\n",
      "Episode 256/5000, Total Reward: 1485, Epsilon: 0.010\n",
      "Episode 257/5000, Total Reward: 1137, Epsilon: 0.010\n",
      "Episode 258/5000, Total Reward: 984, Epsilon: 0.010\n",
      "Episode 259/5000, Total Reward: 1669, Epsilon: 0.010\n",
      "Episode 260/5000, Total Reward: 1231, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 261/5000, Total Reward: 1424, Epsilon: 0.010\n",
      "Episode 262/5000, Total Reward: 1978, Epsilon: 0.010\n",
      "Episode 263/5000, Total Reward: 1559, Epsilon: 0.010\n",
      "Episode 264/5000, Total Reward: 2049, Epsilon: 0.010\n",
      "Episode 265/5000, Total Reward: 748, Epsilon: 0.010\n",
      "Episode 266/5000, Total Reward: 1319, Epsilon: 0.010\n",
      "Episode 267/5000, Total Reward: 909, Epsilon: 0.010\n",
      "Episode 268/5000, Total Reward: 271, Epsilon: 0.010\n",
      "Episode 269/5000, Total Reward: 1076, Epsilon: 0.010\n",
      "Episode 270/5000, Total Reward: 1162, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 271/5000, Total Reward: 1552, Epsilon: 0.010\n",
      "Episode 272/5000, Total Reward: 1639, Epsilon: 0.010\n",
      "Episode 273/5000, Total Reward: 922, Epsilon: 0.010\n",
      "Episode 274/5000, Total Reward: 1256, Epsilon: 0.010\n",
      "Episode 275/5000, Total Reward: 989, Epsilon: 0.010\n",
      "Episode 276/5000, Total Reward: 1317, Epsilon: 0.010\n",
      "Episode 277/5000, Total Reward: 817, Epsilon: 0.010\n",
      "Episode 278/5000, Total Reward: 1406, Epsilon: 0.010\n",
      "Episode 279/5000, Total Reward: 1789, Epsilon: 0.010\n",
      "Episode 280/5000, Total Reward: 2217, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 281/5000, Total Reward: 408, Epsilon: 0.010\n",
      "Episode 282/5000, Total Reward: 1148, Epsilon: 0.010\n",
      "Episode 283/5000, Total Reward: 1720, Epsilon: 0.010\n",
      "Episode 284/5000, Total Reward: 1965, Epsilon: 0.010\n",
      "Episode 285/5000, Total Reward: 1459, Epsilon: 0.010\n",
      "Episode 286/5000, Total Reward: 1504, Epsilon: 0.010\n",
      "Episode 287/5000, Total Reward: 825, Epsilon: 0.010\n",
      "Episode 288/5000, Total Reward: 1620, Epsilon: 0.010\n",
      "Episode 289/5000, Total Reward: 980, Epsilon: 0.010\n",
      "Episode 290/5000, Total Reward: 1561, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 291/5000, Total Reward: 1724, Epsilon: 0.010\n",
      "Episode 292/5000, Total Reward: 919, Epsilon: 0.010\n",
      "Episode 293/5000, Total Reward: 1154, Epsilon: 0.010\n",
      "Episode 294/5000, Total Reward: 1266, Epsilon: 0.010\n",
      "Episode 295/5000, Total Reward: 1316, Epsilon: 0.010\n",
      "Episode 296/5000, Total Reward: 582, Epsilon: 0.010\n",
      "Episode 297/5000, Total Reward: 1171, Epsilon: 0.010\n",
      "Episode 298/5000, Total Reward: 742, Epsilon: 0.010\n",
      "Episode 299/5000, Total Reward: 1159, Epsilon: 0.010\n",
      "Episode 300/5000, Total Reward: 903, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 301/5000, Total Reward: 1073, Epsilon: 0.010\n",
      "Episode 302/5000, Total Reward: 924, Epsilon: 0.010\n",
      "Episode 303/5000, Total Reward: 670, Epsilon: 0.010\n",
      "Episode 304/5000, Total Reward: 1299, Epsilon: 0.010\n",
      "Episode 305/5000, Total Reward: 10, Epsilon: 0.010\n",
      "Episode 306/5000, Total Reward: 1696, Epsilon: 0.010\n",
      "Episode 307/5000, Total Reward: 1319, Epsilon: 0.010\n",
      "Episode 308/5000, Total Reward: 651, Epsilon: 0.010\n",
      "Episode 309/5000, Total Reward: -75, Epsilon: 0.010\n",
      "Episode 310/5000, Total Reward: 1296, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 311/5000, Total Reward: 1251, Epsilon: 0.010\n",
      "Episode 312/5000, Total Reward: 1994, Epsilon: 0.010\n",
      "Episode 313/5000, Total Reward: 2212, Epsilon: 0.010\n",
      "Episode 314/5000, Total Reward: 1151, Epsilon: 0.010\n",
      "Episode 315/5000, Total Reward: 499, Epsilon: 0.010\n",
      "Episode 316/5000, Total Reward: 2448, Epsilon: 0.010\n",
      "Episode 317/5000, Total Reward: 2770, Epsilon: 0.010\n",
      "Episode 318/5000, Total Reward: 1413, Epsilon: 0.010\n",
      "Episode 319/5000, Total Reward: 1555, Epsilon: 0.010\n",
      "Episode 320/5000, Total Reward: 2124, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 321/5000, Total Reward: 735, Epsilon: 0.010\n",
      "Episode 322/5000, Total Reward: 13, Epsilon: 0.010\n",
      "Episode 323/5000, Total Reward: 1493, Epsilon: 0.010\n",
      "Episode 324/5000, Total Reward: 2215, Epsilon: 0.010\n",
      "Episode 325/5000, Total Reward: 2207, Epsilon: 0.010\n",
      "Episode 326/5000, Total Reward: 1552, Epsilon: 0.010\n",
      "Episode 327/5000, Total Reward: 659, Epsilon: 0.010\n",
      "Episode 328/5000, Total Reward: 1896, Epsilon: 0.010\n",
      "Episode 329/5000, Total Reward: 1376, Epsilon: 0.010\n",
      "Episode 330/5000, Total Reward: 2008, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 331/5000, Total Reward: 732, Epsilon: 0.010\n",
      "Episode 332/5000, Total Reward: 1572, Epsilon: 0.010\n",
      "Episode 333/5000, Total Reward: 1896, Epsilon: 0.010\n",
      "Episode 334/5000, Total Reward: 2558, Epsilon: 0.010\n",
      "Episode 335/5000, Total Reward: 2226, Epsilon: 0.010\n",
      "Episode 336/5000, Total Reward: 1848, Epsilon: 0.010\n",
      "Episode 337/5000, Total Reward: 1078, Epsilon: 0.010\n",
      "Episode 338/5000, Total Reward: 1497, Epsilon: 0.010\n",
      "Episode 339/5000, Total Reward: 908, Epsilon: 0.010\n",
      "Episode 340/5000, Total Reward: 1590, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 341/5000, Total Reward: 1652, Epsilon: 0.010\n",
      "Episode 342/5000, Total Reward: 3044, Epsilon: 0.010\n",
      "Episode 343/5000, Total Reward: 331, Epsilon: 0.010\n",
      "Episode 344/5000, Total Reward: 979, Epsilon: 0.010\n",
      "Episode 345/5000, Total Reward: 901, Epsilon: 0.010\n",
      "Episode 346/5000, Total Reward: 2192, Epsilon: 0.010\n",
      "Episode 347/5000, Total Reward: 2362, Epsilon: 0.010\n",
      "Episode 348/5000, Total Reward: 191, Epsilon: 0.010\n",
      "Episode 349/5000, Total Reward: 1236, Epsilon: 0.010\n",
      "Episode 350/5000, Total Reward: 672, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 351/5000, Total Reward: 916, Epsilon: 0.010\n",
      "Episode 352/5000, Total Reward: 1713, Epsilon: 0.010\n",
      "Episode 353/5000, Total Reward: 1472, Epsilon: 0.010\n",
      "Episode 354/5000, Total Reward: 496, Epsilon: 0.010\n",
      "Episode 355/5000, Total Reward: 1226, Epsilon: 0.010\n",
      "Episode 356/5000, Total Reward: 890, Epsilon: 0.010\n",
      "Episode 357/5000, Total Reward: 1955, Epsilon: 0.010\n",
      "Episode 358/5000, Total Reward: 1076, Epsilon: 0.010\n",
      "Episode 359/5000, Total Reward: 1395, Epsilon: 0.010\n",
      "Episode 360/5000, Total Reward: 1974, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 361/5000, Total Reward: 2320, Epsilon: 0.010\n",
      "Episode 362/5000, Total Reward: 1061, Epsilon: 0.010\n",
      "Episode 363/5000, Total Reward: 1668, Epsilon: 0.010\n",
      "Episode 364/5000, Total Reward: 819, Epsilon: 0.010\n",
      "Episode 365/5000, Total Reward: 2061, Epsilon: 0.010\n",
      "Episode 366/5000, Total Reward: 2393, Epsilon: 0.010\n",
      "Episode 367/5000, Total Reward: 1878, Epsilon: 0.010\n",
      "Episode 368/5000, Total Reward: 2614, Epsilon: 0.010\n",
      "Episode 369/5000, Total Reward: 1490, Epsilon: 0.010\n",
      "Episode 370/5000, Total Reward: 9, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 371/5000, Total Reward: 738, Epsilon: 0.010\n",
      "Episode 372/5000, Total Reward: 1368, Epsilon: 0.010\n",
      "Episode 373/5000, Total Reward: 1133, Epsilon: 0.010\n",
      "Episode 374/5000, Total Reward: 834, Epsilon: 0.010\n",
      "Episode 375/5000, Total Reward: 1320, Epsilon: 0.010\n",
      "Episode 376/5000, Total Reward: 997, Epsilon: 0.010\n",
      "Episode 377/5000, Total Reward: 2609, Epsilon: 0.010\n",
      "Episode 378/5000, Total Reward: 1342, Epsilon: 0.010\n",
      "Episode 379/5000, Total Reward: 1750, Epsilon: 0.010\n",
      "Episode 380/5000, Total Reward: 1327, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 381/5000, Total Reward: 1492, Epsilon: 0.010\n",
      "Episode 382/5000, Total Reward: 1531, Epsilon: 0.010\n",
      "Episode 383/5000, Total Reward: 1320, Epsilon: 0.010\n",
      "Episode 384/5000, Total Reward: 573, Epsilon: 0.010\n",
      "Episode 385/5000, Total Reward: 897, Epsilon: 0.010\n",
      "Episode 386/5000, Total Reward: 334, Epsilon: 0.010\n",
      "Episode 387/5000, Total Reward: 1883, Epsilon: 0.010\n",
      "Episode 388/5000, Total Reward: 1896, Epsilon: 0.010\n",
      "Episode 389/5000, Total Reward: 1378, Epsilon: 0.010\n",
      "Episode 390/5000, Total Reward: 2041, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 391/5000, Total Reward: 771, Epsilon: 0.010\n",
      "Episode 392/5000, Total Reward: 2374, Epsilon: 0.010\n",
      "Episode 393/5000, Total Reward: 2267, Epsilon: 0.010\n",
      "Episode 394/5000, Total Reward: 649, Epsilon: 0.010\n",
      "Episode 395/5000, Total Reward: 1819, Epsilon: 0.010\n",
      "Episode 396/5000, Total Reward: 656, Epsilon: 0.010\n",
      "Episode 397/5000, Total Reward: 1976, Epsilon: 0.010\n",
      "Episode 398/5000, Total Reward: 1476, Epsilon: 0.010\n",
      "Episode 399/5000, Total Reward: 1400, Epsilon: 0.010\n",
      "Episode 400/5000, Total Reward: 1845, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 401/5000, Total Reward: 1691, Epsilon: 0.010\n",
      "Episode 402/5000, Total Reward: 1497, Epsilon: 0.010\n",
      "Episode 403/5000, Total Reward: 2235, Epsilon: 0.010\n",
      "Episode 404/5000, Total Reward: 1402, Epsilon: 0.010\n",
      "Episode 405/5000, Total Reward: 1147, Epsilon: 0.010\n",
      "Episode 406/5000, Total Reward: 1155, Epsilon: 0.010\n",
      "Episode 407/5000, Total Reward: 842, Epsilon: 0.010\n",
      "Episode 408/5000, Total Reward: 1072, Epsilon: 0.010\n",
      "Episode 409/5000, Total Reward: 1657, Epsilon: 0.010\n",
      "Episode 410/5000, Total Reward: 2020, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 411/5000, Total Reward: 1426, Epsilon: 0.010\n",
      "Episode 412/5000, Total Reward: 185, Epsilon: 0.010\n",
      "Episode 413/5000, Total Reward: 1078, Epsilon: 0.010\n",
      "Episode 414/5000, Total Reward: 1394, Epsilon: 0.010\n",
      "Episode 415/5000, Total Reward: 1968, Epsilon: 0.010\n",
      "Episode 416/5000, Total Reward: 1718, Epsilon: 0.010\n",
      "Episode 417/5000, Total Reward: 585, Epsilon: 0.010\n",
      "Episode 418/5000, Total Reward: 1622, Epsilon: 0.010\n",
      "Episode 419/5000, Total Reward: 1626, Epsilon: 0.010\n",
      "Episode 420/5000, Total Reward: 1218, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 421/5000, Total Reward: 1221, Epsilon: 0.010\n",
      "Episode 422/5000, Total Reward: 1792, Epsilon: 0.010\n",
      "Episode 423/5000, Total Reward: 1252, Epsilon: 0.010\n",
      "Episode 424/5000, Total Reward: 1488, Epsilon: 0.010\n",
      "Episode 425/5000, Total Reward: 1494, Epsilon: 0.010\n",
      "Episode 426/5000, Total Reward: 2122, Epsilon: 0.010\n",
      "Episode 427/5000, Total Reward: 1169, Epsilon: 0.010\n",
      "Episode 428/5000, Total Reward: 981, Epsilon: 0.010\n",
      "Episode 429/5000, Total Reward: 1719, Epsilon: 0.010\n",
      "Episode 430/5000, Total Reward: 1640, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 431/5000, Total Reward: 1581, Epsilon: 0.010\n",
      "Episode 432/5000, Total Reward: 21, Epsilon: 0.010\n",
      "Episode 433/5000, Total Reward: 1076, Epsilon: 0.010\n",
      "Episode 434/5000, Total Reward: 411, Epsilon: 0.010\n",
      "Episode 435/5000, Total Reward: 1743, Epsilon: 0.010\n",
      "Episode 436/5000, Total Reward: 965, Epsilon: 0.010\n",
      "Episode 437/5000, Total Reward: 1299, Epsilon: 0.010\n",
      "Episode 438/5000, Total Reward: 1996, Epsilon: 0.010\n",
      "Episode 439/5000, Total Reward: 2520, Epsilon: 0.010\n",
      "Episode 440/5000, Total Reward: 1396, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 441/5000, Total Reward: 1801, Epsilon: 0.010\n",
      "Episode 442/5000, Total Reward: 749, Epsilon: 0.010\n",
      "Episode 443/5000, Total Reward: 1568, Epsilon: 0.010\n",
      "Episode 444/5000, Total Reward: 1736, Epsilon: 0.010\n",
      "Episode 445/5000, Total Reward: 659, Epsilon: 0.010\n",
      "Episode 446/5000, Total Reward: 1168, Epsilon: 0.010\n",
      "Episode 447/5000, Total Reward: 987, Epsilon: 0.010\n",
      "Episode 448/5000, Total Reward: 1814, Epsilon: 0.010\n",
      "Episode 449/5000, Total Reward: 1379, Epsilon: 0.010\n",
      "Episode 450/5000, Total Reward: 2296, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 451/5000, Total Reward: 1307, Epsilon: 0.010\n",
      "Episode 452/5000, Total Reward: 1301, Epsilon: 0.010\n",
      "Episode 453/5000, Total Reward: 1943, Epsilon: 0.010\n",
      "Episode 454/5000, Total Reward: 1309, Epsilon: 0.010\n",
      "Episode 455/5000, Total Reward: 2057, Epsilon: 0.010\n",
      "Episode 456/5000, Total Reward: 916, Epsilon: 0.010\n",
      "Episode 457/5000, Total Reward: 1495, Epsilon: 0.010\n",
      "Episode 458/5000, Total Reward: 409, Epsilon: 0.010\n",
      "Episode 459/5000, Total Reward: 1787, Epsilon: 0.010\n",
      "Episode 460/5000, Total Reward: 1448, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 461/5000, Total Reward: 1836, Epsilon: 0.010\n",
      "Episode 462/5000, Total Reward: 2061, Epsilon: 0.010\n",
      "Episode 463/5000, Total Reward: 1241, Epsilon: 0.010\n",
      "Episode 464/5000, Total Reward: 1502, Epsilon: 0.010\n",
      "Episode 465/5000, Total Reward: 1975, Epsilon: 0.010\n",
      "Episode 466/5000, Total Reward: 1140, Epsilon: 0.010\n",
      "Episode 467/5000, Total Reward: 1153, Epsilon: 0.010\n",
      "Episode 468/5000, Total Reward: 1965, Epsilon: 0.010\n",
      "Episode 469/5000, Total Reward: 2349, Epsilon: 0.010\n",
      "Episode 470/5000, Total Reward: 265, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 471/5000, Total Reward: 1072, Epsilon: 0.010\n",
      "Episode 472/5000, Total Reward: 1486, Epsilon: 0.010\n",
      "Episode 473/5000, Total Reward: 1082, Epsilon: 0.010\n",
      "Episode 474/5000, Total Reward: 1312, Epsilon: 0.010\n",
      "Episode 475/5000, Total Reward: 3094, Epsilon: 0.010\n",
      "Episode 476/5000, Total Reward: 995, Epsilon: 0.010\n",
      "Episode 477/5000, Total Reward: 1161, Epsilon: 0.010\n",
      "Episode 478/5000, Total Reward: 1631, Epsilon: 0.010\n",
      "Episode 479/5000, Total Reward: 1957, Epsilon: 0.010\n",
      "Episode 480/5000, Total Reward: 1263, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 481/5000, Total Reward: 1643, Epsilon: 0.010\n",
      "Episode 482/5000, Total Reward: 2147, Epsilon: 0.010\n",
      "Episode 483/5000, Total Reward: 2057, Epsilon: 0.010\n",
      "Episode 484/5000, Total Reward: 1760, Epsilon: 0.010\n",
      "Episode 485/5000, Total Reward: 1431, Epsilon: 0.010\n",
      "Episode 486/5000, Total Reward: 2082, Epsilon: 0.010\n",
      "Episode 487/5000, Total Reward: 1562, Epsilon: 0.010\n",
      "Episode 488/5000, Total Reward: 651, Epsilon: 0.010\n",
      "Episode 489/5000, Total Reward: 663, Epsilon: 0.010\n",
      "Episode 490/5000, Total Reward: 2225, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 491/5000, Total Reward: 1829, Epsilon: 0.010\n",
      "Episode 492/5000, Total Reward: 998, Epsilon: 0.010\n",
      "Episode 493/5000, Total Reward: 2104, Epsilon: 0.010\n",
      "Episode 494/5000, Total Reward: 1561, Epsilon: 0.010\n",
      "Episode 495/5000, Total Reward: 1393, Epsilon: 0.010\n",
      "Episode 496/5000, Total Reward: 1633, Epsilon: 0.010\n",
      "Episode 497/5000, Total Reward: 821, Epsilon: 0.010\n",
      "Episode 498/5000, Total Reward: 736, Epsilon: 0.010\n",
      "Episode 499/5000, Total Reward: 1480, Epsilon: 0.010\n",
      "Episode 500/5000, Total Reward: 1157, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 501/5000, Total Reward: 1160, Epsilon: 0.010\n",
      "Episode 502/5000, Total Reward: 1311, Epsilon: 0.010\n",
      "Episode 503/5000, Total Reward: 811, Epsilon: 0.010\n",
      "Episode 504/5000, Total Reward: 1286, Epsilon: 0.010\n",
      "Episode 505/5000, Total Reward: 1636, Epsilon: 0.010\n",
      "Episode 506/5000, Total Reward: 753, Epsilon: 0.010\n",
      "Episode 507/5000, Total Reward: 1792, Epsilon: 0.010\n",
      "Episode 508/5000, Total Reward: 1002, Epsilon: 0.010\n",
      "Episode 509/5000, Total Reward: 1397, Epsilon: 0.010\n",
      "Episode 510/5000, Total Reward: 1398, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 511/5000, Total Reward: 829, Epsilon: 0.010\n",
      "Episode 512/5000, Total Reward: 1169, Epsilon: 0.010\n",
      "Episode 513/5000, Total Reward: 1714, Epsilon: 0.010\n",
      "Episode 514/5000, Total Reward: 1992, Epsilon: 0.010\n",
      "Episode 515/5000, Total Reward: 1718, Epsilon: 0.010\n",
      "Episode 516/5000, Total Reward: 1567, Epsilon: 0.010\n",
      "Episode 517/5000, Total Reward: 91, Epsilon: 0.010\n",
      "Episode 518/5000, Total Reward: 902, Epsilon: 0.010\n",
      "Episode 519/5000, Total Reward: 1390, Epsilon: 0.010\n",
      "Episode 520/5000, Total Reward: 1552, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 521/5000, Total Reward: 1064, Epsilon: 0.010\n",
      "Episode 522/5000, Total Reward: 1826, Epsilon: 0.010\n",
      "Episode 523/5000, Total Reward: 576, Epsilon: 0.010\n",
      "Episode 524/5000, Total Reward: 1307, Epsilon: 0.010\n",
      "Episode 525/5000, Total Reward: 1563, Epsilon: 0.010\n",
      "Episode 526/5000, Total Reward: 1466, Epsilon: 0.010\n",
      "Episode 527/5000, Total Reward: 1988, Epsilon: 0.010\n",
      "Episode 528/5000, Total Reward: 1150, Epsilon: 0.010\n",
      "Episode 529/5000, Total Reward: 1439, Epsilon: 0.010\n",
      "Episode 530/5000, Total Reward: 1045, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 531/5000, Total Reward: 1164, Epsilon: 0.010\n",
      "Episode 532/5000, Total Reward: 1493, Epsilon: 0.010\n",
      "Episode 533/5000, Total Reward: 838, Epsilon: 0.010\n",
      "Episode 534/5000, Total Reward: 1316, Epsilon: 0.010\n",
      "Episode 535/5000, Total Reward: 1269, Epsilon: 0.010\n",
      "Episode 536/5000, Total Reward: 984, Epsilon: 0.010\n",
      "Episode 537/5000, Total Reward: 401, Epsilon: 0.010\n",
      "Episode 538/5000, Total Reward: 1134, Epsilon: 0.010\n",
      "Episode 539/5000, Total Reward: 1732, Epsilon: 0.010\n",
      "Episode 540/5000, Total Reward: 822, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 541/5000, Total Reward: 1494, Epsilon: 0.010\n",
      "Episode 542/5000, Total Reward: 1139, Epsilon: 0.010\n",
      "Episode 543/5000, Total Reward: 1974, Epsilon: 0.010\n",
      "Episode 544/5000, Total Reward: 1219, Epsilon: 0.010\n",
      "Episode 545/5000, Total Reward: 985, Epsilon: 0.010\n",
      "Episode 546/5000, Total Reward: 10, Epsilon: 0.010\n",
      "Episode 547/5000, Total Reward: 1020, Epsilon: 0.010\n",
      "Episode 548/5000, Total Reward: 1992, Epsilon: 0.010\n",
      "Episode 549/5000, Total Reward: 998, Epsilon: 0.010\n",
      "Episode 550/5000, Total Reward: 2066, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 551/5000, Total Reward: 2138, Epsilon: 0.010\n",
      "Episode 552/5000, Total Reward: 2696, Epsilon: 0.010\n",
      "Episode 553/5000, Total Reward: 974, Epsilon: 0.010\n",
      "Episode 554/5000, Total Reward: 1309, Epsilon: 0.010\n",
      "Episode 555/5000, Total Reward: 1364, Epsilon: 0.010\n",
      "Episode 556/5000, Total Reward: 675, Epsilon: 0.010\n",
      "Episode 557/5000, Total Reward: 741, Epsilon: 0.010\n",
      "Episode 558/5000, Total Reward: 1306, Epsilon: 0.010\n",
      "Episode 559/5000, Total Reward: 1915, Epsilon: 0.010\n",
      "Episode 560/5000, Total Reward: 676, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 561/5000, Total Reward: 1326, Epsilon: 0.010\n",
      "Episode 562/5000, Total Reward: 1645, Epsilon: 0.010\n",
      "Episode 563/5000, Total Reward: 652, Epsilon: 0.010\n",
      "Episode 564/5000, Total Reward: 1727, Epsilon: 0.010\n",
      "Episode 565/5000, Total Reward: 1653, Epsilon: 0.010\n",
      "Episode 566/5000, Total Reward: 909, Epsilon: 0.010\n",
      "Episode 567/5000, Total Reward: 506, Epsilon: 0.010\n",
      "Episode 568/5000, Total Reward: 1558, Epsilon: 0.010\n",
      "Episode 569/5000, Total Reward: 2065, Epsilon: 0.010\n",
      "Episode 570/5000, Total Reward: 1000, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 571/5000, Total Reward: 1489, Epsilon: 0.010\n",
      "Episode 572/5000, Total Reward: 2402, Epsilon: 0.010\n",
      "Episode 573/5000, Total Reward: 487, Epsilon: 0.010\n",
      "Episode 574/5000, Total Reward: 798, Epsilon: 0.010\n",
      "Episode 575/5000, Total Reward: 1672, Epsilon: 0.010\n",
      "Episode 576/5000, Total Reward: 1390, Epsilon: 0.010\n",
      "Episode 577/5000, Total Reward: 1250, Epsilon: 0.010\n",
      "Episode 578/5000, Total Reward: 890, Epsilon: 0.010\n",
      "Episode 579/5000, Total Reward: 2463, Epsilon: 0.010\n",
      "Episode 580/5000, Total Reward: 1801, Epsilon: 0.010\n",
      "Updated target model.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[1;32m      5\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(state_dim, action_dim)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 72\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m(env, agent, episodes, update_target_every)\u001b[0m\n\u001b[1;32m     69\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 72\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     74\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m, in \u001b[0;36mDQNAgent.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     25\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state)\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = SnakeEnv(grid_size=10, interact=False)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "train_dqn(env, agent, episodes=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -75\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = SnakeEnv(grid_size=20)\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.act(tuple(state.flatten()))\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
