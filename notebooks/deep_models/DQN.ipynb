{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "from src.ParObsSnakeEnv import ParObsSnakeEnv\n",
    "from src.FullObsSnakeEnv import FullObsSnakeEnv\n",
    "from src.utils import compute_metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(input_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.fc3 = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, memory_size=10000, batch_size=64):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_model = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # Metrics for plotting\n",
    "        self.episode_rewards = []\n",
    "        self.episode_losses = []\n",
    "        self.epsilon_values = []\n",
    "        self.average_q_values = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1), None\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        state = state.reshape((1, -1))\n",
    "        with torch.no_grad():\n",
    "            # print(\">>>>\", state.shape)\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item(), None\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        states = states.reshape((self.batch_size, -1))\n",
    "        next_states = next_states.reshape((self.batch_size, -1))\n",
    "\n",
    "        # Compute current Q-values\n",
    "        q_values = self.model(states)\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        next_q_values = self.target_model(next_states).max(1)[0]\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # Update the Q-network\n",
    "        loss = self.criterion(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Log the loss\n",
    "        self.episode_losses.append(loss.item())\n",
    "\n",
    "        # Track average Q-value\n",
    "        self.average_q_values.append(q_values.mean().item())\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves the entire agent to a file.\"\"\"\n",
    "        state = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'target_model_state_dict': self.target_model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'hyperparameters': {\n",
    "                'state_dim': self.state_dim,\n",
    "                'action_dim': self.action_dim,\n",
    "                'gamma': self.gamma,\n",
    "                'epsilon': self.epsilon,\n",
    "                'epsilon_decay': self.epsilon_decay,\n",
    "                'epsilon_min': self.epsilon_min,\n",
    "                'batch_size': self.batch_size,\n",
    "            },\n",
    "            'memory': list(self.memory),  # Convert deque to list\n",
    "        }\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        print(f\"Agent saved to {filename}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename, lr=0.001):\n",
    "        \"\"\"Loads the agent from a file.\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "        \n",
    "        # Recreate the agent\n",
    "        agent = cls(\n",
    "            state['hyperparameters']['state_dim'],\n",
    "            state['hyperparameters']['action_dim'],\n",
    "            lr=lr,\n",
    "            gamma=state['hyperparameters']['gamma'],\n",
    "            epsilon=state['hyperparameters']['epsilon'],\n",
    "            epsilon_decay=state['hyperparameters']['epsilon_decay'],\n",
    "            epsilon_min=state['hyperparameters']['epsilon_min'],\n",
    "            batch_size=state['hyperparameters']['batch_size'],\n",
    "        )\n",
    "        # Restore the agent's state\n",
    "        agent.model.load_state_dict(state['model_state_dict'])\n",
    "        agent.target_model.load_state_dict(state['target_model_state_dict'])\n",
    "        agent.optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        agent.memory = deque(state['memory'], maxlen=len(state['memory']))\n",
    "        print(f\"Agent loaded from {filename}\")\n",
    "        return agent\n",
    "\n",
    "    # Train the agent\n",
    "    def train(self, env, episodes=1000, update_target_every=10, save_plots=False, plots_path='dqn_training_plots.png'):\n",
    "        for episode in tqdm(range(episodes), desc=\"Training\", unit='episode'):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action, _ = self.choose_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                self.replay()\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.epsilon_values.append(self.epsilon)\n",
    "\n",
    "            if (episode + 1) % update_target_every == 0:\n",
    "                self.update_target_model()\n",
    "\n",
    "        if save_plots:\n",
    "            self.save_plots(plots_path)\n",
    "    \n",
    "    def save_plots(self, plots_path):\n",
    "        plots_dir = os.path.dirname(plots_path)\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Rewards per episode\n",
    "        axs[0, 0].plot(self.episode_rewards)\n",
    "        axs[0, 0].set_title(\"Episode Rewards\")\n",
    "        axs[0, 0].set_xlabel(\"Episode\")\n",
    "        axs[0, 0].set_ylabel(\"Total Reward\")\n",
    "\n",
    "        # Loss per episode\n",
    "        axs[0, 1].plot(self.episode_losses)\n",
    "        axs[0, 1].set_title(\"Loss Over Training\")\n",
    "        axs[0, 1].set_xlabel(\"Episode\")\n",
    "        axs[0, 1].set_ylabel(\"Loss\")\n",
    "\n",
    "        # Epsilon decay\n",
    "        axs[1, 0].plot(self.epsilon_values)\n",
    "        axs[1, 0].set_title(\"Epsilon Decay\")\n",
    "        axs[1, 0].set_xlabel(\"Episode\")\n",
    "        axs[1, 0].set_ylabel(\"Epsilon Value\")\n",
    "\n",
    "        # Average Q-values\n",
    "        axs[1, 1].plot(self.average_q_values)\n",
    "        axs[1, 1].set_title(\"Average Q-Values\")\n",
    "        axs[1, 1].set_xlabel(\"Episode\")\n",
    "        axs[1, 1].set_ylabel(\"Average Q-Value\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plots_path)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "# env = FullObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "env = ParObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "if isinstance(env, FullObsSnakeEnv):\n",
    "    state_dim = env.observation_space.shape[0] * env.observation_space.shape[1] * env.observation_space.shape[2]\n",
    "else:\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "\n",
    "action_dim = env.action_space.n\n",
    "agent = DQNAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/300 [00:00<?, ?episode/s]/tmp/ipykernel_24247/1154075020.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  states = torch.FloatTensor(states).to(self.device)\n",
      "Training: 100%|██████████| 300/300 [01:09<00:00,  4.33episode/s]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 300\n",
    "agent.train(env, episodes=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent saved to ../../models/dqn/dqn_agent_par_300_10.pkl\n"
     ]
    }
   ],
   "source": [
    "environment = 'full 'if isinstance(env, FullObsSnakeEnv) else 'par'\n",
    "\n",
    "agent_name = f'dqn_agent_{environment}_{num_episodes}_{grid_size}.pkl'\n",
    "model_weights_dir = os.path.join('../..', 'models', 'dqn')\n",
    "os.makedirs(model_weights_dir, exist_ok=True)\n",
    "agent_path = os.path.join(model_weights_dir, agent_name)\n",
    "\n",
    "agent.save(agent_path)\n",
    "# agent = DQNAgent.load(agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:00<00:03, 27.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 14, Episode reward: 1085\n",
      "Snake length: 12, Episode reward: 883\n",
      "Snake length: 13, Episode reward: 957\n",
      "Snake length: 27, Episode reward: 2175\n",
      "Snake length: 26, Episode reward: 2108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:00<00:04, 20.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 26, Episode reward: 2157\n",
      "Snake length: 10, Episode reward: 742\n",
      "Snake length: 14, Episode reward: 1083\n",
      "Snake length: 45, Episode reward: 3785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:00<00:04, 17.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 19, Episode reward: 1471\n",
      "Snake length: 23, Episode reward: 1891\n",
      "Snake length: 21, Episode reward: 1664\n",
      "Snake length: 24, Episode reward: 1947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:00<00:05, 15.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 19, Episode reward: 1483\n",
      "Snake length: 35, Episode reward: 2902\n",
      "Snake length: 18, Episode reward: 1514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [00:01<00:06, 13.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 15, Episode reward: 1186\n",
      "Snake length: 53, Episode reward: 4559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:01<00:05, 13.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 23, Episode reward: 1892\n",
      "Snake length: 26, Episode reward: 2128\n",
      "Snake length: 15, Episode reward: 1139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:01<00:04, 15.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 34, Episode reward: 2837\n",
      "Snake length: 18, Episode reward: 1411\n",
      "Snake length: 25, Episode reward: 2012\n",
      "Snake length: 25, Episode reward: 2078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:01<00:04, 15.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 24, Episode reward: 1896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:01<00:04, 16.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 63, Episode reward: 5485\n",
      "Snake length: 2, Episode reward: 26\n",
      "Snake length: 15, Episode reward: 1124\n",
      "Snake length: 9, Episode reward: 655\n",
      "Snake length: 24, Episode reward: 1896\n",
      "Snake length: 29, Episode reward: 2398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:02<00:04, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 17, Episode reward: 1390\n",
      "Snake length: 28, Episode reward: 2305\n",
      "Snake length: 35, Episode reward: 2980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:02<00:04, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 27, Episode reward: 2227\n",
      "Snake length: 30, Episode reward: 2473\n",
      "Snake length: 23, Episode reward: 1857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:02<00:05, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 59, Episode reward: 5074\n",
      "Snake length: 35, Episode reward: 2911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:02<00:05, 11.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 35, Episode reward: 2959\n",
      "Snake length: 51, Episode reward: 4337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:03<00:04, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 42, Episode reward: 3509\n",
      "Snake length: 15, Episode reward: 1193\n",
      "Snake length: 16, Episode reward: 1220\n",
      "Snake length: 26, Episode reward: 2234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [00:03<00:04, 11.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 25, Episode reward: 2049\n",
      "Snake length: 28, Episode reward: 2340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:03<00:04, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 53, Episode reward: 4508\n",
      "Snake length: 18, Episode reward: 1433\n",
      "Snake length: 35, Episode reward: 2988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [00:03<00:03, 12.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 29, Episode reward: 2327\n",
      "Snake length: 23, Episode reward: 1825\n",
      "Snake length: 19, Episode reward: 1535\n",
      "Snake length: 35, Episode reward: 2904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [00:04<00:03, 12.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 38, Episode reward: 3181\n",
      "Snake length: 27, Episode reward: 2214\n",
      "Snake length: 24, Episode reward: 1924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:04<00:03, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 38, Episode reward: 3205\n",
      "Snake length: 48, Episode reward: 4135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [00:04<00:03, 11.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 28, Episode reward: 2264\n",
      "Snake length: 18, Episode reward: 1491\n",
      "Snake length: 25, Episode reward: 2065\n",
      "Snake length: 27, Episode reward: 2217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [00:05<00:02, 11.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 35, Episode reward: 2883\n",
      "Snake length: 34, Episode reward: 2809\n",
      "Snake length: 22, Episode reward: 1822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [00:05<00:02, 11.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 39, Episode reward: 3259\n",
      "Snake length: 39, Episode reward: 3221\n",
      "Snake length: 25, Episode reward: 2038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:05<00:01, 15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 12, Episode reward: 910\n",
      "Snake length: 15, Episode reward: 1155\n",
      "Snake length: 16, Episode reward: 1197\n",
      "Snake length: 32, Episode reward: 2699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [00:05<00:01, 14.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 28, Episode reward: 2305\n",
      "Snake length: 15, Episode reward: 1119\n",
      "Snake length: 22, Episode reward: 1727\n",
      "Snake length: 33, Episode reward: 2746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [00:06<00:01, 11.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 33, Episode reward: 2764\n",
      "Snake length: 60, Episode reward: 5127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:06<00:01, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 21, Episode reward: 1678\n",
      "Snake length: 13, Episode reward: 971\n",
      "Snake length: 40, Episode reward: 3440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [00:06<00:01, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 20, Episode reward: 1629\n",
      "Snake length: 17, Episode reward: 1324\n",
      "Snake length: 11, Episode reward: 798\n",
      "Snake length: 25, Episode reward: 1981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [00:06<00:00, 13.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 45, Episode reward: 3758\n",
      "Snake length: 48, Episode reward: 4071\n",
      "Snake length: 15, Episode reward: 1136\n",
      "Snake length: 15, Episode reward: 1173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [00:07<00:00, 14.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 22, Episode reward: 1799\n",
      "Snake length: 32, Episode reward: 2643\n",
      "Snake length: 24, Episode reward: 1938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [00:07<00:00, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 34, Episode reward: 2945\n",
      "Snake length: 21, Episode reward: 1699\n",
      "Snake length: 21, Episode reward: 1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 13.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 41, Episode reward: 3380\n",
      "Snake length: 30, Episode reward: 2456\n",
      "Snake length: 15, Episode reward: 1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'snake_lengths': [14,\n",
       "  12,\n",
       "  13,\n",
       "  27,\n",
       "  26,\n",
       "  26,\n",
       "  10,\n",
       "  14,\n",
       "  45,\n",
       "  19,\n",
       "  23,\n",
       "  21,\n",
       "  24,\n",
       "  19,\n",
       "  35,\n",
       "  18,\n",
       "  15,\n",
       "  53,\n",
       "  23,\n",
       "  26,\n",
       "  15,\n",
       "  34,\n",
       "  18,\n",
       "  25,\n",
       "  25,\n",
       "  24,\n",
       "  63,\n",
       "  2,\n",
       "  15,\n",
       "  9,\n",
       "  24,\n",
       "  29,\n",
       "  17,\n",
       "  28,\n",
       "  35,\n",
       "  27,\n",
       "  30,\n",
       "  23,\n",
       "  59,\n",
       "  35,\n",
       "  35,\n",
       "  51,\n",
       "  42,\n",
       "  15,\n",
       "  16,\n",
       "  26,\n",
       "  25,\n",
       "  28,\n",
       "  53,\n",
       "  18,\n",
       "  35,\n",
       "  29,\n",
       "  23,\n",
       "  19,\n",
       "  35,\n",
       "  38,\n",
       "  27,\n",
       "  24,\n",
       "  38,\n",
       "  48,\n",
       "  28,\n",
       "  18,\n",
       "  25,\n",
       "  27,\n",
       "  35,\n",
       "  34,\n",
       "  22,\n",
       "  39,\n",
       "  39,\n",
       "  25,\n",
       "  12,\n",
       "  15,\n",
       "  16,\n",
       "  32,\n",
       "  28,\n",
       "  15,\n",
       "  22,\n",
       "  33,\n",
       "  33,\n",
       "  60,\n",
       "  21,\n",
       "  13,\n",
       "  40,\n",
       "  20,\n",
       "  17,\n",
       "  11,\n",
       "  25,\n",
       "  45,\n",
       "  48,\n",
       "  15,\n",
       "  15,\n",
       "  22,\n",
       "  32,\n",
       "  24,\n",
       "  34,\n",
       "  21,\n",
       "  21,\n",
       "  41,\n",
       "  30,\n",
       "  15],\n",
       " 'episode_rewards': [1085,\n",
       "  883,\n",
       "  957,\n",
       "  2175,\n",
       "  2108,\n",
       "  2157,\n",
       "  742,\n",
       "  1083,\n",
       "  3785,\n",
       "  1471,\n",
       "  1891,\n",
       "  1664,\n",
       "  1947,\n",
       "  1483,\n",
       "  2902,\n",
       "  1514,\n",
       "  1186,\n",
       "  4559,\n",
       "  1892,\n",
       "  2128,\n",
       "  1139,\n",
       "  2837,\n",
       "  1411,\n",
       "  2012,\n",
       "  2078,\n",
       "  1896,\n",
       "  5485,\n",
       "  26,\n",
       "  1124,\n",
       "  655,\n",
       "  1896,\n",
       "  2398,\n",
       "  1390,\n",
       "  2305,\n",
       "  2980,\n",
       "  2227,\n",
       "  2473,\n",
       "  1857,\n",
       "  5074,\n",
       "  2911,\n",
       "  2959,\n",
       "  4337,\n",
       "  3509,\n",
       "  1193,\n",
       "  1220,\n",
       "  2234,\n",
       "  2049,\n",
       "  2340,\n",
       "  4508,\n",
       "  1433,\n",
       "  2988,\n",
       "  2327,\n",
       "  1825,\n",
       "  1535,\n",
       "  2904,\n",
       "  3181,\n",
       "  2214,\n",
       "  1924,\n",
       "  3205,\n",
       "  4135,\n",
       "  2264,\n",
       "  1491,\n",
       "  2065,\n",
       "  2217,\n",
       "  2883,\n",
       "  2809,\n",
       "  1822,\n",
       "  3259,\n",
       "  3221,\n",
       "  2038,\n",
       "  910,\n",
       "  1155,\n",
       "  1197,\n",
       "  2699,\n",
       "  2305,\n",
       "  1119,\n",
       "  1727,\n",
       "  2746,\n",
       "  2764,\n",
       "  5127,\n",
       "  1678,\n",
       "  971,\n",
       "  3440,\n",
       "  1629,\n",
       "  1324,\n",
       "  798,\n",
       "  1981,\n",
       "  3758,\n",
       "  4071,\n",
       "  1136,\n",
       "  1173,\n",
       "  1799,\n",
       "  2643,\n",
       "  1938,\n",
       "  2945,\n",
       "  1699,\n",
       "  1701,\n",
       "  3380,\n",
       "  2456,\n",
       "  1151]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(env, ParObsSnakeEnv):\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size, interact=False)\n",
    "\n",
    "model_metrics_dir = os.path.join('../..', 'artifacts', 'models_stats', 'dqn')\n",
    "os.makedirs(model_metrics_dir, exist_ok=True)\n",
    "\n",
    "train_metrics_name = f'dqn_train_metrics_{environment}_{num_episodes}_{grid_size}.png'\n",
    "train_metrics_path = os.path.join(model_metrics_dir, train_metrics_name)\n",
    "agent.save_plots(train_metrics_path)\n",
    "\n",
    "num_simulations = 100\n",
    "sim_metrics_name = f'dqn_sim_metrics_{environment}_{num_episodes}_{env.grid_size}_{num_simulations}.jsn'\n",
    "sim_metrics_path = os.path.join(model_metrics_dir, sim_metrics_name)\n",
    "compute_metrics(agent, env, sim_metrics_path, num_simulations=num_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -75\n"
     ]
    }
   ],
   "source": [
    "if isinstance(env, FullObsSnakeEnv):\n",
    "    env.interact = True\n",
    "else:\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size)\n",
    "    \n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = agent.choose_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
