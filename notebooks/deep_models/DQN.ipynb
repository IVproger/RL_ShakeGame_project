{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "from src.ParObsSnakeEnv import ParObsSnakeEnv\n",
    "from src.FullObsSnakeEnv import FullObsSnakeEnv\n",
    "from src.utils import compute_metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(input_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.fc3 = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, memory_size=10000, batch_size=64):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_model = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # Metrics for plotting\n",
    "        self.episode_rewards = []\n",
    "        self.episode_losses = []\n",
    "        self.epsilon_values = []\n",
    "        self.average_q_values = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1), None\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        state = state.reshape((1, -1))\n",
    "        with torch.no_grad():\n",
    "            # print(\">>>>\", state.shape)\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item(), None\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        states = states.reshape((self.batch_size, -1))\n",
    "        next_states = next_states.reshape((self.batch_size, -1))\n",
    "\n",
    "        # Compute current Q-values\n",
    "        q_values = self.model(states)\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        next_q_values = self.target_model(next_states).max(1)[0]\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # Update the Q-network\n",
    "        loss = self.criterion(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Log the loss\n",
    "        self.episode_losses.append(loss.item())\n",
    "\n",
    "        # Track average Q-value\n",
    "        self.average_q_values.append(q_values.mean().item())\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves the entire agent to a file with all tensors moved to CPU.\"\"\"\n",
    "        # Save the current device\n",
    "        current_device = next(self.model.parameters()).device\n",
    "\n",
    "        # Move models to CPU\n",
    "        self.model.to('cpu')\n",
    "        self.target_model.to('cpu')\n",
    "\n",
    "        state = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'target_model_state_dict': self.target_model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'hyperparameters': {\n",
    "                'state_dim': self.state_dim,\n",
    "                'action_dim': self.action_dim,\n",
    "                'gamma': self.gamma,\n",
    "                'epsilon': self.epsilon,\n",
    "                'epsilon_decay': self.epsilon_decay,\n",
    "                'epsilon_min': self.epsilon_min,\n",
    "                'batch_size': self.batch_size,\n",
    "            },\n",
    "            'memory': list(self.memory),  # Convert deque to list\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        print(f\"Agent saved to {filename}\")\n",
    "\n",
    "        # Move the models back to their original device\n",
    "        self.model.to(current_device)\n",
    "        self.target_model.to(current_device)\n",
    "    \n",
    "    def load(cls, filename, lr=0.001, device=None):\n",
    "        \"\"\"Loads the agent from a file.\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "\n",
    "        # Determine the device to load the model onto\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Recreate the agent\n",
    "        agent = cls(\n",
    "            state['hyperparameters']['state_dim'],\n",
    "            state['hyperparameters']['action_dim'],\n",
    "            lr=lr,\n",
    "            gamma=state['hyperparameters']['gamma'],\n",
    "            epsilon=state['hyperparameters']['epsilon'],\n",
    "            epsilon_decay=state['hyperparameters']['epsilon_decay'],\n",
    "            epsilon_min=state['hyperparameters']['epsilon_min'],\n",
    "            batch_size=state['hyperparameters']['batch_size'],\n",
    "        )\n",
    "\n",
    "        # Restore the agent's state\n",
    "        agent.model.load_state_dict(state['model_state_dict'])\n",
    "        agent.target_model.load_state_dict(state['target_model_state_dict'])\n",
    "        agent.optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        agent.memory = deque(state['memory'], maxlen=len(state['memory']))\n",
    "\n",
    "        # Move the model to the specified device\n",
    "        agent.model = agent.model.to(device)\n",
    "        agent.target_model = agent.target_model.to(device)\n",
    "\n",
    "        print(f\"Agent loaded from {filename} onto device: {device}\")\n",
    "        return agent\n",
    "\n",
    "    # Train the agent\n",
    "    @classmethod\n",
    "    def train(self, env, episodes=1000, update_target_every=10, save_plots=False, plots_path='dqn_training_plots.png'):\n",
    "        for episode in tqdm(range(episodes), desc=\"Training\", unit='episode'):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action, _ = self.choose_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                self.replay()\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.epsilon_values.append(self.epsilon)\n",
    "\n",
    "            if (episode + 1) % update_target_every == 0:\n",
    "                self.update_target_model()\n",
    "\n",
    "        if save_plots:\n",
    "            self.save_plots(plots_path)\n",
    "    \n",
    "    def save_plots(self, plots_path):\n",
    "        plots_dir = os.path.dirname(plots_path)\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Rewards per episode\n",
    "        axs[0, 0].plot(self.episode_rewards)\n",
    "        axs[0, 0].set_title(\"Episode Rewards\")\n",
    "        axs[0, 0].set_xlabel(\"Episode\")\n",
    "        axs[0, 0].set_ylabel(\"Total Reward\")\n",
    "\n",
    "        # Loss per episode\n",
    "        axs[0, 1].plot(self.episode_losses)\n",
    "        axs[0, 1].set_title(\"Loss Over Training\")\n",
    "        axs[0, 1].set_xlabel(\"Episode\")\n",
    "        axs[0, 1].set_ylabel(\"Loss\")\n",
    "\n",
    "        # Epsilon decay\n",
    "        axs[1, 0].plot(self.epsilon_values)\n",
    "        axs[1, 0].set_title(\"Epsilon Decay\")\n",
    "        axs[1, 0].set_xlabel(\"Episode\")\n",
    "        axs[1, 0].set_ylabel(\"Epsilon Value\")\n",
    "\n",
    "        # Average Q-values\n",
    "        axs[1, 1].plot(self.average_q_values)\n",
    "        axs[1, 1].set_title(\"Average Q-Values\")\n",
    "        axs[1, 1].set_xlabel(\"Episode\")\n",
    "        axs[1, 1].set_ylabel(\"Average Q-Value\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plots_path)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "# env = FullObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "env = ParObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "if isinstance(env, FullObsSnakeEnv):\n",
    "    state_dim = env.observation_space.shape[0] * env.observation_space.shape[1] * env.observation_space.shape[2]\n",
    "else:\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "\n",
    "action_dim = env.action_space.n\n",
    "agent = DQNAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/300 [00:00<?, ?episode/s]/tmp/ipykernel_32573/1027833410.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  states = torch.FloatTensor(states).to(self.device)\n",
      "Training: 100%|██████████| 300/300 [01:03<00:00,  4.70episode/s]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 300\n",
    "agent.train(env, episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent saved to ../../models/dqn/dqn_agent_par_300_10.pkl\n"
     ]
    }
   ],
   "source": [
    "environment = 'full 'if isinstance(env, FullObsSnakeEnv) else 'par'\n",
    "\n",
    "agent_name = f'dqn_agent_{environment}_{num_episodes}_{grid_size}.pkl'\n",
    "model_weights_dir = os.path.join('../..', 'models', 'dqn')\n",
    "os.makedirs(model_weights_dir, exist_ok=True)\n",
    "agent_path = os.path.join(model_weights_dir, agent_name)\n",
    "\n",
    "agent.save(agent_path)\n",
    "agent = DQNAgent.load(agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:07, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 32, Episode reward: 2698\n",
      "Snake length: 24, Episode reward: 1979\n",
      "Snake length: 22, Episode reward: 1726\n",
      "Snake length: 19, Episode reward: 1542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:00<00:06, 15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 34, Episode reward: 2845\n",
      "Snake length: 20, Episode reward: 1592\n",
      "Snake length: 26, Episode reward: 2120\n",
      "Snake length: 27, Episode reward: 2172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:00<00:06, 13.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 35, Episode reward: 2914\n",
      "Snake length: 30, Episode reward: 2443\n",
      "Snake length: 38, Episode reward: 3284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:01<00:08, 10.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 38, Episode reward: 3188\n",
      "Snake length: 42, Episode reward: 3535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:01<00:07, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 29, Episode reward: 2410\n",
      "Snake length: 22, Episode reward: 1759\n",
      "Snake length: 43, Episode reward: 3618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:01<00:06, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 31, Episode reward: 2629\n",
      "Snake length: 21, Episode reward: 1661\n",
      "Snake length: 23, Episode reward: 1869\n",
      "Snake length: 23, Episode reward: 1878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:01<00:06, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 30, Episode reward: 2434\n",
      "Snake length: 13, Episode reward: 991\n",
      "Snake length: 42, Episode reward: 3622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:01<00:05, 12.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 27, Episode reward: 2177\n",
      "Snake length: 35, Episode reward: 2941\n",
      "Snake length: 33, Episode reward: 2722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:02<00:06, 11.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 38, Episode reward: 3187\n",
      "Snake length: 39, Episode reward: 3283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:02<00:06, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 33, Episode reward: 2754\n",
      "Snake length: 29, Episode reward: 2399\n",
      "Snake length: 39, Episode reward: 3297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:02<00:05, 11.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 28, Episode reward: 2271\n",
      "Snake length: 25, Episode reward: 2047\n",
      "Snake length: 18, Episode reward: 1460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:03<00:05, 11.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 41, Episode reward: 3483\n",
      "Snake length: 22, Episode reward: 1818\n",
      "Snake length: 32, Episode reward: 2680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:03<00:04, 13.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 30, Episode reward: 2466\n",
      "Snake length: 27, Episode reward: 2231\n",
      "Snake length: 25, Episode reward: 2008\n",
      "Snake length: 15, Episode reward: 1216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:03<00:04, 14.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 29, Episode reward: 2358\n",
      "Snake length: 23, Episode reward: 1831\n",
      "Snake length: 41, Episode reward: 3466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:03<00:04, 11.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 33, Episode reward: 2804\n",
      "Snake length: 31, Episode reward: 2540\n",
      "Snake length: 26, Episode reward: 2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:04<00:03, 13.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 25, Episode reward: 2028\n",
      "Snake length: 10, Episode reward: 710\n",
      "Snake length: 31, Episode reward: 2587\n",
      "Snake length: 28, Episode reward: 2317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [00:04<00:03, 13.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 18, Episode reward: 1399\n",
      "Snake length: 21, Episode reward: 1668\n",
      "Snake length: 36, Episode reward: 2972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [00:04<00:03, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 33, Episode reward: 2752\n",
      "Snake length: 29, Episode reward: 2456\n",
      "Snake length: 33, Episode reward: 2808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:04<00:02, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 13, Episode reward: 957\n",
      "Snake length: 29, Episode reward: 2454\n",
      "Snake length: 22, Episode reward: 1795\n",
      "Snake length: 20, Episode reward: 1554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:04<00:02, 14.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 17, Episode reward: 1306\n",
      "Snake length: 38, Episode reward: 3228\n",
      "Snake length: 20, Episode reward: 1616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [00:05<00:02, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 30, Episode reward: 2511\n",
      "Snake length: 39, Episode reward: 3354\n",
      "Snake length: 28, Episode reward: 2282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [00:05<00:01, 17.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 5, Episode reward: 313\n",
      "Snake length: 26, Episode reward: 2089\n",
      "Snake length: 3, Episode reward: 101\n",
      "Snake length: 17, Episode reward: 1317\n",
      "Snake length: 16, Episode reward: 1209\n",
      "Snake length: 22, Episode reward: 1758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [00:05<00:01, 14.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 36, Episode reward: 3003\n",
      "Snake length: 37, Episode reward: 3180\n",
      "Snake length: 26, Episode reward: 2140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [00:05<00:01, 14.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 25, Episode reward: 2114\n",
      "Snake length: 30, Episode reward: 2480\n",
      "Snake length: 28, Episode reward: 2350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [00:06<00:01, 13.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 29, Episode reward: 2419\n",
      "Snake length: 40, Episode reward: 3408\n",
      "Snake length: 16, Episode reward: 1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [00:06<00:00, 17.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 21, Episode reward: 1682\n",
      "Snake length: 3, Episode reward: 122\n",
      "Snake length: 7, Episode reward: 460\n",
      "Snake length: 20, Episode reward: 1620\n",
      "Snake length: 38, Episode reward: 3161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [00:06<00:00, 14.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 15, Episode reward: 1164\n",
      "Snake length: 38, Episode reward: 3229\n",
      "Snake length: 23, Episode reward: 1898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [00:06<00:00, 14.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 23, Episode reward: 1874\n",
      "Snake length: 34, Episode reward: 2876\n",
      "Snake length: 25, Episode reward: 2032\n",
      "Snake length: 8, Episode reward: 546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [00:07<00:00, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 22, Episode reward: 1779\n",
      "Snake length: 36, Episode reward: 3017\n",
      "Snake length: 24, Episode reward: 1967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [00:07<00:00, 13.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 34, Episode reward: 2871\n",
      "Snake length: 20, Episode reward: 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 46, Episode reward: 3921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'snake_lengths': [32,\n",
       "  24,\n",
       "  22,\n",
       "  19,\n",
       "  34,\n",
       "  20,\n",
       "  26,\n",
       "  27,\n",
       "  35,\n",
       "  30,\n",
       "  38,\n",
       "  38,\n",
       "  42,\n",
       "  29,\n",
       "  22,\n",
       "  43,\n",
       "  31,\n",
       "  21,\n",
       "  23,\n",
       "  23,\n",
       "  30,\n",
       "  13,\n",
       "  42,\n",
       "  27,\n",
       "  35,\n",
       "  33,\n",
       "  38,\n",
       "  39,\n",
       "  33,\n",
       "  29,\n",
       "  39,\n",
       "  28,\n",
       "  25,\n",
       "  18,\n",
       "  41,\n",
       "  22,\n",
       "  32,\n",
       "  30,\n",
       "  27,\n",
       "  25,\n",
       "  15,\n",
       "  29,\n",
       "  23,\n",
       "  41,\n",
       "  33,\n",
       "  31,\n",
       "  26,\n",
       "  25,\n",
       "  10,\n",
       "  31,\n",
       "  28,\n",
       "  18,\n",
       "  21,\n",
       "  36,\n",
       "  33,\n",
       "  29,\n",
       "  33,\n",
       "  13,\n",
       "  29,\n",
       "  22,\n",
       "  20,\n",
       "  17,\n",
       "  38,\n",
       "  20,\n",
       "  30,\n",
       "  39,\n",
       "  28,\n",
       "  5,\n",
       "  26,\n",
       "  3,\n",
       "  17,\n",
       "  16,\n",
       "  22,\n",
       "  36,\n",
       "  37,\n",
       "  26,\n",
       "  25,\n",
       "  30,\n",
       "  28,\n",
       "  29,\n",
       "  40,\n",
       "  16,\n",
       "  21,\n",
       "  3,\n",
       "  7,\n",
       "  20,\n",
       "  38,\n",
       "  15,\n",
       "  38,\n",
       "  23,\n",
       "  23,\n",
       "  34,\n",
       "  25,\n",
       "  8,\n",
       "  22,\n",
       "  36,\n",
       "  24,\n",
       "  34,\n",
       "  20,\n",
       "  46],\n",
       " 'episode_rewards': [2698,\n",
       "  1979,\n",
       "  1726,\n",
       "  1542,\n",
       "  2845,\n",
       "  1592,\n",
       "  2120,\n",
       "  2172,\n",
       "  2914,\n",
       "  2443,\n",
       "  3284,\n",
       "  3188,\n",
       "  3535,\n",
       "  2410,\n",
       "  1759,\n",
       "  3618,\n",
       "  2629,\n",
       "  1661,\n",
       "  1869,\n",
       "  1878,\n",
       "  2434,\n",
       "  991,\n",
       "  3622,\n",
       "  2177,\n",
       "  2941,\n",
       "  2722,\n",
       "  3187,\n",
       "  3283,\n",
       "  2754,\n",
       "  2399,\n",
       "  3297,\n",
       "  2271,\n",
       "  2047,\n",
       "  1460,\n",
       "  3483,\n",
       "  1818,\n",
       "  2680,\n",
       "  2466,\n",
       "  2231,\n",
       "  2008,\n",
       "  1216,\n",
       "  2358,\n",
       "  1831,\n",
       "  3466,\n",
       "  2804,\n",
       "  2540,\n",
       "  2200,\n",
       "  2028,\n",
       "  710,\n",
       "  2587,\n",
       "  2317,\n",
       "  1399,\n",
       "  1668,\n",
       "  2972,\n",
       "  2752,\n",
       "  2456,\n",
       "  2808,\n",
       "  957,\n",
       "  2454,\n",
       "  1795,\n",
       "  1554,\n",
       "  1306,\n",
       "  3228,\n",
       "  1616,\n",
       "  2511,\n",
       "  3354,\n",
       "  2282,\n",
       "  313,\n",
       "  2089,\n",
       "  101,\n",
       "  1317,\n",
       "  1209,\n",
       "  1758,\n",
       "  3003,\n",
       "  3180,\n",
       "  2140,\n",
       "  2114,\n",
       "  2480,\n",
       "  2350,\n",
       "  2419,\n",
       "  3408,\n",
       "  1284,\n",
       "  1682,\n",
       "  122,\n",
       "  460,\n",
       "  1620,\n",
       "  3161,\n",
       "  1164,\n",
       "  3229,\n",
       "  1898,\n",
       "  1874,\n",
       "  2876,\n",
       "  2032,\n",
       "  546,\n",
       "  1779,\n",
       "  3017,\n",
       "  1967,\n",
       "  2871,\n",
       "  1600,\n",
       "  3921]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(env, ParObsSnakeEnv):\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size, interact=False)\n",
    "\n",
    "model_metrics_dir = os.path.join('../..', 'artifacts', 'models_stats', 'dqn')\n",
    "os.makedirs(model_metrics_dir, exist_ok=True)\n",
    "\n",
    "train_metrics_name = f'dqn_train_metrics_{environment}_{num_episodes}_{grid_size}.png'\n",
    "train_metrics_path = os.path.join(model_metrics_dir, train_metrics_name)\n",
    "agent.save_plots(train_metrics_path)\n",
    "\n",
    "num_simulations = 100\n",
    "sim_metrics_name = f'dqn_sim_metrics_{environment}_{num_episodes}_{env.grid_size}_{num_simulations}.json'\n",
    "sim_metrics_path = os.path.join(model_metrics_dir, sim_metrics_name)\n",
    "compute_metrics(agent, env, sim_metrics_path, num_simulations=num_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if isinstance(env, FullObsSnakeEnv):\n",
    "    env.interact = True\n",
    "else:\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size)\n",
    "    \n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = agent.choose_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DQNAgent' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(state_dim, action_dim)\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../models/dqn/dqn_agent_par_300_10.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 123\u001b[0m, in \u001b[0;36mDQNAgent.load\u001b[0;34m(cls, filename, lr, device)\u001b[0m\n\u001b[1;32m    120\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Recreate the agent\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyperparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyperparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyperparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyperparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepsilon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyperparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepsilon_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyperparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepsilon_min\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyperparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Restore the agent's state\u001b[39;00m\n\u001b[1;32m    135\u001b[0m agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DQNAgent' object is not callable"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_dim, action_dim)\n",
    "agent.load(\"../../models/dqn/dqn_agent_par_300_10.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
