{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "from src.ParObsSnakeEnv import SnakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "        super(DQN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(input_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.fc3 = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.9995, epsilon_min=0.01, memory_size=10000, batch_size=64):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_model = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        # Compute current Q-values\n",
    "        q_values = self.model(states)\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        next_q_values = self.target_model(next_states).max(1)[0]\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # Update the Q-network\n",
    "        loss = self.criterion(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "# Train the agent\n",
    "def train_dqn(env, agent, episodes=1000, update_target_every=10):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        agent.update_target_model()\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "        if (episode + 1) % update_target_every == 0:\n",
    "            print(\"Updated target model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/600, Total Reward: -79, Epsilon: 1.000\n",
      "Episode 2/600, Total Reward: 10, Epsilon: 1.000\n",
      "Episode 3/600, Total Reward: -77, Epsilon: 1.000\n",
      "Episode 4/600, Total Reward: -75, Epsilon: 1.000\n",
      "Episode 5/600, Total Reward: -79, Epsilon: 0.996\n",
      "Episode 6/600, Total Reward: -77, Epsilon: 0.995\n",
      "Episode 7/600, Total Reward: -72, Epsilon: 0.977\n",
      "Episode 8/600, Total Reward: -76, Epsilon: 0.976\n",
      "Episode 9/600, Total Reward: 6, Epsilon: 0.967\n",
      "Episode 10/600, Total Reward: -78, Epsilon: 0.956\n",
      "Updated target model.\n",
      "Episode 11/600, Total Reward: -75, Epsilon: 0.956\n",
      "Episode 12/600, Total Reward: -75, Epsilon: 0.955\n",
      "Episode 13/600, Total Reward: -72, Epsilon: 0.944\n",
      "Episode 14/600, Total Reward: -75, Epsilon: 0.941\n",
      "Episode 15/600, Total Reward: -76, Epsilon: 0.936\n",
      "Episode 16/600, Total Reward: 84, Epsilon: 0.919\n",
      "Episode 17/600, Total Reward: -1, Epsilon: 0.910\n",
      "Episode 18/600, Total Reward: -82, Epsilon: 0.892\n",
      "Episode 19/600, Total Reward: -75, Epsilon: 0.890\n",
      "Episode 20/600, Total Reward: 14, Epsilon: 0.880\n",
      "Updated target model.\n",
      "Episode 21/600, Total Reward: -79, Epsilon: 0.873\n",
      "Episode 22/600, Total Reward: -69, Epsilon: 0.854\n",
      "Episode 23/600, Total Reward: -79, Epsilon: 0.851\n",
      "Episode 24/600, Total Reward: -75, Epsilon: 0.850\n",
      "Episode 25/600, Total Reward: -75, Epsilon: 0.830\n",
      "Episode 26/600, Total Reward: -75, Epsilon: 0.826\n",
      "Episode 27/600, Total Reward: -76, Epsilon: 0.825\n",
      "Episode 28/600, Total Reward: -80, Epsilon: 0.818\n",
      "Episode 29/600, Total Reward: -75, Epsilon: 0.818\n",
      "Episode 30/600, Total Reward: -80, Epsilon: 0.811\n",
      "Updated target model.\n",
      "Episode 31/600, Total Reward: -77, Epsilon: 0.805\n",
      "Episode 32/600, Total Reward: -71, Epsilon: 0.798\n",
      "Episode 33/600, Total Reward: 6, Epsilon: 0.787\n",
      "Episode 34/600, Total Reward: -2, Epsilon: 0.780\n",
      "Episode 35/600, Total Reward: -69, Epsilon: 0.773\n",
      "Episode 36/600, Total Reward: -77, Epsilon: 0.758\n",
      "Episode 37/600, Total Reward: -75, Epsilon: 0.757\n",
      "Episode 38/600, Total Reward: -68, Epsilon: 0.749\n",
      "Episode 39/600, Total Reward: -72, Epsilon: 0.746\n",
      "Episode 40/600, Total Reward: -77, Epsilon: 0.745\n",
      "Updated target model.\n",
      "Episode 41/600, Total Reward: -73, Epsilon: 0.741\n",
      "Episode 42/600, Total Reward: 9, Epsilon: 0.734\n",
      "Episode 43/600, Total Reward: -75, Epsilon: 0.733\n",
      "Episode 44/600, Total Reward: 11, Epsilon: 0.724\n",
      "Episode 45/600, Total Reward: 3, Epsilon: 0.709\n",
      "Episode 46/600, Total Reward: -72, Epsilon: 0.707\n",
      "Episode 47/600, Total Reward: -75, Epsilon: 0.706\n",
      "Episode 48/600, Total Reward: -70, Epsilon: 0.699\n",
      "Episode 49/600, Total Reward: -75, Epsilon: 0.696\n",
      "Episode 50/600, Total Reward: -74, Epsilon: 0.696\n",
      "Updated target model.\n",
      "Episode 51/600, Total Reward: -76, Epsilon: 0.693\n",
      "Episode 52/600, Total Reward: -72, Epsilon: 0.690\n",
      "Episode 53/600, Total Reward: -77, Epsilon: 0.689\n",
      "Episode 54/600, Total Reward: 3, Epsilon: 0.686\n",
      "Episode 55/600, Total Reward: -74, Epsilon: 0.683\n",
      "Episode 56/600, Total Reward: -64, Epsilon: 0.673\n",
      "Episode 57/600, Total Reward: 5, Epsilon: 0.669\n",
      "Episode 58/600, Total Reward: -78, Epsilon: 0.662\n",
      "Episode 59/600, Total Reward: -73, Epsilon: 0.656\n",
      "Episode 60/600, Total Reward: -74, Epsilon: 0.656\n",
      "Updated target model.\n",
      "Episode 61/600, Total Reward: -72, Epsilon: 0.647\n",
      "Episode 62/600, Total Reward: -69, Epsilon: 0.643\n",
      "Episode 63/600, Total Reward: -72, Epsilon: 0.635\n",
      "Episode 64/600, Total Reward: -74, Epsilon: 0.635\n",
      "Episode 65/600, Total Reward: -76, Epsilon: 0.634\n",
      "Episode 66/600, Total Reward: -74, Epsilon: 0.633\n",
      "Episode 67/600, Total Reward: 82, Epsilon: 0.629\n",
      "Episode 68/600, Total Reward: -74, Epsilon: 0.626\n",
      "Episode 69/600, Total Reward: -73, Epsilon: 0.625\n",
      "Episode 70/600, Total Reward: -77, Epsilon: 0.624\n",
      "Updated target model.\n",
      "Episode 71/600, Total Reward: -73, Epsilon: 0.623\n",
      "Episode 72/600, Total Reward: -68, Epsilon: 0.604\n",
      "Episode 73/600, Total Reward: 4, Epsilon: 0.601\n",
      "Episode 74/600, Total Reward: -75, Epsilon: 0.601\n",
      "Episode 75/600, Total Reward: -65, Epsilon: 0.597\n",
      "Episode 76/600, Total Reward: -74, Epsilon: 0.592\n",
      "Episode 77/600, Total Reward: -71, Epsilon: 0.590\n",
      "Episode 78/600, Total Reward: -75, Epsilon: 0.589\n",
      "Episode 79/600, Total Reward: 4, Epsilon: 0.584\n",
      "Episode 80/600, Total Reward: 89, Epsilon: 0.574\n",
      "Updated target model.\n",
      "Episode 81/600, Total Reward: -77, Epsilon: 0.571\n",
      "Episode 82/600, Total Reward: -77, Epsilon: 0.570\n",
      "Episode 83/600, Total Reward: -75, Epsilon: 0.570\n",
      "Episode 84/600, Total Reward: -74, Epsilon: 0.569\n",
      "Episode 85/600, Total Reward: -71, Epsilon: 0.566\n",
      "Episode 86/600, Total Reward: -75, Epsilon: 0.566\n",
      "Episode 87/600, Total Reward: -76, Epsilon: 0.565\n",
      "Episode 88/600, Total Reward: 83, Epsilon: 0.562\n",
      "Episode 89/600, Total Reward: 86, Epsilon: 0.555\n",
      "Episode 90/600, Total Reward: -77, Epsilon: 0.548\n",
      "Updated target model.\n",
      "Episode 91/600, Total Reward: -80, Epsilon: 0.544\n",
      "Episode 92/600, Total Reward: 94, Epsilon: 0.537\n",
      "Episode 93/600, Total Reward: 169, Epsilon: 0.528\n",
      "Episode 94/600, Total Reward: -73, Epsilon: 0.524\n",
      "Episode 95/600, Total Reward: 20, Epsilon: 0.516\n",
      "Episode 96/600, Total Reward: -77, Epsilon: 0.515\n",
      "Episode 97/600, Total Reward: -75, Epsilon: 0.515\n",
      "Episode 98/600, Total Reward: -81, Epsilon: 0.513\n",
      "Episode 99/600, Total Reward: 169, Epsilon: 0.501\n",
      "Episode 100/600, Total Reward: -77, Epsilon: 0.500\n",
      "Updated target model.\n",
      "Episode 101/600, Total Reward: 14, Epsilon: 0.495\n",
      "Episode 102/600, Total Reward: -69, Epsilon: 0.492\n",
      "Episode 103/600, Total Reward: 10, Epsilon: 0.487\n",
      "Episode 104/600, Total Reward: 9, Epsilon: 0.480\n",
      "Episode 105/600, Total Reward: -72, Epsilon: 0.476\n",
      "Episode 106/600, Total Reward: -72, Epsilon: 0.473\n",
      "Episode 107/600, Total Reward: -71, Epsilon: 0.469\n",
      "Episode 108/600, Total Reward: 7, Epsilon: 0.466\n",
      "Episode 109/600, Total Reward: -76, Epsilon: 0.462\n",
      "Episode 110/600, Total Reward: -73, Epsilon: 0.459\n",
      "Updated target model.\n",
      "Episode 111/600, Total Reward: -74, Epsilon: 0.459\n",
      "Episode 112/600, Total Reward: -75, Epsilon: 0.458\n",
      "Episode 113/600, Total Reward: 15, Epsilon: 0.452\n",
      "Episode 114/600, Total Reward: -68, Epsilon: 0.449\n",
      "Episode 115/600, Total Reward: 181, Epsilon: 0.422\n",
      "Episode 116/600, Total Reward: 255, Epsilon: 0.404\n",
      "Episode 117/600, Total Reward: -81, Epsilon: 0.402\n",
      "Episode 118/600, Total Reward: -74, Epsilon: 0.401\n",
      "Episode 119/600, Total Reward: 4, Epsilon: 0.397\n",
      "Episode 120/600, Total Reward: -71, Epsilon: 0.394\n",
      "Updated target model.\n",
      "Episode 121/600, Total Reward: 9, Epsilon: 0.392\n",
      "Episode 122/600, Total Reward: -74, Epsilon: 0.391\n",
      "Episode 123/600, Total Reward: -74, Epsilon: 0.388\n",
      "Episode 124/600, Total Reward: -72, Epsilon: 0.385\n",
      "Episode 125/600, Total Reward: 95, Epsilon: 0.371\n",
      "Episode 126/600, Total Reward: -75, Epsilon: 0.371\n",
      "Episode 127/600, Total Reward: -75, Epsilon: 0.371\n",
      "Episode 128/600, Total Reward: 92, Epsilon: 0.352\n",
      "Episode 129/600, Total Reward: -79, Epsilon: 0.351\n",
      "Episode 130/600, Total Reward: -70, Epsilon: 0.348\n",
      "Updated target model.\n",
      "Episode 131/600, Total Reward: -63, Epsilon: 0.345\n",
      "Episode 132/600, Total Reward: -71, Epsilon: 0.344\n",
      "Episode 133/600, Total Reward: 13, Epsilon: 0.338\n",
      "Episode 134/600, Total Reward: -78, Epsilon: 0.337\n",
      "Episode 135/600, Total Reward: -74, Epsilon: 0.335\n",
      "Episode 136/600, Total Reward: -73, Epsilon: 0.334\n",
      "Episode 137/600, Total Reward: -7, Epsilon: 0.325\n",
      "Episode 138/600, Total Reward: -74, Epsilon: 0.323\n",
      "Episode 139/600, Total Reward: -75, Epsilon: 0.323\n",
      "Episode 140/600, Total Reward: 81, Epsilon: 0.315\n",
      "Updated target model.\n",
      "Episode 141/600, Total Reward: 18, Epsilon: 0.311\n",
      "Episode 142/600, Total Reward: 172, Epsilon: 0.306\n",
      "Episode 143/600, Total Reward: 6, Epsilon: 0.304\n",
      "Episode 144/600, Total Reward: -82, Epsilon: 0.302\n",
      "Episode 145/600, Total Reward: -74, Epsilon: 0.301\n",
      "Episode 146/600, Total Reward: 10, Epsilon: 0.294\n",
      "Episode 147/600, Total Reward: 89, Epsilon: 0.291\n",
      "Episode 148/600, Total Reward: 1, Epsilon: 0.289\n",
      "Episode 149/600, Total Reward: 84, Epsilon: 0.287\n",
      "Episode 150/600, Total Reward: -2, Epsilon: 0.284\n",
      "Updated target model.\n",
      "Episode 151/600, Total Reward: -69, Epsilon: 0.281\n",
      "Episode 152/600, Total Reward: 332, Epsilon: 0.274\n",
      "Episode 153/600, Total Reward: 94, Epsilon: 0.265\n",
      "Episode 154/600, Total Reward: 253, Epsilon: 0.258\n",
      "Episode 155/600, Total Reward: 86, Epsilon: 0.254\n",
      "Episode 156/600, Total Reward: 6, Epsilon: 0.250\n",
      "Episode 157/600, Total Reward: 15, Epsilon: 0.248\n",
      "Episode 158/600, Total Reward: 574, Epsilon: 0.237\n",
      "Episode 159/600, Total Reward: 324, Epsilon: 0.230\n",
      "Episode 160/600, Total Reward: 83, Epsilon: 0.228\n",
      "Updated target model.\n",
      "Episode 161/600, Total Reward: 170, Epsilon: 0.221\n",
      "Episode 162/600, Total Reward: 10, Epsilon: 0.219\n",
      "Episode 163/600, Total Reward: -77, Epsilon: 0.219\n",
      "Episode 164/600, Total Reward: 6, Epsilon: 0.217\n",
      "Episode 165/600, Total Reward: 12, Epsilon: 0.216\n",
      "Episode 166/600, Total Reward: -71, Epsilon: 0.215\n",
      "Episode 167/600, Total Reward: -67, Epsilon: 0.213\n",
      "Episode 168/600, Total Reward: 166, Epsilon: 0.211\n",
      "Episode 169/600, Total Reward: 97, Epsilon: 0.207\n",
      "Episode 170/600, Total Reward: -74, Epsilon: 0.207\n",
      "Updated target model.\n",
      "Episode 171/600, Total Reward: -78, Epsilon: 0.205\n",
      "Episode 172/600, Total Reward: -71, Epsilon: 0.204\n",
      "Episode 173/600, Total Reward: 88, Epsilon: 0.202\n",
      "Episode 174/600, Total Reward: 91, Epsilon: 0.199\n",
      "Episode 175/600, Total Reward: 15, Epsilon: 0.197\n",
      "Episode 176/600, Total Reward: -78, Epsilon: 0.196\n",
      "Episode 177/600, Total Reward: 257, Epsilon: 0.193\n",
      "Episode 178/600, Total Reward: 170, Epsilon: 0.187\n",
      "Episode 179/600, Total Reward: 167, Epsilon: 0.183\n",
      "Episode 180/600, Total Reward: -75, Epsilon: 0.183\n",
      "Updated target model.\n",
      "Episode 181/600, Total Reward: 567, Epsilon: 0.178\n",
      "Episode 182/600, Total Reward: 176, Epsilon: 0.176\n",
      "Episode 183/600, Total Reward: 5, Epsilon: 0.174\n",
      "Episode 184/600, Total Reward: 89, Epsilon: 0.172\n",
      "Episode 185/600, Total Reward: 420, Epsilon: 0.166\n",
      "Episode 186/600, Total Reward: 89, Epsilon: 0.163\n",
      "Episode 187/600, Total Reward: 93, Epsilon: 0.161\n",
      "Episode 188/600, Total Reward: 9, Epsilon: 0.159\n",
      "Episode 189/600, Total Reward: 178, Epsilon: 0.156\n",
      "Episode 190/600, Total Reward: 5, Epsilon: 0.154\n",
      "Updated target model.\n",
      "Episode 191/600, Total Reward: 167, Epsilon: 0.152\n",
      "Episode 192/600, Total Reward: 93, Epsilon: 0.150\n",
      "Episode 193/600, Total Reward: 7, Epsilon: 0.149\n",
      "Episode 194/600, Total Reward: 168, Epsilon: 0.147\n",
      "Episode 195/600, Total Reward: 344, Epsilon: 0.143\n",
      "Episode 196/600, Total Reward: -64, Epsilon: 0.142\n",
      "Episode 197/600, Total Reward: 349, Epsilon: 0.135\n",
      "Episode 198/600, Total Reward: -73, Epsilon: 0.133\n",
      "Episode 199/600, Total Reward: 177, Epsilon: 0.129\n",
      "Episode 200/600, Total Reward: 496, Epsilon: 0.123\n",
      "Updated target model.\n",
      "Episode 201/600, Total Reward: 347, Epsilon: 0.120\n",
      "Episode 202/600, Total Reward: 575, Epsilon: 0.112\n",
      "Episode 203/600, Total Reward: 16, Epsilon: 0.111\n",
      "Episode 204/600, Total Reward: -70, Epsilon: 0.110\n",
      "Episode 205/600, Total Reward: 93, Epsilon: 0.109\n",
      "Episode 206/600, Total Reward: 750, Epsilon: 0.104\n",
      "Episode 207/600, Total Reward: 518, Epsilon: 0.099\n",
      "Episode 208/600, Total Reward: 826, Epsilon: 0.095\n",
      "Episode 209/600, Total Reward: 434, Epsilon: 0.092\n",
      "Episode 210/600, Total Reward: 166, Epsilon: 0.091\n",
      "Updated target model.\n",
      "Episode 211/600, Total Reward: 994, Epsilon: 0.084\n",
      "Episode 212/600, Total Reward: 424, Epsilon: 0.082\n",
      "Episode 213/600, Total Reward: -72, Epsilon: 0.081\n",
      "Episode 214/600, Total Reward: 1078, Epsilon: 0.075\n",
      "Episode 215/600, Total Reward: -69, Epsilon: 0.075\n",
      "Episode 216/600, Total Reward: 1079, Epsilon: 0.069\n",
      "Episode 217/600, Total Reward: 262, Epsilon: 0.068\n",
      "Episode 218/600, Total Reward: 574, Epsilon: 0.066\n",
      "Episode 219/600, Total Reward: 574, Epsilon: 0.064\n",
      "Episode 220/600, Total Reward: 1218, Epsilon: 0.059\n",
      "Updated target model.\n",
      "Episode 221/600, Total Reward: 577, Epsilon: 0.056\n",
      "Episode 222/600, Total Reward: 499, Epsilon: 0.054\n",
      "Episode 223/600, Total Reward: 805, Epsilon: 0.051\n",
      "Episode 224/600, Total Reward: 330, Epsilon: 0.050\n",
      "Episode 225/600, Total Reward: 167, Epsilon: 0.050\n",
      "Episode 226/600, Total Reward: 905, Epsilon: 0.048\n",
      "Episode 227/600, Total Reward: 243, Epsilon: 0.047\n",
      "Episode 228/600, Total Reward: 177, Epsilon: 0.046\n",
      "Episode 229/600, Total Reward: 257, Epsilon: 0.045\n",
      "Episode 230/600, Total Reward: 1657, Epsilon: 0.041\n",
      "Updated target model.\n",
      "Episode 231/600, Total Reward: 834, Epsilon: 0.038\n",
      "Episode 232/600, Total Reward: 1165, Epsilon: 0.035\n",
      "Episode 233/600, Total Reward: 1876, Epsilon: 0.031\n",
      "Episode 234/600, Total Reward: 654, Epsilon: 0.030\n",
      "Episode 235/600, Total Reward: 817, Epsilon: 0.029\n",
      "Episode 236/600, Total Reward: 416, Epsilon: 0.028\n",
      "Episode 237/600, Total Reward: 406, Epsilon: 0.028\n",
      "Episode 238/600, Total Reward: 818, Epsilon: 0.027\n",
      "Episode 239/600, Total Reward: 732, Epsilon: 0.026\n",
      "Episode 240/600, Total Reward: 1381, Epsilon: 0.024\n",
      "Updated target model.\n",
      "Episode 241/600, Total Reward: 1137, Epsilon: 0.023\n",
      "Episode 242/600, Total Reward: 575, Epsilon: 0.022\n",
      "Episode 243/600, Total Reward: 942, Epsilon: 0.020\n",
      "Episode 244/600, Total Reward: 1811, Epsilon: 0.018\n",
      "Episode 245/600, Total Reward: 1480, Epsilon: 0.016\n",
      "Episode 246/600, Total Reward: 404, Epsilon: 0.016\n",
      "Episode 247/600, Total Reward: 1660, Epsilon: 0.014\n",
      "Episode 248/600, Total Reward: 1062, Epsilon: 0.013\n",
      "Episode 249/600, Total Reward: 1296, Epsilon: 0.013\n",
      "Episode 250/600, Total Reward: 1375, Epsilon: 0.012\n",
      "Updated target model.\n",
      "Episode 251/600, Total Reward: 1502, Epsilon: 0.011\n",
      "Episode 252/600, Total Reward: 565, Epsilon: 0.010\n",
      "Episode 253/600, Total Reward: 808, Epsilon: 0.010\n",
      "Episode 254/600, Total Reward: 992, Epsilon: 0.010\n",
      "Episode 255/600, Total Reward: 882, Epsilon: 0.010\n",
      "Episode 256/600, Total Reward: 1468, Epsilon: 0.010\n",
      "Episode 257/600, Total Reward: 976, Epsilon: 0.010\n",
      "Episode 258/600, Total Reward: 1637, Epsilon: 0.010\n",
      "Episode 259/600, Total Reward: 995, Epsilon: 0.010\n",
      "Episode 260/600, Total Reward: 1566, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 261/600, Total Reward: 1391, Epsilon: 0.010\n",
      "Episode 262/600, Total Reward: 1050, Epsilon: 0.010\n",
      "Episode 263/600, Total Reward: 1410, Epsilon: 0.010\n",
      "Episode 264/600, Total Reward: 743, Epsilon: 0.010\n",
      "Episode 265/600, Total Reward: 1881, Epsilon: 0.010\n",
      "Episode 266/600, Total Reward: 651, Epsilon: 0.010\n",
      "Episode 267/600, Total Reward: 1265, Epsilon: 0.010\n",
      "Episode 268/600, Total Reward: 1393, Epsilon: 0.010\n",
      "Episode 269/600, Total Reward: 1746, Epsilon: 0.010\n",
      "Episode 270/600, Total Reward: 1907, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 271/600, Total Reward: 1310, Epsilon: 0.010\n",
      "Episode 272/600, Total Reward: 1166, Epsilon: 0.010\n",
      "Episode 273/600, Total Reward: 2350, Epsilon: 0.010\n",
      "Episode 274/600, Total Reward: 354, Epsilon: 0.010\n",
      "Episode 275/600, Total Reward: 2010, Epsilon: 0.010\n",
      "Episode 276/600, Total Reward: 651, Epsilon: 0.010\n",
      "Episode 277/600, Total Reward: 1446, Epsilon: 0.010\n",
      "Episode 278/600, Total Reward: 1167, Epsilon: 0.010\n",
      "Episode 279/600, Total Reward: 1488, Epsilon: 0.010\n",
      "Episode 280/600, Total Reward: 175, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 281/600, Total Reward: 1160, Epsilon: 0.010\n",
      "Episode 282/600, Total Reward: 2065, Epsilon: 0.010\n",
      "Episode 283/600, Total Reward: 971, Epsilon: 0.010\n",
      "Episode 284/600, Total Reward: 1411, Epsilon: 0.010\n",
      "Episode 285/600, Total Reward: 2441, Epsilon: 0.010\n",
      "Episode 286/600, Total Reward: 984, Epsilon: 0.010\n",
      "Episode 287/600, Total Reward: 1173, Epsilon: 0.010\n",
      "Episode 288/600, Total Reward: 2130, Epsilon: 0.010\n",
      "Episode 289/600, Total Reward: 1490, Epsilon: 0.010\n",
      "Episode 290/600, Total Reward: 2692, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 291/600, Total Reward: 2476, Epsilon: 0.010\n",
      "Episode 292/600, Total Reward: 823, Epsilon: 0.010\n",
      "Episode 293/600, Total Reward: 337, Epsilon: 0.010\n",
      "Episode 294/600, Total Reward: 1320, Epsilon: 0.010\n",
      "Episode 295/600, Total Reward: 1910, Epsilon: 0.010\n",
      "Episode 296/600, Total Reward: 1555, Epsilon: 0.010\n",
      "Episode 297/600, Total Reward: 662, Epsilon: 0.010\n",
      "Episode 298/600, Total Reward: 1668, Epsilon: 0.010\n",
      "Episode 299/600, Total Reward: 1074, Epsilon: 0.010\n",
      "Episode 300/600, Total Reward: 984, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 301/600, Total Reward: 1580, Epsilon: 0.010\n",
      "Episode 302/600, Total Reward: 1313, Epsilon: 0.010\n",
      "Episode 303/600, Total Reward: 1896, Epsilon: 0.010\n",
      "Episode 304/600, Total Reward: 1568, Epsilon: 0.010\n",
      "Episode 305/600, Total Reward: 2121, Epsilon: 0.010\n",
      "Episode 306/600, Total Reward: 1957, Epsilon: 0.010\n",
      "Episode 307/600, Total Reward: 1724, Epsilon: 0.010\n",
      "Episode 308/600, Total Reward: 1394, Epsilon: 0.010\n",
      "Episode 309/600, Total Reward: 1727, Epsilon: 0.010\n",
      "Episode 310/600, Total Reward: 1471, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 311/600, Total Reward: 1179, Epsilon: 0.010\n",
      "Episode 312/600, Total Reward: 1047, Epsilon: 0.010\n",
      "Episode 313/600, Total Reward: 677, Epsilon: 0.010\n",
      "Episode 314/600, Total Reward: 1314, Epsilon: 0.010\n",
      "Episode 315/600, Total Reward: 576, Epsilon: 0.010\n",
      "Episode 316/600, Total Reward: 1074, Epsilon: 0.010\n",
      "Episode 317/600, Total Reward: 263, Epsilon: 0.010\n",
      "Episode 318/600, Total Reward: 893, Epsilon: 0.010\n",
      "Episode 319/600, Total Reward: 1480, Epsilon: 0.010\n",
      "Episode 320/600, Total Reward: 1556, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 321/600, Total Reward: 1092, Epsilon: 0.010\n",
      "Episode 322/600, Total Reward: 756, Epsilon: 0.010\n",
      "Episode 323/600, Total Reward: 2221, Epsilon: 0.010\n",
      "Episode 324/600, Total Reward: 825, Epsilon: 0.010\n",
      "Episode 325/600, Total Reward: 1739, Epsilon: 0.010\n",
      "Episode 326/600, Total Reward: 1337, Epsilon: 0.010\n",
      "Episode 327/600, Total Reward: 1304, Epsilon: 0.010\n",
      "Episode 328/600, Total Reward: 835, Epsilon: 0.010\n",
      "Episode 329/600, Total Reward: 1980, Epsilon: 0.010\n",
      "Episode 330/600, Total Reward: 832, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 331/600, Total Reward: 729, Epsilon: 0.010\n",
      "Episode 332/600, Total Reward: 1750, Epsilon: 0.010\n",
      "Episode 333/600, Total Reward: 724, Epsilon: 0.010\n",
      "Episode 334/600, Total Reward: 1791, Epsilon: 0.010\n",
      "Episode 335/600, Total Reward: 1730, Epsilon: 0.010\n",
      "Episode 336/600, Total Reward: 1519, Epsilon: 0.010\n",
      "Episode 337/600, Total Reward: 899, Epsilon: 0.010\n",
      "Episode 338/600, Total Reward: 1664, Epsilon: 0.010\n",
      "Episode 339/600, Total Reward: 250, Epsilon: 0.010\n",
      "Episode 340/600, Total Reward: 1646, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 341/600, Total Reward: 752, Epsilon: 0.010\n",
      "Episode 342/600, Total Reward: 982, Epsilon: 0.010\n",
      "Episode 343/600, Total Reward: 1884, Epsilon: 0.010\n",
      "Episode 344/600, Total Reward: 931, Epsilon: 0.010\n",
      "Episode 345/600, Total Reward: 1694, Epsilon: 0.010\n",
      "Episode 346/600, Total Reward: 333, Epsilon: 0.010\n",
      "Episode 347/600, Total Reward: 1722, Epsilon: 0.010\n",
      "Episode 348/600, Total Reward: 1227, Epsilon: 0.010\n",
      "Episode 349/600, Total Reward: 818, Epsilon: 0.010\n",
      "Episode 350/600, Total Reward: 329, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 351/600, Total Reward: 2119, Epsilon: 0.010\n",
      "Episode 352/600, Total Reward: 1219, Epsilon: 0.010\n",
      "Episode 353/600, Total Reward: 1306, Epsilon: 0.010\n",
      "Episode 354/600, Total Reward: 574, Epsilon: 0.010\n",
      "Episode 355/600, Total Reward: 1153, Epsilon: 0.010\n",
      "Episode 356/600, Total Reward: 833, Epsilon: 0.010\n",
      "Episode 357/600, Total Reward: 2027, Epsilon: 0.010\n",
      "Episode 358/600, Total Reward: 1068, Epsilon: 0.010\n",
      "Episode 359/600, Total Reward: 2112, Epsilon: 0.010\n",
      "Episode 360/600, Total Reward: 2720, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 361/600, Total Reward: 665, Epsilon: 0.010\n",
      "Episode 362/600, Total Reward: 2385, Epsilon: 0.010\n",
      "Episode 363/600, Total Reward: -68, Epsilon: 0.010\n",
      "Episode 364/600, Total Reward: 2129, Epsilon: 0.010\n",
      "Episode 365/600, Total Reward: 1413, Epsilon: 0.010\n",
      "Episode 366/600, Total Reward: 1291, Epsilon: 0.010\n",
      "Episode 367/600, Total Reward: 1288, Epsilon: 0.010\n",
      "Episode 368/600, Total Reward: 2040, Epsilon: 0.010\n",
      "Episode 369/600, Total Reward: 1450, Epsilon: 0.010\n",
      "Episode 370/600, Total Reward: 1656, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 371/600, Total Reward: 906, Epsilon: 0.010\n",
      "Episode 372/600, Total Reward: 1481, Epsilon: 0.010\n",
      "Episode 373/600, Total Reward: 1967, Epsilon: 0.010\n",
      "Episode 374/600, Total Reward: 751, Epsilon: 0.010\n",
      "Episode 375/600, Total Reward: 1568, Epsilon: 0.010\n",
      "Episode 376/600, Total Reward: 1650, Epsilon: 0.010\n",
      "Episode 377/600, Total Reward: 1736, Epsilon: 0.010\n",
      "Episode 378/600, Total Reward: 1251, Epsilon: 0.010\n",
      "Episode 379/600, Total Reward: 1157, Epsilon: 0.010\n",
      "Episode 380/600, Total Reward: 1238, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 381/600, Total Reward: 998, Epsilon: 0.010\n",
      "Episode 382/600, Total Reward: 1080, Epsilon: 0.010\n",
      "Episode 383/600, Total Reward: 10, Epsilon: 0.010\n",
      "Episode 384/600, Total Reward: 2806, Epsilon: 0.010\n",
      "Episode 385/600, Total Reward: 1880, Epsilon: 0.010\n",
      "Episode 386/600, Total Reward: 1854, Epsilon: 0.010\n",
      "Episode 387/600, Total Reward: 2100, Epsilon: 0.010\n",
      "Episode 388/600, Total Reward: 2152, Epsilon: 0.010\n",
      "Episode 389/600, Total Reward: 1730, Epsilon: 0.010\n",
      "Episode 390/600, Total Reward: 416, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 391/600, Total Reward: 1646, Epsilon: 0.010\n",
      "Episode 392/600, Total Reward: 971, Epsilon: 0.010\n",
      "Episode 393/600, Total Reward: 993, Epsilon: 0.010\n",
      "Episode 394/600, Total Reward: 1068, Epsilon: 0.010\n",
      "Episode 395/600, Total Reward: 256, Epsilon: 0.010\n",
      "Episode 396/600, Total Reward: 1815, Epsilon: 0.010\n",
      "Episode 397/600, Total Reward: 1387, Epsilon: 0.010\n",
      "Episode 398/600, Total Reward: 1810, Epsilon: 0.010\n",
      "Episode 399/600, Total Reward: 1078, Epsilon: 0.010\n",
      "Episode 400/600, Total Reward: 1173, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 401/600, Total Reward: 998, Epsilon: 0.010\n",
      "Episode 402/600, Total Reward: 497, Epsilon: 0.010\n",
      "Episode 403/600, Total Reward: 838, Epsilon: 0.010\n",
      "Episode 404/600, Total Reward: 1725, Epsilon: 0.010\n",
      "Episode 405/600, Total Reward: 1147, Epsilon: 0.010\n",
      "Episode 406/600, Total Reward: 1149, Epsilon: 0.010\n",
      "Episode 407/600, Total Reward: 1224, Epsilon: 0.010\n",
      "Episode 408/600, Total Reward: 1241, Epsilon: 0.010\n",
      "Episode 409/600, Total Reward: 2121, Epsilon: 0.010\n",
      "Episode 410/600, Total Reward: 2934, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 411/600, Total Reward: 1146, Epsilon: 0.010\n",
      "Episode 412/600, Total Reward: 975, Epsilon: 0.010\n",
      "Episode 413/600, Total Reward: 1143, Epsilon: 0.010\n",
      "Episode 414/600, Total Reward: 1659, Epsilon: 0.010\n",
      "Episode 415/600, Total Reward: 828, Epsilon: 0.010\n",
      "Episode 416/600, Total Reward: 1225, Epsilon: 0.010\n",
      "Episode 417/600, Total Reward: 573, Epsilon: 0.010\n",
      "Episode 418/600, Total Reward: 2140, Epsilon: 0.010\n",
      "Episode 419/600, Total Reward: 1443, Epsilon: 0.010\n",
      "Episode 420/600, Total Reward: 2264, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 421/600, Total Reward: 740, Epsilon: 0.010\n",
      "Episode 422/600, Total Reward: 832, Epsilon: 0.010\n",
      "Episode 423/600, Total Reward: 2272, Epsilon: 0.010\n",
      "Episode 424/600, Total Reward: 16, Epsilon: 0.010\n",
      "Episode 425/600, Total Reward: 408, Epsilon: 0.010\n",
      "Episode 426/600, Total Reward: 2461, Epsilon: 0.010\n",
      "Episode 427/600, Total Reward: 1175, Epsilon: 0.010\n",
      "Episode 428/600, Total Reward: 2304, Epsilon: 0.010\n",
      "Episode 429/600, Total Reward: 172, Epsilon: 0.010\n",
      "Episode 430/600, Total Reward: 1069, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 431/600, Total Reward: 1229, Epsilon: 0.010\n",
      "Episode 432/600, Total Reward: 1323, Epsilon: 0.010\n",
      "Episode 433/600, Total Reward: 1224, Epsilon: 0.010\n",
      "Episode 434/600, Total Reward: 1148, Epsilon: 0.010\n",
      "Episode 435/600, Total Reward: 1640, Epsilon: 0.010\n",
      "Episode 436/600, Total Reward: 1315, Epsilon: 0.010\n",
      "Episode 437/600, Total Reward: 739, Epsilon: 0.010\n",
      "Episode 438/600, Total Reward: 1833, Epsilon: 0.010\n",
      "Episode 439/600, Total Reward: 1404, Epsilon: 0.010\n",
      "Episode 440/600, Total Reward: 1623, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 441/600, Total Reward: 2618, Epsilon: 0.010\n",
      "Episode 442/600, Total Reward: 923, Epsilon: 0.010\n",
      "Episode 443/600, Total Reward: 2146, Epsilon: 0.010\n",
      "Episode 444/600, Total Reward: 1719, Epsilon: 0.010\n",
      "Episode 445/600, Total Reward: 1822, Epsilon: 0.010\n",
      "Episode 446/600, Total Reward: 583, Epsilon: 0.010\n",
      "Episode 447/600, Total Reward: 1392, Epsilon: 0.010\n",
      "Episode 448/600, Total Reward: 2395, Epsilon: 0.010\n",
      "Episode 449/600, Total Reward: 1376, Epsilon: 0.010\n",
      "Episode 450/600, Total Reward: 1346, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 451/600, Total Reward: 806, Epsilon: 0.010\n",
      "Episode 452/600, Total Reward: 1147, Epsilon: 0.010\n",
      "Episode 453/600, Total Reward: 1330, Epsilon: 0.010\n",
      "Episode 454/600, Total Reward: 660, Epsilon: 0.010\n",
      "Episode 455/600, Total Reward: 1811, Epsilon: 0.010\n",
      "Episode 456/600, Total Reward: 1468, Epsilon: 0.010\n",
      "Episode 457/600, Total Reward: 1571, Epsilon: 0.010\n",
      "Episode 458/600, Total Reward: 1996, Epsilon: 0.010\n",
      "Episode 459/600, Total Reward: 2058, Epsilon: 0.010\n",
      "Episode 460/600, Total Reward: 1486, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 461/600, Total Reward: 1402, Epsilon: 0.010\n",
      "Episode 462/600, Total Reward: 1149, Epsilon: 0.010\n",
      "Episode 463/600, Total Reward: 1419, Epsilon: 0.010\n",
      "Episode 464/600, Total Reward: 332, Epsilon: 0.010\n",
      "Episode 465/600, Total Reward: 844, Epsilon: 0.010\n",
      "Episode 466/600, Total Reward: 973, Epsilon: 0.010\n",
      "Episode 467/600, Total Reward: 910, Epsilon: 0.010\n",
      "Episode 468/600, Total Reward: 1583, Epsilon: 0.010\n",
      "Episode 469/600, Total Reward: 1503, Epsilon: 0.010\n",
      "Episode 470/600, Total Reward: 1736, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 471/600, Total Reward: 1144, Epsilon: 0.010\n",
      "Episode 472/600, Total Reward: 735, Epsilon: 0.010\n",
      "Episode 473/600, Total Reward: 1381, Epsilon: 0.010\n",
      "Episode 474/600, Total Reward: 2222, Epsilon: 0.010\n",
      "Episode 475/600, Total Reward: 991, Epsilon: 0.010\n",
      "Episode 476/600, Total Reward: 1129, Epsilon: 0.010\n",
      "Episode 477/600, Total Reward: 1698, Epsilon: 0.010\n",
      "Episode 478/600, Total Reward: 966, Epsilon: 0.010\n",
      "Episode 479/600, Total Reward: 1013, Epsilon: 0.010\n",
      "Episode 480/600, Total Reward: 1833, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 481/600, Total Reward: 1400, Epsilon: 0.010\n",
      "Episode 482/600, Total Reward: 492, Epsilon: 0.010\n",
      "Episode 483/600, Total Reward: 595, Epsilon: 0.010\n",
      "Episode 484/600, Total Reward: 1176, Epsilon: 0.010\n",
      "Episode 485/600, Total Reward: 1673, Epsilon: 0.010\n",
      "Episode 486/600, Total Reward: 904, Epsilon: 0.010\n",
      "Episode 487/600, Total Reward: 256, Epsilon: 0.010\n",
      "Episode 488/600, Total Reward: 838, Epsilon: 0.010\n",
      "Episode 489/600, Total Reward: 665, Epsilon: 0.010\n",
      "Episode 490/600, Total Reward: 2231, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 491/600, Total Reward: 1140, Epsilon: 0.010\n",
      "Episode 492/600, Total Reward: 1988, Epsilon: 0.010\n",
      "Episode 493/600, Total Reward: 1814, Epsilon: 0.010\n",
      "Episode 494/600, Total Reward: -75, Epsilon: 0.010\n",
      "Episode 495/600, Total Reward: 1157, Epsilon: 0.010\n",
      "Episode 496/600, Total Reward: 920, Epsilon: 0.010\n",
      "Episode 497/600, Total Reward: 1819, Epsilon: 0.010\n",
      "Episode 498/600, Total Reward: 1306, Epsilon: 0.010\n",
      "Episode 499/600, Total Reward: 1376, Epsilon: 0.010\n",
      "Episode 500/600, Total Reward: 1644, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 501/600, Total Reward: 1229, Epsilon: 0.010\n",
      "Episode 502/600, Total Reward: 902, Epsilon: 0.010\n",
      "Episode 503/600, Total Reward: 1402, Epsilon: 0.010\n",
      "Episode 504/600, Total Reward: 1148, Epsilon: 0.010\n",
      "Episode 505/600, Total Reward: 1234, Epsilon: 0.010\n",
      "Episode 506/600, Total Reward: 2016, Epsilon: 0.010\n",
      "Episode 507/600, Total Reward: 1297, Epsilon: 0.010\n",
      "Episode 508/600, Total Reward: 1401, Epsilon: 0.010\n",
      "Episode 509/600, Total Reward: 1596, Epsilon: 0.010\n",
      "Episode 510/600, Total Reward: 1228, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 511/600, Total Reward: 2469, Epsilon: 0.010\n",
      "Episode 512/600, Total Reward: 1935, Epsilon: 0.010\n",
      "Episode 513/600, Total Reward: 1075, Epsilon: 0.010\n",
      "Episode 514/600, Total Reward: 992, Epsilon: 0.010\n",
      "Episode 515/600, Total Reward: 905, Epsilon: 0.010\n",
      "Episode 516/600, Total Reward: 1602, Epsilon: 0.010\n",
      "Episode 517/600, Total Reward: 180, Epsilon: 0.010\n",
      "Episode 518/600, Total Reward: 926, Epsilon: 0.010\n",
      "Episode 519/600, Total Reward: 1988, Epsilon: 0.010\n",
      "Episode 520/600, Total Reward: 1985, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 521/600, Total Reward: 1563, Epsilon: 0.010\n",
      "Episode 522/600, Total Reward: 1645, Epsilon: 0.010\n",
      "Episode 523/600, Total Reward: 1151, Epsilon: 0.010\n",
      "Episode 524/600, Total Reward: 1148, Epsilon: 0.010\n",
      "Episode 525/600, Total Reward: 848, Epsilon: 0.010\n",
      "Episode 526/600, Total Reward: 1401, Epsilon: 0.010\n",
      "Episode 527/600, Total Reward: 1721, Epsilon: 0.010\n",
      "Episode 528/600, Total Reward: 1082, Epsilon: 0.010\n",
      "Episode 529/600, Total Reward: 1314, Epsilon: 0.010\n",
      "Episode 530/600, Total Reward: 1662, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 531/600, Total Reward: 1004, Epsilon: 0.010\n",
      "Episode 532/600, Total Reward: 1397, Epsilon: 0.010\n",
      "Episode 533/600, Total Reward: 2066, Epsilon: 0.010\n",
      "Episode 534/600, Total Reward: 7, Epsilon: 0.010\n",
      "Episode 535/600, Total Reward: 1542, Epsilon: 0.010\n",
      "Episode 536/600, Total Reward: 755, Epsilon: 0.010\n",
      "Episode 537/600, Total Reward: 1481, Epsilon: 0.010\n",
      "Episode 538/600, Total Reward: 18, Epsilon: 0.010\n",
      "Episode 539/600, Total Reward: 1704, Epsilon: 0.010\n",
      "Episode 540/600, Total Reward: 835, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 541/600, Total Reward: 249, Epsilon: 0.010\n",
      "Episode 542/600, Total Reward: 1332, Epsilon: 0.010\n",
      "Episode 543/600, Total Reward: 1728, Epsilon: 0.010\n",
      "Episode 544/600, Total Reward: 892, Epsilon: 0.010\n",
      "Episode 545/600, Total Reward: 1185, Epsilon: 0.010\n",
      "Episode 546/600, Total Reward: 1070, Epsilon: 0.010\n",
      "Episode 547/600, Total Reward: 529, Epsilon: 0.010\n",
      "Episode 548/600, Total Reward: 1462, Epsilon: 0.010\n",
      "Episode 549/600, Total Reward: 1167, Epsilon: 0.010\n",
      "Episode 550/600, Total Reward: 1687, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 551/600, Total Reward: 1578, Epsilon: 0.010\n",
      "Episode 552/600, Total Reward: 1321, Epsilon: 0.010\n",
      "Episode 553/600, Total Reward: 1814, Epsilon: 0.010\n",
      "Episode 554/600, Total Reward: 1148, Epsilon: 0.010\n",
      "Episode 555/600, Total Reward: 1978, Epsilon: 0.010\n",
      "Episode 556/600, Total Reward: 983, Epsilon: 0.010\n",
      "Episode 557/600, Total Reward: 2698, Epsilon: 0.010\n",
      "Episode 558/600, Total Reward: 2057, Epsilon: 0.010\n",
      "Episode 559/600, Total Reward: 2202, Epsilon: 0.010\n",
      "Episode 560/600, Total Reward: 2423, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 561/600, Total Reward: 327, Epsilon: 0.010\n",
      "Episode 562/600, Total Reward: 1483, Epsilon: 0.010\n",
      "Episode 563/600, Total Reward: 1624, Epsilon: 0.010\n",
      "Episode 564/600, Total Reward: 2053, Epsilon: 0.010\n",
      "Episode 565/600, Total Reward: 1670, Epsilon: 0.010\n",
      "Episode 566/600, Total Reward: 1168, Epsilon: 0.010\n",
      "Episode 567/600, Total Reward: 2027, Epsilon: 0.010\n",
      "Episode 568/600, Total Reward: 1812, Epsilon: 0.010\n",
      "Episode 569/600, Total Reward: 2036, Epsilon: 0.010\n",
      "Episode 570/600, Total Reward: 2603, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 571/600, Total Reward: 974, Epsilon: 0.010\n",
      "Episode 572/600, Total Reward: 2195, Epsilon: 0.010\n",
      "Episode 573/600, Total Reward: 1540, Epsilon: 0.010\n",
      "Episode 574/600, Total Reward: 575, Epsilon: 0.010\n",
      "Episode 575/600, Total Reward: 1886, Epsilon: 0.010\n",
      "Episode 576/600, Total Reward: 1744, Epsilon: 0.010\n",
      "Episode 577/600, Total Reward: 1809, Epsilon: 0.010\n",
      "Episode 578/600, Total Reward: 1242, Epsilon: 0.010\n",
      "Episode 579/600, Total Reward: 1219, Epsilon: 0.010\n",
      "Episode 580/600, Total Reward: 1652, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 581/600, Total Reward: 1978, Epsilon: 0.010\n",
      "Episode 582/600, Total Reward: 1054, Epsilon: 0.010\n",
      "Episode 583/600, Total Reward: 1001, Epsilon: 0.010\n",
      "Episode 584/600, Total Reward: 1171, Epsilon: 0.010\n",
      "Episode 585/600, Total Reward: 1227, Epsilon: 0.010\n",
      "Episode 586/600, Total Reward: 1555, Epsilon: 0.010\n",
      "Episode 587/600, Total Reward: 1149, Epsilon: 0.010\n",
      "Episode 588/600, Total Reward: 1825, Epsilon: 0.010\n",
      "Episode 589/600, Total Reward: 834, Epsilon: 0.010\n",
      "Episode 590/600, Total Reward: 1745, Epsilon: 0.010\n",
      "Updated target model.\n",
      "Episode 591/600, Total Reward: 2061, Epsilon: 0.010\n",
      "Episode 592/600, Total Reward: 1575, Epsilon: 0.010\n",
      "Episode 593/600, Total Reward: 1709, Epsilon: 0.010\n",
      "Episode 594/600, Total Reward: 2516, Epsilon: 0.010\n",
      "Episode 595/600, Total Reward: 1221, Epsilon: 0.010\n",
      "Episode 596/600, Total Reward: 830, Epsilon: 0.010\n",
      "Episode 597/600, Total Reward: 2348, Epsilon: 0.010\n",
      "Episode 598/600, Total Reward: 1558, Epsilon: 0.010\n",
      "Episode 599/600, Total Reward: 1551, Epsilon: 0.010\n",
      "Episode 600/600, Total Reward: 749, Epsilon: 0.010\n",
      "Updated target model.\n"
     ]
    }
   ],
   "source": [
    "env = SnakeEnv(grid_size=10, interact=False)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "train_dqn(env, agent, episodes=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -75\n"
     ]
    }
   ],
   "source": [
    "env = SnakeEnv(grid_size=20)\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.act(tuple(state.flatten()))\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
