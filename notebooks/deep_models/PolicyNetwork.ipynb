{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "# Add the project root directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "from src.ParObsSnakeEnv import SnakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=8):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(input_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.fc3 = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, device='cpu'):\n",
    "        self.device = device\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # Add batch dimension\n",
    "        action_probs = self.policy(state).detach().cpu().numpy().squeeze()\n",
    "        print(action_probs)\n",
    "        action = np.random.choice(len(action_probs), p=action_probs)\n",
    "        return action\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        \"\"\"Compute discounted returns for an episode.\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        # Normalize returns to improve training stability\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "        return returns\n",
    "\n",
    "    def update_policy(self, log_probs, returns):\n",
    "        \"\"\"Perform policy gradient update.\"\"\"\n",
    "        loss = -torch.sum(torch.stack(log_probs) * returns)  # Negative log-prob * return\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# Train the REINFORCE Agent\n",
    "def train_reinforce(env, agent, episodes=1000):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            state_tensor = torch.FloatTensor(state).to(agent.device)\n",
    "            action_prob = agent.policy(state_tensor)[action]\n",
    "            log_prob = torch.log(action_prob)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        # Compute returns and update policy\n",
    "        returns = agent.compute_returns(rewards)\n",
    "        agent.update_policy(log_probs, returns)\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[0.22000895 0.21754985 0.31720832 0.24523295]\n",
      "[0.22000895 0.21754985 0.31720832 0.24523295]\n",
      "[0.22000895 0.21754985 0.31720832 0.24523295]\n",
      "[0.22777414 0.2177263  0.30226052 0.25223905]\n",
      "[0.22777414 0.2177263  0.30226052 0.25223905]\n",
      "[0.2214016  0.21717985 0.30085826 0.26056027]\n",
      "[0.24398457 0.21784614 0.29525647 0.24291283]\n",
      "[0.22536449 0.22169687 0.30950156 0.24343705]\n",
      "[0.22536449 0.22169687 0.30950156 0.24343705]\n",
      "[0.2441416  0.22446391 0.29220763 0.23918685]\n",
      "Episode 1/500, Total Reward: -74\n",
      "[0.2279274  0.23163623 0.29399836 0.24643804]\n",
      "[0.23240316 0.23216662 0.2955215  0.23990868]\n",
      "[0.2457022  0.2336044  0.27908117 0.24161229]\n",
      "[0.25170746 0.22428249 0.2686575  0.2553525 ]\n",
      "[0.25170746 0.22428249 0.2686575  0.2553525 ]\n",
      "[0.23628801 0.22779863 0.28580946 0.2501039 ]\n",
      "[0.23439482 0.21239856 0.28339705 0.2698096 ]\n",
      "[0.22658981 0.21833886 0.3022195  0.25285184]\n",
      "[0.22022016 0.21780613 0.3007522  0.26122156]\n",
      "[0.22739038 0.23511699 0.2947845  0.24270815]\n",
      "[0.23439482 0.21239856 0.28339705 0.2698096 ]\n",
      "Episode 2/500, Total Reward: -77\n",
      "[0.23380527 0.21295972 0.28305784 0.2701772 ]\n",
      "[0.24874003 0.21763478 0.2693176  0.2643076 ]\n",
      "[0.24010907 0.21507907 0.28311515 0.2616967 ]\n",
      "Episode 3/500, Total Reward: -75\n",
      "[0.21672945 0.22243841 0.3208312  0.240001  ]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.21376425 0.2244661  0.3117964  0.24997322]\n",
      "[0.21376425 0.2244661  0.3117964  0.24997322]\n",
      "[0.21376425 0.2244661  0.3117964  0.24997322]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.21376425 0.2244661  0.3117964  0.24997322]\n",
      "[0.21376425 0.2244661  0.3117964  0.24997322]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.21376425 0.2244661  0.3117964  0.24997322]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.21672945 0.22243841 0.3208312  0.240001  ]\n",
      "[0.21672945 0.22243841 0.3208312  0.240001  ]\n",
      "[0.21672945 0.22243841 0.3208312  0.240001  ]\n",
      "[0.21672945 0.22243841 0.3208312  0.240001  ]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.24031992 0.2264073  0.2930066  0.24026619]\n",
      "[0.23239939 0.22157861 0.30184355 0.24417847]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.21376425 0.2244661  0.3117964  0.24997322]\n",
      "[0.24031992 0.2264073  0.2930066  0.24026619]\n",
      "[0.20011216 0.22775918 0.32325047 0.24887817]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.21672945 0.22243841 0.3208312  0.240001  ]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.21376425 0.2244661  0.3117964  0.24997322]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.21672945 0.22243841 0.3208312  0.240001  ]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.21672945 0.22243841 0.3208312  0.240001  ]\n",
      "[0.21672945 0.22243841 0.3208312  0.240001  ]\n",
      "[0.2076253  0.23604235 0.3132108  0.24312155]\n",
      "[0.21672945 0.22243841 0.3208312  0.240001  ]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.24031992 0.2264073  0.2930066  0.24026619]\n",
      "[0.23994568 0.23506841 0.28883284 0.23615305]\n",
      "[0.21218102 0.23754722 0.3050142  0.24525759]\n",
      "[0.21218102 0.23754722 0.3050142  0.24525759]\n",
      "[0.21218102 0.23754722 0.3050142  0.24525759]\n",
      "[0.21376425 0.2244661  0.3117964  0.24997322]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.22212791 0.22359261 0.3099105  0.24436902]\n",
      "[0.24031992 0.2264073  0.2930066  0.24026619]\n",
      "Episode 4/500, Total Reward: -75\n",
      "[0.21632653 0.21918415 0.31692645 0.24756293]\n",
      "[0.20974854 0.23743325 0.3092134  0.24360482]\n",
      "[0.20974854 0.23743325 0.3092134  0.24360482]\n",
      "[0.20974854 0.23743325 0.3092134  0.24360482]\n",
      "[0.20974854 0.23743325 0.3092134  0.24360482]\n",
      "[0.20974854 0.23743325 0.3092134  0.24360482]\n",
      "[0.21632653 0.21918415 0.31692645 0.24756293]\n",
      "[0.22553366 0.236061   0.29505676 0.24334855]\n",
      "[0.20233689 0.2343611  0.32213357 0.24116842]\n",
      "[0.22383276 0.23343213 0.29919615 0.243539  ]\n",
      "[0.23023717 0.233091   0.295995   0.24067691]\n",
      "[0.23023717 0.233091   0.295995   0.24067691]\n",
      "[0.23023717 0.233091   0.295995   0.24067691]\n",
      "[0.24372861 0.23445722 0.27949864 0.24231555]\n",
      "Episode 5/500, Total Reward: -82\n",
      "[0.2115733  0.23573038 0.30658352 0.24611287]\n",
      "[0.21799996 0.2190055  0.3009063  0.26208824]\n",
      "[0.20901248 0.2373126  0.31004718 0.24362779]\n",
      "[0.21640597 0.23130122 0.31186226 0.24043062]\n",
      "[0.24549252 0.23412381 0.28399357 0.23639005]\n",
      "[0.22008519 0.23632357 0.298127   0.24546422]\n",
      "[0.21540356 0.23439012 0.30438483 0.24582145]\n",
      "[0.2115733  0.23573038 0.30658352 0.24611287]\n",
      "[0.24549252 0.23412381 0.28399357 0.23639005]\n",
      "Episode 6/500, Total Reward: -77\n",
      "[0.2368783  0.23047318 0.2839727  0.2486758 ]\n",
      "[0.2368783  0.23047318 0.2839727  0.2486758 ]\n",
      "[0.2368783  0.23047318 0.2839727  0.2486758 ]\n",
      "[0.2368783  0.23047318 0.2839727  0.2486758 ]\n",
      "[0.24950904 0.22770803 0.2766471  0.24613588]\n",
      "[0.2475414  0.23358162 0.27986088 0.23901609]\n",
      "Episode 7/500, Total Reward: -72\n",
      "[0.23708221 0.22584352 0.29545772 0.24161655]\n",
      "Episode 8/500, Total Reward: -75\n",
      "[nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82304/1946865778.py:24: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m agent \u001b[38;5;241m=\u001b[39m REINFORCEAgent(state_dim, action_dim, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain_reinforce\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 44\u001b[0m, in \u001b[0;36mtrain_reinforce\u001b[0;34m(env, agent, episodes)\u001b[0m\n\u001b[1;32m     41\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 44\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39mto(agent\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     46\u001b[0m     action_prob \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mpolicy(state_tensor)[action]\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mREINFORCEAgent.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     10\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(state)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(action_probs)\n\u001b[0;32m---> 12\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maction_probs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:994\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "env = SnakeEnv(grid_size=10, interact=False)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Determine the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "agent = REINFORCEAgent(state_dim, action_dim, device=device)\n",
    "\n",
    "train_reinforce(env, agent, episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
