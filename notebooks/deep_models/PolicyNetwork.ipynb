{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "# Add the project root directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "from src.ParObsSnakeEnv import ParObsSnakeEnv\n",
    "from src.FullObsSnakeEnv import FullObsSnakeEnv\n",
    "from src.utils import compute_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, device=None):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        # Metrics for plotting\n",
    "        self.episode_rewards = []\n",
    "        self.episode_losses = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # Add batch dimension\n",
    "        action_probs = self.policy(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def remember(self, log_prob, reward):\n",
    "        if not hasattr(self, 'log_probs'):\n",
    "            self.log_probs = []\n",
    "        if not hasattr(self, 'rewards'):\n",
    "            self.rewards = []\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"Update the policy using stored rewards and log probabilities.\"\"\"\n",
    "        if not hasattr(self, 'log_probs') or len(self.log_probs) == 0:\n",
    "            return\n",
    "\n",
    "        returns = self.compute_returns(self.rewards)\n",
    "        loss = -torch.sum(torch.stack(self.log_probs) * returns)  # Negative log-prob * return\n",
    "\n",
    "        # Update the policy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Log the loss\n",
    "        self.episode_losses.append(loss.item())\n",
    "\n",
    "        # Clear memory\n",
    "        self.log_probs.clear()\n",
    "        self.rewards.clear()\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        \"\"\"Compute discounted returns for an episode.\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        # Normalize returns to improve training stability\n",
    "        if len(returns) > 1 and returns.std() > 1e-5:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "        return returns\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves the entire agent to a file.\"\"\"\n",
    "        state = {\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'hyperparameters': {\n",
    "                'state_dim': self.state_dim,\n",
    "                'action_dim': self.action_dim,\n",
    "                'gamma': self.gamma,\n",
    "            },\n",
    "        }\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        print(f\"Agent saved to {filename}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename, lr=0.001):\n",
    "        \"\"\"Loads the agent from a file.\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "\n",
    "        # Recreate the agent\n",
    "        agent = cls(\n",
    "            state['hyperparameters']['state_dim'],\n",
    "            state['hyperparameters']['action_dim'],\n",
    "            lr=lr,\n",
    "            gamma=state['hyperparameters']['gamma'],\n",
    "        )\n",
    "        # Restore the agent's state\n",
    "        agent.policy.load_state_dict(state['policy_state_dict'])\n",
    "        agent.optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        print(f\"Agent loaded from {filename}\")\n",
    "        return agent\n",
    "\n",
    "    def train(self, env, episodes=1000, save_plots=False, plots_path='reinforce_training_plots.png'):\n",
    "        for episode in tqdm(range(episodes), desc=\"Training\", unit=\"episode\"):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            self.policy.train()\n",
    "\n",
    "            while not done:\n",
    "                action, log_prob = self.choose_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                self.remember(log_prob, reward)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            self.update_policy()\n",
    "            self.episode_rewards.append(total_reward)\n",
    "\n",
    "        if save_plots:\n",
    "            self.save_plots(plots_path)\n",
    "        self.policy.eval()\n",
    "\n",
    "\n",
    "    def save_plots(self, plots_path):\n",
    "        plots_dir = os.path.dirname(plots_path)\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        # Rewards per episode\n",
    "        axs[0].plot(self.episode_rewards)\n",
    "        axs[0].set_title(\"Episode Rewards\")\n",
    "        axs[0].set_xlabel(\"Episode\")\n",
    "        axs[0].set_ylabel(\"Total Reward\")\n",
    "\n",
    "        # Loss per episode\n",
    "        axs[1].plot(self.episode_losses)\n",
    "        axs[1].set_title(\"Loss Over Training\")\n",
    "        axs[1].set_xlabel(\"Episode\")\n",
    "        axs[1].set_ylabel(\"Loss\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plots_path)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "# env = FullObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "env = ParObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "if isinstance(env, FullObsSnakeEnv):\n",
    "    state_dim = env.observation_space.shape[0] * env.observation_space.shape[1] * env.observation_space.shape[2]\n",
    "else:\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "\n",
    "action_dim = env.action_space.n\n",
    "agent = REINFORCEAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10000/10000 [05:57<00:00, 27.98episode/s]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "agent.train(env, episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent saved to ../../models/polNet/polNet_agent_par_10000_10.pkl\n"
     ]
    }
   ],
   "source": [
    "environment = 'full 'if isinstance(env, FullObsSnakeEnv) else 'par'\n",
    "\n",
    "agent_name = f'polNet_agent_{environment}_{num_episodes}_{grid_size}.pkl'\n",
    "model_weights_dir = os.path.join('../..', 'models', 'polNet')\n",
    "os.makedirs(model_weights_dir, exist_ok=True)\n",
    "agent_path = os.path.join(model_weights_dir, agent_name)\n",
    "\n",
    "agent.save(agent_path)\n",
    "# agent = REINFORCEAgent.load(agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:04, 19.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 2, Episode reward: 20\n",
      "Snake length: 11, Episode reward: 787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:00<00:03, 23.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 16, Episode reward: 1239\n",
      "Snake length: 8, Episode reward: 525\n",
      "Snake length: 2, Episode reward: 23\n",
      "Snake length: 2, Episode reward: 17\n",
      "Snake length: 3, Episode reward: 107\n",
      "Snake length: 2, Episode reward: 31\n",
      "Snake length: 6, Episode reward: 370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:00<00:04, 21.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 9, Episode reward: 615\n",
      "Snake length: 4, Episode reward: 193\n",
      "Snake length: 7, Episode reward: 444\n",
      "Snake length: 5, Episode reward: 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:00<00:03, 21.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 7, Episode reward: 468\n",
      "Snake length: 1, Episode reward: -64\n",
      "Snake length: 8, Episode reward: 560\n",
      "Snake length: 10, Episode reward: 745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [00:00<00:04, 17.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 5, Episode reward: 314\n",
      "Snake length: 11, Episode reward: 817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:01<00:05, 13.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 15, Episode reward: 1217\n",
      "Snake length: 8, Episode reward: 534\n",
      "Snake length: 4, Episode reward: 191\n",
      "Snake length: 8, Episode reward: 530\n",
      "Snake length: 1, Episode reward: -72\n",
      "Snake length: 2, Episode reward: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:01<00:04, 15.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 4, Episode reward: 210\n",
      "Snake length: 5, Episode reward: 265\n",
      "Snake length: 11, Episode reward: 855\n",
      "Snake length: 2, Episode reward: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:01<00:03, 18.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 7, Episode reward: 429\n",
      "Snake length: 5, Episode reward: 296\n",
      "Snake length: 8, Episode reward: 536\n",
      "Snake length: 8, Episode reward: 515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:02<00:03, 17.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 6, Episode reward: 356\n",
      "Snake length: 6, Episode reward: 384\n",
      "Snake length: 4, Episode reward: 245\n",
      "Snake length: 4, Episode reward: 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:02<00:03, 19.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 7, Episode reward: 444\n",
      "Snake length: 3, Episode reward: 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:02<00:03, 15.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 11, Episode reward: 770\n",
      "Snake length: 4, Episode reward: 186\n",
      "Snake length: 8, Episode reward: 559\n",
      "Snake length: 6, Episode reward: 361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:02<00:03, 15.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 9, Episode reward: 631\n",
      "Snake length: 12, Episode reward: 904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [00:03<00:04, 12.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 9, Episode reward: 615\n",
      "Snake length: 4, Episode reward: 210\n",
      "Snake length: 12, Episode reward: 877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:03<00:03, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 4, Episode reward: 181\n",
      "Snake length: 8, Episode reward: 511\n",
      "Snake length: 9, Episode reward: 632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [00:03<00:03, 14.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 10, Episode reward: 711\n",
      "Snake length: 3, Episode reward: 102\n",
      "Snake length: 5, Episode reward: 285\n",
      "Snake length: 7, Episode reward: 461\n",
      "Snake length: 8, Episode reward: 530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [00:03<00:02, 15.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 4, Episode reward: 218\n",
      "Snake length: 7, Episode reward: 442\n",
      "Snake length: 6, Episode reward: 369\n",
      "Snake length: 8, Episode reward: 520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:03<00:02, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 7, Episode reward: 460\n",
      "Snake length: 4, Episode reward: 219\n",
      "Snake length: 8, Episode reward: 541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [00:04<00:02, 14.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 9, Episode reward: 649\n",
      "Snake length: 6, Episode reward: 358\n",
      "Snake length: 15, Episode reward: 1205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [00:04<00:02, 10.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 20, Episode reward: 1615\n",
      "Snake length: 6, Episode reward: 363\n",
      "Snake length: 4, Episode reward: 208\n",
      "Snake length: 8, Episode reward: 562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [00:05<00:02, 10.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 8, Episode reward: 558\n",
      "Snake length: 3, Episode reward: 123\n",
      "Snake length: 7, Episode reward: 445\n",
      "Snake length: 7, Episode reward: 496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [00:05<00:01, 13.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 4, Episode reward: 193\n",
      "Snake length: 4, Episode reward: 216\n",
      "Snake length: 4, Episode reward: 198\n",
      "Snake length: 2, Episode reward: 15\n",
      "Snake length: 4, Episode reward: 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:05<00:01, 15.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 16, Episode reward: 1253\n",
      "Snake length: 6, Episode reward: 373\n",
      "Snake length: 2, Episode reward: 36\n",
      "Snake length: 6, Episode reward: 357\n",
      "Snake length: 5, Episode reward: 270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [00:05<00:01, 14.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 14, Episode reward: 1059\n",
      "Snake length: 3, Episode reward: 114\n",
      "Snake length: 7, Episode reward: 430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [00:06<00:00, 14.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 14, Episode reward: 1066\n",
      "Snake length: 6, Episode reward: 380\n",
      "Snake length: 9, Episode reward: 599\n",
      "Snake length: 2, Episode reward: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [00:06<00:00, 15.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 10, Episode reward: 719\n",
      "Snake length: 9, Episode reward: 657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [00:06<00:00, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 14, Episode reward: 1067\n",
      "Snake length: 7, Episode reward: 476\n",
      "Snake length: 6, Episode reward: 349\n",
      "Snake length: 11, Episode reward: 768\n",
      "Snake length: 1, Episode reward: -70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 3, Episode reward: 108\n",
      "Snake length: 8, Episode reward: 518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'snake_lengths': [2,\n",
       "  11,\n",
       "  16,\n",
       "  8,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  6,\n",
       "  9,\n",
       "  4,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  1,\n",
       "  8,\n",
       "  10,\n",
       "  5,\n",
       "  11,\n",
       "  15,\n",
       "  8,\n",
       "  4,\n",
       "  8,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  11,\n",
       "  2,\n",
       "  7,\n",
       "  5,\n",
       "  8,\n",
       "  8,\n",
       "  6,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  7,\n",
       "  3,\n",
       "  11,\n",
       "  4,\n",
       "  8,\n",
       "  6,\n",
       "  9,\n",
       "  12,\n",
       "  9,\n",
       "  4,\n",
       "  12,\n",
       "  4,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  3,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  4,\n",
       "  7,\n",
       "  6,\n",
       "  8,\n",
       "  7,\n",
       "  4,\n",
       "  8,\n",
       "  9,\n",
       "  6,\n",
       "  15,\n",
       "  20,\n",
       "  6,\n",
       "  4,\n",
       "  8,\n",
       "  8,\n",
       "  3,\n",
       "  7,\n",
       "  7,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  16,\n",
       "  6,\n",
       "  2,\n",
       "  6,\n",
       "  5,\n",
       "  14,\n",
       "  3,\n",
       "  7,\n",
       "  14,\n",
       "  6,\n",
       "  9,\n",
       "  2,\n",
       "  10,\n",
       "  9,\n",
       "  14,\n",
       "  7,\n",
       "  6,\n",
       "  11,\n",
       "  1,\n",
       "  3,\n",
       "  8],\n",
       " 'episode_rewards': [20,\n",
       "  787,\n",
       "  1239,\n",
       "  525,\n",
       "  23,\n",
       "  17,\n",
       "  107,\n",
       "  31,\n",
       "  370,\n",
       "  615,\n",
       "  193,\n",
       "  444,\n",
       "  294,\n",
       "  468,\n",
       "  -64,\n",
       "  560,\n",
       "  745,\n",
       "  314,\n",
       "  817,\n",
       "  1217,\n",
       "  534,\n",
       "  191,\n",
       "  530,\n",
       "  -72,\n",
       "  19,\n",
       "  210,\n",
       "  265,\n",
       "  855,\n",
       "  31,\n",
       "  429,\n",
       "  296,\n",
       "  536,\n",
       "  515,\n",
       "  356,\n",
       "  384,\n",
       "  245,\n",
       "  199,\n",
       "  444,\n",
       "  125,\n",
       "  770,\n",
       "  186,\n",
       "  559,\n",
       "  361,\n",
       "  631,\n",
       "  904,\n",
       "  615,\n",
       "  210,\n",
       "  877,\n",
       "  181,\n",
       "  511,\n",
       "  632,\n",
       "  711,\n",
       "  102,\n",
       "  285,\n",
       "  461,\n",
       "  530,\n",
       "  218,\n",
       "  442,\n",
       "  369,\n",
       "  520,\n",
       "  460,\n",
       "  219,\n",
       "  541,\n",
       "  649,\n",
       "  358,\n",
       "  1205,\n",
       "  1615,\n",
       "  363,\n",
       "  208,\n",
       "  562,\n",
       "  558,\n",
       "  123,\n",
       "  445,\n",
       "  496,\n",
       "  193,\n",
       "  216,\n",
       "  198,\n",
       "  15,\n",
       "  179,\n",
       "  1253,\n",
       "  373,\n",
       "  36,\n",
       "  357,\n",
       "  270,\n",
       "  1059,\n",
       "  114,\n",
       "  430,\n",
       "  1066,\n",
       "  380,\n",
       "  599,\n",
       "  33,\n",
       "  719,\n",
       "  657,\n",
       "  1067,\n",
       "  476,\n",
       "  349,\n",
       "  768,\n",
       "  -70,\n",
       "  108,\n",
       "  518]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(env, ParObsSnakeEnv):\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size, interact=False)\n",
    "\n",
    "model_metrics_dir = os.path.join('../..', 'artifacts', 'models_stats', 'polNet')\n",
    "os.makedirs(model_metrics_dir, exist_ok=True)\n",
    "\n",
    "train_metrics_name = f'polNet_train_metrics_{environment}_{num_episodes}_{grid_size}.png'\n",
    "train_metrics_path = os.path.join(model_metrics_dir, train_metrics_name)\n",
    "agent.save_plots(train_metrics_path)\n",
    "\n",
    "num_simulations = 100\n",
    "sim_metrics_name = f'polNet_sim_metrics_{environment}_{num_episodes}_{env.grid_size}_{num_simulations}.json'\n",
    "sim_metrics_path = os.path.join(model_metrics_dir, sim_metrics_name)\n",
    "compute_metrics(agent, env, sim_metrics_path, num_simulations=num_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 13:51:28.193 python[90124:2021557] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-04 13:51:28.193 python[90124:2021557] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 76\n",
      "Reward: -75\n"
     ]
    }
   ],
   "source": [
    "if isinstance(env, FullObsSnakeEnv):\n",
    "    env.interact = True\n",
    "else:\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size)\n",
    "    \n",
    "state = env.reset()\n",
    "done = False\n",
    "with torch.no_grad():\n",
    "    while not done:\n",
    "        action, _ = agent.choose_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
