{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "# Add the project root directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "from src.ParObsSnakeEnv import ParObsSnakeEnv\n",
    "from src.FullObsSnakeEnv import FullObsSnakeEnv\n",
    "from src.utils import compute_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        # Metrics for plotting\n",
    "        self.episode_rewards = []\n",
    "        self.episode_losses = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # Add batch dimension\n",
    "        action_probs = self.policy(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def remember(self, log_prob, reward):\n",
    "        if not hasattr(self, 'log_probs'):\n",
    "            self.log_probs = []\n",
    "        if not hasattr(self, 'rewards'):\n",
    "            self.rewards = []\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"Update the policy using stored rewards and log probabilities.\"\"\"\n",
    "        if not hasattr(self, 'log_probs') or len(self.log_probs) == 0:\n",
    "            return\n",
    "\n",
    "        returns = self.compute_returns(self.rewards)\n",
    "        loss = -torch.sum(torch.stack(self.log_probs) * returns)  # Negative log-prob * return\n",
    "\n",
    "        # Update the policy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Log the loss\n",
    "        self.episode_losses.append(loss.item())\n",
    "\n",
    "        # Clear memory\n",
    "        self.log_probs.clear()\n",
    "        self.rewards.clear()\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        \"\"\"Compute discounted returns for an episode.\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        # Normalize returns to improve training stability\n",
    "        if len(returns) > 1 and returns.std() > 1e-5:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "        return returns\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves the entire agent to a file.\"\"\"\n",
    "        state = {\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'hyperparameters': {\n",
    "                'state_dim': self.state_dim,\n",
    "                'action_dim': self.action_dim,\n",
    "                'gamma': self.gamma,\n",
    "            },\n",
    "        }\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        print(f\"Agent saved to {filename}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename, lr=0.001):\n",
    "        \"\"\"Loads the agent from a file.\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "\n",
    "        # Recreate the agent\n",
    "        agent = cls(\n",
    "            state['hyperparameters']['state_dim'],\n",
    "            state['hyperparameters']['action_dim'],\n",
    "            lr=lr,\n",
    "            gamma=state['hyperparameters']['gamma'],\n",
    "        )\n",
    "        # Restore the agent's state\n",
    "        agent.policy.load_state_dict(state['policy_state_dict'])\n",
    "        agent.optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        print(f\"Agent loaded from {filename}\")\n",
    "        return agent\n",
    "\n",
    "    def train(self, env, episodes=1000, save_plots=False, plots_path='reinforce_training_plots.png'):\n",
    "        for episode in tqdm(range(episodes), desc=\"Training\", unit=\"episode\"):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action, log_prob = self.choose_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                self.remember(log_prob, reward)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            self.update_policy()\n",
    "            self.episode_rewards.append(total_reward)\n",
    "\n",
    "        if save_plots:\n",
    "            self.save_plots(plots_path)\n",
    "\n",
    "    def save_plots(self, plots_path):\n",
    "        plots_dir = os.path.dirname(plots_path)\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        # Rewards per episode\n",
    "        axs[0].plot(self.episode_rewards)\n",
    "        axs[0].set_title(\"Episode Rewards\")\n",
    "        axs[0].set_xlabel(\"Episode\")\n",
    "        axs[0].set_ylabel(\"Total Reward\")\n",
    "\n",
    "        # Loss per episode\n",
    "        axs[1].plot(self.episode_losses)\n",
    "        axs[1].set_title(\"Loss Over Training\")\n",
    "        axs[1].set_xlabel(\"Episode\")\n",
    "        axs[1].set_ylabel(\"Loss\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plots_path)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "# env = FullObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "env = ParObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "if isinstance(env, FullObsSnakeEnv):\n",
    "    state_dim = env.observation_space.shape[0] * env.observation_space.shape[1] * env.observation_space.shape[2]\n",
    "else:\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "\n",
    "action_dim = env.action_space.n\n",
    "agent = REINFORCEAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 6353/15000 [02:39<03:37, 39.72episode/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 105\u001b[0m, in \u001b[0;36mREINFORCEAgent.train\u001b[0;34m(self, env, episodes, save_plots, plots_path)\u001b[0m\n\u001b[1;32m    102\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m--> 105\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremember(log_prob, reward)\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mREINFORCEAgent.choose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     17\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(state)\n\u001b[1;32m     18\u001b[0m dist \u001b[38;5;241m=\u001b[39m Categorical(action_probs)\n\u001b[0;32m---> 19\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem(), log_prob\n",
      "File \u001b[0;32m~/5sem/RL_ShakeGame_project/.venv/lib/python3.12/site-packages/torch/distributions/categorical.py:134\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    132\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    133\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 134\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 15000\n",
    "agent.train(env, episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent saved to ../../models/polNet/polNet_agent_par_15000_10.pkl\n"
     ]
    }
   ],
   "source": [
    "environment = 'full 'if isinstance(env, FullObsSnakeEnv) else 'par'\n",
    "\n",
    "agent_name = f'polNet_agent_{environment}_{num_episodes}_{grid_size}.pkl'\n",
    "model_weights_dir = os.path.join('../..', 'models', 'polNet')\n",
    "os.makedirs(model_weights_dir, exist_ok=True)\n",
    "agent_path = os.path.join(model_weights_dir, agent_name)\n",
    "\n",
    "agent.save(agent_path)\n",
    "# agent = REINFORCEAgent.load(agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:11,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 2, Episode reward: 12\n",
      "Snake length: 1, Episode reward: -75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:00<00:07, 12.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 1, Episode reward: -77\n",
      "Snake length: 1, Episode reward: -73\n",
      "Snake length: 1, Episode reward: -75\n",
      "Snake length: 3, Episode reward: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:00<00:08, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 1, Episode reward: -80\n",
      "Snake length: 3, Episode reward: 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:01<00:13,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 2, Episode reward: 20\n",
      "Snake length: 1, Episode reward: -75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:01<00:11,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 2, Episode reward: 0\n",
      "Snake length: 1, Episode reward: -75\n",
      "Snake length: 2, Episode reward: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:01<00:11,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 4, Episode reward: 156\n",
      "Snake length: 2, Episode reward: -5\n",
      "Snake length: 1, Episode reward: -80\n",
      "Snake length: 1, Episode reward: -76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:02<00:06, 12.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 2, Episode reward: 17\n",
      "Snake length: 1, Episode reward: -71\n",
      "Snake length: 1, Episode reward: -77\n",
      "Snake length: 1, Episode reward: -78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:02<00:05, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 1, Episode reward: -66\n",
      "Snake length: 1, Episode reward: -67\n",
      "Snake length: 1, Episode reward: -71\n",
      "Snake length: 1, Episode reward: -80\n",
      "Snake length: 1, Episode reward: -67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:02<00:04, 15.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 1, Episode reward: -73\n",
      "Snake length: 1, Episode reward: -68\n",
      "Snake length: 1, Episode reward: -79\n",
      "Snake length: 1, Episode reward: -76\n",
      "Snake length: 1, Episode reward: -78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:03<00:03, 18.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 2, Episode reward: 9\n",
      "Snake length: 1, Episode reward: -81\n",
      "Snake length: 3, Episode reward: 111\n",
      "Snake length: 1, Episode reward: -80\n",
      "Snake length: 2, Episode reward: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:03<00:05, 11.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 1, Episode reward: -68\n",
      "Snake length: 1, Episode reward: -93\n",
      "Snake length: 1, Episode reward: -78\n",
      "Snake length: 1, Episode reward: -64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:03<00:03, 14.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 1, Episode reward: -78\n",
      "Snake length: 1, Episode reward: -66\n",
      "Snake length: 1, Episode reward: -77\n",
      "Snake length: 3, Episode reward: 94\n",
      "Snake length: 1, Episode reward: -88\n",
      "Snake length: 1, Episode reward: -63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [00:04<00:05,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 4, Episode reward: 189\n",
      "Snake length: 2, Episode reward: 15\n",
      "Snake length: 1, Episode reward: -74\n",
      "Snake length: 1, Episode reward: -73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:04<00:04,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 1, Episode reward: -75\n",
      "Snake length: 1, Episode reward: -87\n",
      "Snake length: 1, Episode reward: -65\n",
      "Snake length: 1, Episode reward: -70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:05<00:01, 20.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 2, Episode reward: 1\n",
      "Snake length: 1, Episode reward: -64\n",
      "Snake length: 1, Episode reward: -52\n",
      "Snake length: 1, Episode reward: -77\n",
      "Snake length: 1, Episode reward: -69\n",
      "Snake length: 1, Episode reward: -78\n",
      "Snake length: 1, Episode reward: -62\n",
      "Snake length: 1, Episode reward: -84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [00:05<00:02, 16.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 3, Episode reward: 102\n",
      "Snake length: 1, Episode reward: -58\n",
      "Snake length: 1, Episode reward: -71\n",
      "Snake length: 1, Episode reward: -62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [00:06<00:02, 11.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 3, Episode reward: 110\n",
      "Snake length: 2, Episode reward: 10\n",
      "Snake length: 1, Episode reward: -62\n",
      "Snake length: 2, Episode reward: 12\n",
      "Snake length: 1, Episode reward: -77\n",
      "Snake length: 1, Episode reward: -78\n",
      "Snake length: 1, Episode reward: -73\n",
      "Snake length: 1, Episode reward: -64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [00:06<00:01, 14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 2, Episode reward: -4\n",
      "Snake length: 1, Episode reward: -72\n",
      "Snake length: 1, Episode reward: -67\n",
      "Snake length: 1, Episode reward: -80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [00:06<00:01, 15.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 2, Episode reward: -10\n",
      "Snake length: 1, Episode reward: -78\n",
      "Snake length: 1, Episode reward: -72\n",
      "Snake length: 1, Episode reward: -78\n",
      "Snake length: 1, Episode reward: -73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [00:07<00:00, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 3, Episode reward: 102\n",
      "Snake length: 1, Episode reward: -64\n",
      "Snake length: 1, Episode reward: -75\n",
      "Snake length: 1, Episode reward: -71\n",
      "Snake length: 1, Episode reward: -77\n",
      "Snake length: 2, Episode reward: 17\n",
      "Snake length: 2, Episode reward: 8\n",
      "Snake length: 1, Episode reward: -73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [00:08<00:00,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 2, Episode reward: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [00:08<00:00,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 3, Episode reward: 93\n",
      "Snake length: 3, Episode reward: 89\n",
      "Snake length: 1, Episode reward: -74\n",
      "Snake length: 1, Episode reward: -78\n",
      "Snake length: 1, Episode reward: -62\n",
      "Snake length: 1, Episode reward: -80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 1, Episode reward: -58\n",
      "Snake length: 2, Episode reward: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'snake_lengths': [2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2],\n",
       " 'episode_rewards': [12,\n",
       "  -75,\n",
       "  -77,\n",
       "  -73,\n",
       "  -75,\n",
       "  74,\n",
       "  -80,\n",
       "  118,\n",
       "  20,\n",
       "  -75,\n",
       "  0,\n",
       "  -75,\n",
       "  3,\n",
       "  156,\n",
       "  -5,\n",
       "  -80,\n",
       "  -76,\n",
       "  17,\n",
       "  -71,\n",
       "  -77,\n",
       "  -78,\n",
       "  -66,\n",
       "  -67,\n",
       "  -71,\n",
       "  -80,\n",
       "  -67,\n",
       "  -73,\n",
       "  -68,\n",
       "  -79,\n",
       "  -76,\n",
       "  -78,\n",
       "  9,\n",
       "  -81,\n",
       "  111,\n",
       "  -80,\n",
       "  11,\n",
       "  -68,\n",
       "  -93,\n",
       "  -78,\n",
       "  -64,\n",
       "  -78,\n",
       "  -66,\n",
       "  -77,\n",
       "  94,\n",
       "  -88,\n",
       "  -63,\n",
       "  189,\n",
       "  15,\n",
       "  -74,\n",
       "  -73,\n",
       "  -75,\n",
       "  -87,\n",
       "  -65,\n",
       "  -70,\n",
       "  1,\n",
       "  -64,\n",
       "  -52,\n",
       "  -77,\n",
       "  -69,\n",
       "  -78,\n",
       "  -62,\n",
       "  -84,\n",
       "  102,\n",
       "  -58,\n",
       "  -71,\n",
       "  -62,\n",
       "  110,\n",
       "  10,\n",
       "  -62,\n",
       "  12,\n",
       "  -77,\n",
       "  -78,\n",
       "  -73,\n",
       "  -64,\n",
       "  -4,\n",
       "  -72,\n",
       "  -67,\n",
       "  -80,\n",
       "  -10,\n",
       "  -78,\n",
       "  -72,\n",
       "  -78,\n",
       "  -73,\n",
       "  102,\n",
       "  -64,\n",
       "  -75,\n",
       "  -71,\n",
       "  -77,\n",
       "  17,\n",
       "  8,\n",
       "  -73,\n",
       "  25,\n",
       "  93,\n",
       "  89,\n",
       "  -74,\n",
       "  -78,\n",
       "  -62,\n",
       "  -80,\n",
       "  -58,\n",
       "  20]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(env, ParObsSnakeEnv):\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size, interact=False)\n",
    "\n",
    "model_metrics_dir = os.path.join('../..', 'artifacts', 'models_stats', 'polNet')\n",
    "os.makedirs(model_metrics_dir, exist_ok=True)\n",
    "\n",
    "train_metrics_name = f'polNet_train_metrics_{environment}_{num_episodes}_{grid_size}.png'\n",
    "train_metrics_path = os.path.join(model_metrics_dir, train_metrics_name)\n",
    "agent.save_plots(train_metrics_path)\n",
    "\n",
    "num_simulations = 100\n",
    "sim_metrics_name = f'polNet_sim_metrics_{environment}_{num_episodes}_{env.grid_size}_{num_simulations}.jsn'\n",
    "sim_metrics_path = os.path.join(model_metrics_dir, sim_metrics_name)\n",
    "compute_metrics(agent, env, sim_metrics_path, num_simulations=num_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: -75\n"
     ]
    }
   ],
   "source": [
    "if isinstance(env, FullObsSnakeEnv):\n",
    "    env.interact = True\n",
    "else:\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size)\n",
    "    \n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = agent.choose_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
