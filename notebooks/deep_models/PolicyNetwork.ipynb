{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "# Add the project root directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "from src.ParObsSnakeEnv import ParObsSnakeEnv\n",
    "from src.FullObsSnakeEnv import FullObsSnakeEnv\n",
    "from src.utils import compute_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, device=None):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        # Metrics for plotting\n",
    "        self.episode_rewards = []\n",
    "        self.episode_losses = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # Add batch dimension\n",
    "        action_probs = self.policy(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def remember(self, log_prob, reward):\n",
    "        if not hasattr(self, 'log_probs'):\n",
    "            self.log_probs = []\n",
    "        if not hasattr(self, 'rewards'):\n",
    "            self.rewards = []\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"Update the policy using stored rewards and log probabilities.\"\"\"\n",
    "        if not hasattr(self, 'log_probs') or len(self.log_probs) == 0:\n",
    "            return\n",
    "\n",
    "        returns = self.compute_returns(self.rewards)\n",
    "        loss = -torch.sum(torch.stack(self.log_probs) * returns)  # Negative log-prob * return\n",
    "\n",
    "        # Update the policy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Log the loss\n",
    "        self.episode_losses.append(loss.item())\n",
    "\n",
    "        # Clear memory\n",
    "        self.log_probs.clear()\n",
    "        self.rewards.clear()\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        \"\"\"Compute discounted returns for an episode.\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        # Normalize returns to improve training stability\n",
    "        if len(returns) > 1 and returns.std() > 1e-5:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "        return returns\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Saves the entire agent to a file.\"\"\"\n",
    "        state = {\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'hyperparameters': {\n",
    "                'state_dim': self.state_dim,\n",
    "                'action_dim': self.action_dim,\n",
    "                'gamma': self.gamma,\n",
    "            },\n",
    "        }\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        print(f\"Agent saved to {filename}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename, lr=0.001):\n",
    "        \"\"\"Loads the agent from a file.\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "\n",
    "        # Recreate the agent\n",
    "        agent = cls(\n",
    "            state['hyperparameters']['state_dim'],\n",
    "            state['hyperparameters']['action_dim'],\n",
    "            lr=lr,\n",
    "            gamma=state['hyperparameters']['gamma'],\n",
    "        )\n",
    "        # Restore the agent's state\n",
    "        agent.policy.load_state_dict(state['policy_state_dict'])\n",
    "        agent.optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        print(f\"Agent loaded from {filename}\")\n",
    "        return agent\n",
    "\n",
    "    def train(self, env, episodes=1000, save_plots=False, plots_path='reinforce_training_plots.png'):\n",
    "        for episode in tqdm(range(episodes), desc=\"Training\", unit=\"episode\"):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            self.policy.train()\n",
    "\n",
    "            while not done:\n",
    "                action, log_prob = self.choose_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                self.remember(log_prob, reward)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            self.update_policy()\n",
    "            self.episode_rewards.append(total_reward)\n",
    "\n",
    "        if save_plots:\n",
    "            self.save_plots(plots_path)\n",
    "        self.policy.eval()\n",
    "\n",
    "\n",
    "    def save_plots(self, plots_path):\n",
    "        plots_dir = os.path.dirname(plots_path)\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        # Rewards per episode\n",
    "        axs[0].plot(self.episode_rewards)\n",
    "        axs[0].set_title(\"Episode Rewards\")\n",
    "        axs[0].set_xlabel(\"Episode\")\n",
    "        axs[0].set_ylabel(\"Total Reward\")\n",
    "\n",
    "        # Loss per episode\n",
    "        axs[1].plot(self.episode_losses)\n",
    "        axs[1].set_title(\"Loss Over Training\")\n",
    "        axs[1].set_xlabel(\"Episode\")\n",
    "        axs[1].set_ylabel(\"Loss\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plots_path)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "# env = FullObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "env = ParObsSnakeEnv(grid_size=grid_size, interact=False)\n",
    "if isinstance(env, FullObsSnakeEnv):\n",
    "    state_dim = env.observation_space.shape[0] * env.observation_space.shape[1] * env.observation_space.shape[2]\n",
    "else:\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "\n",
    "action_dim = env.action_space.n\n",
    "agent = REINFORCEAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10000/10000 [06:07<00:00, 27.20episode/s]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "agent.train(env, episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent saved to ../../models/polNet/polNet_agent_par_10000_10.pkl\n"
     ]
    }
   ],
   "source": [
    "environment = 'full 'if isinstance(env, FullObsSnakeEnv) else 'par'\n",
    "\n",
    "agent_name = f'polNet_agent_{environment}_{num_episodes}_{grid_size}.pkl'\n",
    "model_weights_dir = os.path.join('../..', 'models', 'polNet')\n",
    "os.makedirs(model_weights_dir, exist_ok=True)\n",
    "agent_path = os.path.join(model_weights_dir, agent_name)\n",
    "\n",
    "agent.save(agent_path)\n",
    "# agent = REINFORCEAgent.load(agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:00<00:06, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 1, Episode reward: -74\n",
      "Snake length: 4, Episode reward: 198\n",
      "Snake length: 8, Episode reward: 552\n",
      "Snake length: 4, Episode reward: 228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:00<00:05, 15.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 6, Episode reward: 352\n",
      "Snake length: 7, Episode reward: 467\n",
      "Snake length: 5, Episode reward: 304\n",
      "Snake length: 9, Episode reward: 619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:00<00:04, 19.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 7, Episode reward: 451\n",
      "Snake length: 4, Episode reward: 182\n",
      "Snake length: 4, Episode reward: 184\n",
      "Snake length: 2, Episode reward: 21\n",
      "Snake length: 7, Episode reward: 450\n",
      "Snake length: 2, Episode reward: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:01<00:04, 17.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 12, Episode reward: 891\n",
      "Snake length: 10, Episode reward: 724\n",
      "Snake length: 5, Episode reward: 251\n",
      "Snake length: 4, Episode reward: 192\n",
      "Snake length: 4, Episode reward: 191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:01<00:04, 17.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 7, Episode reward: 442\n",
      "Snake length: 6, Episode reward: 385\n",
      "Snake length: 9, Episode reward: 625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:01<00:04, 16.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 6, Episode reward: 372\n",
      "Snake length: 2, Episode reward: 41\n",
      "Snake length: 8, Episode reward: 573\n",
      "Snake length: 5, Episode reward: 273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:01<00:03, 18.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 3, Episode reward: 108\n",
      "Snake length: 7, Episode reward: 426\n",
      "Snake length: 4, Episode reward: 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:01<00:04, 14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 17, Episode reward: 1336\n",
      "Snake length: 9, Episode reward: 632\n",
      "Snake length: 4, Episode reward: 215\n",
      "Snake length: 5, Episode reward: 263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:02<00:04, 15.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 8, Episode reward: 530\n",
      "Snake length: 5, Episode reward: 274\n",
      "Snake length: 8, Episode reward: 546\n",
      "Snake length: 12, Episode reward: 872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:02<00:04, 13.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 12, Episode reward: 937\n",
      "Snake length: 7, Episode reward: 494\n",
      "Snake length: 6, Episode reward: 393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:02<00:03, 16.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 7, Episode reward: 446\n",
      "Snake length: 3, Episode reward: 113\n",
      "Snake length: 5, Episode reward: 255\n",
      "Snake length: 5, Episode reward: 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [00:02<00:03, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 12, Episode reward: 887\n",
      "Snake length: 7, Episode reward: 446\n",
      "Snake length: 7, Episode reward: 454\n",
      "Snake length: 10, Episode reward: 684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:03<00:02, 18.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 4, Episode reward: 189\n",
      "Snake length: 8, Episode reward: 548\n",
      "Snake length: 6, Episode reward: 350\n",
      "Snake length: 5, Episode reward: 259\n",
      "Snake length: 8, Episode reward: 517\n",
      "Snake length: 5, Episode reward: 263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [00:03<00:02, 16.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 21, Episode reward: 1705\n",
      "Snake length: 8, Episode reward: 534\n",
      "Snake length: 4, Episode reward: 193\n",
      "Snake length: 4, Episode reward: 180\n",
      "Snake length: 6, Episode reward: 379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:03<00:02, 15.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 10, Episode reward: 694\n",
      "Snake length: 9, Episode reward: 614\n",
      "Snake length: 6, Episode reward: 397\n",
      "Snake length: 4, Episode reward: 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [00:04<00:02, 15.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 9, Episode reward: 635\n",
      "Snake length: 11, Episode reward: 764\n",
      "Snake length: 5, Episode reward: 282\n",
      "Snake length: 4, Episode reward: 197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [00:04<00:01, 15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 15, Episode reward: 1141\n",
      "Snake length: 7, Episode reward: 441\n",
      "Snake length: 4, Episode reward: 189\n",
      "Snake length: 6, Episode reward: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [00:04<00:01, 16.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 6, Episode reward: 342\n",
      "Snake length: 3, Episode reward: 99\n",
      "Snake length: 10, Episode reward: 727\n",
      "Snake length: 1, Episode reward: -73\n",
      "Snake length: 7, Episode reward: 467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [00:04<00:01, 15.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 9, Episode reward: 645\n",
      "Snake length: 14, Episode reward: 1061\n",
      "Snake length: 6, Episode reward: 376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [00:05<00:01, 14.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 6, Episode reward: 367\n",
      "Snake length: 6, Episode reward: 395\n",
      "Snake length: 4, Episode reward: 201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [00:05<00:00, 15.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 8, Episode reward: 577\n",
      "Snake length: 6, Episode reward: 375\n",
      "Snake length: 4, Episode reward: 184\n",
      "Snake length: 7, Episode reward: 461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [00:05<00:00, 15.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 7, Episode reward: 444\n",
      "Snake length: 6, Episode reward: 364\n",
      "Snake length: 7, Episode reward: 450\n",
      "Snake length: 6, Episode reward: 373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [00:05<00:00, 15.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 4, Episode reward: 186\n",
      "Snake length: 12, Episode reward: 866\n",
      "Snake length: 1, Episode reward: -71\n",
      "Snake length: 8, Episode reward: 548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [00:06<00:00, 17.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 11, Episode reward: 801\n",
      "Snake length: 5, Episode reward: 299\n",
      "Snake length: 2, Episode reward: 10\n",
      "Snake length: 8, Episode reward: 524\n",
      "Snake length: 7, Episode reward: 446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 16.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snake length: 5, Episode reward: 279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'snake_lengths': [1,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  6,\n",
       "  7,\n",
       "  5,\n",
       "  9,\n",
       "  7,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  7,\n",
       "  2,\n",
       "  12,\n",
       "  10,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  7,\n",
       "  6,\n",
       "  9,\n",
       "  6,\n",
       "  2,\n",
       "  8,\n",
       "  5,\n",
       "  3,\n",
       "  7,\n",
       "  4,\n",
       "  17,\n",
       "  9,\n",
       "  4,\n",
       "  5,\n",
       "  8,\n",
       "  5,\n",
       "  8,\n",
       "  12,\n",
       "  12,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  12,\n",
       "  7,\n",
       "  7,\n",
       "  10,\n",
       "  4,\n",
       "  8,\n",
       "  6,\n",
       "  5,\n",
       "  8,\n",
       "  5,\n",
       "  21,\n",
       "  8,\n",
       "  4,\n",
       "  4,\n",
       "  6,\n",
       "  10,\n",
       "  9,\n",
       "  6,\n",
       "  4,\n",
       "  9,\n",
       "  11,\n",
       "  5,\n",
       "  4,\n",
       "  15,\n",
       "  7,\n",
       "  4,\n",
       "  6,\n",
       "  6,\n",
       "  3,\n",
       "  10,\n",
       "  1,\n",
       "  7,\n",
       "  9,\n",
       "  14,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  4,\n",
       "  8,\n",
       "  6,\n",
       "  4,\n",
       "  7,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  6,\n",
       "  4,\n",
       "  12,\n",
       "  1,\n",
       "  8,\n",
       "  11,\n",
       "  5,\n",
       "  2,\n",
       "  8,\n",
       "  7,\n",
       "  5],\n",
       " 'episode_rewards': [-74,\n",
       "  198,\n",
       "  552,\n",
       "  228,\n",
       "  352,\n",
       "  467,\n",
       "  304,\n",
       "  619,\n",
       "  451,\n",
       "  182,\n",
       "  184,\n",
       "  21,\n",
       "  450,\n",
       "  15,\n",
       "  891,\n",
       "  724,\n",
       "  251,\n",
       "  192,\n",
       "  191,\n",
       "  442,\n",
       "  385,\n",
       "  625,\n",
       "  372,\n",
       "  41,\n",
       "  573,\n",
       "  273,\n",
       "  108,\n",
       "  426,\n",
       "  180,\n",
       "  1336,\n",
       "  632,\n",
       "  215,\n",
       "  263,\n",
       "  530,\n",
       "  274,\n",
       "  546,\n",
       "  872,\n",
       "  937,\n",
       "  494,\n",
       "  393,\n",
       "  446,\n",
       "  113,\n",
       "  255,\n",
       "  275,\n",
       "  887,\n",
       "  446,\n",
       "  454,\n",
       "  684,\n",
       "  189,\n",
       "  548,\n",
       "  350,\n",
       "  259,\n",
       "  517,\n",
       "  263,\n",
       "  1705,\n",
       "  534,\n",
       "  193,\n",
       "  180,\n",
       "  379,\n",
       "  694,\n",
       "  614,\n",
       "  397,\n",
       "  199,\n",
       "  635,\n",
       "  764,\n",
       "  282,\n",
       "  197,\n",
       "  1141,\n",
       "  441,\n",
       "  189,\n",
       "  384,\n",
       "  342,\n",
       "  99,\n",
       "  727,\n",
       "  -73,\n",
       "  467,\n",
       "  645,\n",
       "  1061,\n",
       "  376,\n",
       "  367,\n",
       "  395,\n",
       "  201,\n",
       "  577,\n",
       "  375,\n",
       "  184,\n",
       "  461,\n",
       "  444,\n",
       "  364,\n",
       "  450,\n",
       "  373,\n",
       "  186,\n",
       "  866,\n",
       "  -71,\n",
       "  548,\n",
       "  801,\n",
       "  299,\n",
       "  10,\n",
       "  524,\n",
       "  446,\n",
       "  279]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(env, ParObsSnakeEnv):\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size, interact=False)\n",
    "\n",
    "model_metrics_dir = os.path.join('../..', 'artifacts', 'models_stats', 'polNet')\n",
    "os.makedirs(model_metrics_dir, exist_ok=True)\n",
    "\n",
    "train_metrics_name = f'polNet_train_metrics_{environment}_{num_episodes}_{grid_size}.png'\n",
    "train_metrics_path = os.path.join(model_metrics_dir, train_metrics_name)\n",
    "agent.save_plots(train_metrics_path)\n",
    "\n",
    "num_simulations = 100\n",
    "sim_metrics_name = f'polNet_sim_metrics_{environment}_{num_episodes}_{env.grid_size}_{num_simulations}.json'\n",
    "sim_metrics_path = os.path.join(model_metrics_dir, sim_metrics_name)\n",
    "compute_metrics(agent, env, sim_metrics_path, num_simulations=num_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: 1\n",
      "Reward: -1\n",
      "Reward: 1\n",
      "Reward: 76\n",
      "Reward: -75\n"
     ]
    }
   ],
   "source": [
    "if isinstance(env, FullObsSnakeEnv):\n",
    "    env.interact = True\n",
    "else:\n",
    "    env = ParObsSnakeEnv(grid_size=2*grid_size)\n",
    "    \n",
    "state = env.reset()\n",
    "done = False\n",
    "with torch.no_grad():\n",
    "    while not done:\n",
    "        action, _ = agent.choose_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
